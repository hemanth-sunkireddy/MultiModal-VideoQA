{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f1c1376",
   "metadata": {},
   "source": [
    "## Qwen 0.6B Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d34d872",
   "metadata": {},
   "outputs": [],
   "source": [
    "FOLDER_NAME = \"Data/Machine-Learning/\"\n",
    "\n",
    "VIDEO_DIR = FOLDER_NAME + \"Videos\"             # Folder containing input video files\n",
    "AUDIO_DIR = FOLDER_NAME + \"Audios\"             # Folder to store extracted audio files\n",
    "CHUNK_DIR = FOLDER_NAME + \"Audio-Chunks\"       # Folder to save audio chunks after VAD# Directory containing SRT files\n",
    "srt_directory = FOLDER_NAME + \"SRT-Files\"\n",
    "sentences_file = FOLDER_NAME + 'sentences.txt'\n",
    "metadata_file = FOLDER_NAME + 'srt-embedding-metadata.tsv'\n",
    "grouped_sentences_file = FOLDER_NAME + \"grouped_sentences.pkl\"\n",
    "grouped_sent_to_metadata_file = FOLDER_NAME + \"grouped_sent_to_metadata.pkl\"\n",
    "grouped_sentences_embeddings_file = FOLDER_NAME + \"grouped-sentences-embeddings.idx\"\n",
    "final_video_stitched_output_srt_file = FOLDER_NAME + \"stitched_output.srt\"\n",
    "final_video_file = FOLDER_NAME + \"answer.mp4\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "056097c4",
   "metadata": {},
   "source": [
    "### Install SUMMARISER MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc777f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install bert-extractive-summarizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d657e91d",
   "metadata": {},
   "source": [
    "Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bfbf162",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ffmpeg\n",
    "import os\n",
    "import faiss\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import re\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import csv\n",
    "import pickle\n",
    "\n",
    "# Paths\n",
    "\n",
    "\n",
    "# Create output directories if they don't exist\n",
    "os.makedirs(AUDIO_DIR, exist_ok=True)\n",
    "os.makedirs(CHUNK_DIR, exist_ok=True)\n",
    "\n",
    "# Audio processing\n",
    "TARGET_SAMPLE_RATE = 16000  # or 32000 Hz depending on your use case\n",
    "\n",
    "# VAD settings\n",
    "MIN_CHUNK_DURATION_SEC = 30  # Minimum duration for an audio chunk\n",
    "USE_ONNX_MODEL = False      # Set True to use ONNX version of Silero VAD\n",
    "\n",
    "from silero_vad import (\n",
    "    load_silero_vad, read_audio, get_speech_timestamps, \n",
    "    save_audio, VADIterator\n",
    ")\n",
    "\n",
    "# faiss_index = faiss.read_index(\"Data/sentence_embeddings.index\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a321f47e",
   "metadata": {},
   "source": [
    "### Model for Semantic Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "554479d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer(\"Qwen/Qwen3-Embedding-0.6B\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d41c6490",
   "metadata": {},
   "source": [
    "## Video to Audio Conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d21fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through all .mp4 files in the input folder\n",
    "for filename in os.listdir(VIDEO_DIR):\n",
    "    if filename.endswith(\".mp4\"):\n",
    "        input_path = os.path.join(VIDEO_DIR, filename)\n",
    "        output_path = os.path.join(AUDIO_DIR, filename.replace(\".mp4\", \".wav\"))\n",
    "\n",
    "        print(f\"Processing: {input_path} -> {output_path}\")\n",
    "        \n",
    "        # Extract audio\n",
    "        input_video = ffmpeg.input(input_path)\n",
    "        output_audio = ffmpeg.output(input_video.audio, output_path, ac=1, ar=TARGET_SAMPLE_RATE)\n",
    "        ffmpeg.run(output_audio, overwrite_output=True)\n",
    "        \n",
    "        # Probe the generated audio file for details\n",
    "        audio_info = ffmpeg.probe(output_path, v=\"error\", select_streams=\"a\", show_entries=\"stream=codec_name,codec_type,sample_rate,channels,bit_rate,duration\")\n",
    "        \n",
    "        codec_name = audio_info['streams'][0]['codec_name']\n",
    "        sample_rate = int(audio_info['streams'][0]['sample_rate'])\n",
    "        channels = int(audio_info['streams'][0]['channels'])\n",
    "        bit_rate = audio_info['streams'][0].get('bit_rate', 'N/A')\n",
    "        duration_sec = float(audio_info['streams'][0]['duration'])\n",
    "        duration_ms = duration_sec * 1000\n",
    "        \n",
    "        print(f\"Audio extracted: {output_path}\")\n",
    "        # print(f\"Codec: {codec_name}, Sample Rate: {sample_rate} Hz, Channels: {channels}, Bit Rate: {bit_rate}, Duration: {duration_ms} ms\\n\")\n",
    "\n",
    "print(\"Video to Audio converted successfully!.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be5d8a67",
   "metadata": {},
   "source": [
    "Voice Activity Detection Algorithm on Audio Files - Converting into Smaller Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "184e3aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Silero VAD model\n",
    "model = load_silero_vad(onnx=USE_ONNX_MODEL)\n",
    "\n",
    "def process_audio_file(audio_path, output_chunk_dir):\n",
    "    os.makedirs(output_chunk_dir, exist_ok=True)\n",
    "    \"\"\"Process an audio file, split it into chunks, and save them.\"\"\"\n",
    "    wav = read_audio(audio_path, sampling_rate=TARGET_SAMPLE_RATE)\n",
    "    speech_timestamps = get_speech_timestamps(\n",
    "        wav, model, sampling_rate=TARGET_SAMPLE_RATE, return_seconds=True\n",
    "    )\n",
    "    \n",
    "    # Format timestamps to 4 decimal places\n",
    "    for segment in speech_timestamps:\n",
    "        segment['start'] = float(f\"{segment['start']:.4f}\")\n",
    "        segment['end'] = float(f\"{segment['end']:.4f}\")\n",
    "    \n",
    "    vad_iterator = VADIterator(model, sampling_rate=TARGET_SAMPLE_RATE)\n",
    "    chunks = []\n",
    "    current_chunk_start = 0\n",
    "    \n",
    "    for segment in speech_timestamps:\n",
    "        start, end = segment['start'], segment['end']\n",
    "        if (end - current_chunk_start) >= MIN_CHUNK_DURATION_SEC:\n",
    "            chunk_wav = wav[int(current_chunk_start * TARGET_SAMPLE_RATE):int(end * TARGET_SAMPLE_RATE)]\n",
    "            chunk_path = os.path.join(output_chunk_dir, f\"{len(chunks) + 1}.wav\")\n",
    "            save_audio(chunk_path, chunk_wav, sampling_rate=TARGET_SAMPLE_RATE)\n",
    "            chunks.append((current_chunk_start, end, chunk_wav))\n",
    "            current_chunk_start = end\n",
    "    \n",
    "    # Save the last chunk if necessary\n",
    "    if current_chunk_start < speech_timestamps[-1]['end']:\n",
    "        chunk_wav = wav[int(current_chunk_start * TARGET_SAMPLE_RATE):]\n",
    "        chunk_path = os.path.join(output_chunk_dir, f\"{len(chunks) + 1}.wav\")\n",
    "        save_audio(chunk_path, chunk_wav, sampling_rate=TARGET_SAMPLE_RATE)\n",
    "        chunks.append((current_chunk_start, speech_timestamps[-1]['end'], chunk_wav))\n",
    "    \n",
    "    vad_iterator.reset_states()\n",
    "    print(f\"Processed {audio_path}, saved chunks in {output_chunk_dir}\")\n",
    "\n",
    "def process_all_audio_files():\n",
    "    \"\"\"Process all .wav files in the main audio folder and save their chunks.\"\"\"\n",
    "    if not os.path.exists(AUDIO_DIR):\n",
    "        print(f\"Audio folder '{AUDIO_DIR}' does not exist.\")\n",
    "        return\n",
    "    \n",
    "    for file_name in sorted(os.listdir(AUDIO_DIR)):\n",
    "        if file_name.endswith(\".wav\"):\n",
    "            audio_path = os.path.join(AUDIO_DIR, file_name)\n",
    "            audio_id = os.path.splitext(file_name)[0]  # Extract the number without extension\n",
    "            output_chunk_dir = os.path.join(CHUNK_DIR, audio_id)\n",
    "            process_audio_file(audio_path, output_chunk_dir)\n",
    "\n",
    "# Process VAD on Audio Files.\n",
    "process_all_audio_files()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0d2bc95",
   "metadata": {},
   "source": [
    "### Transcribing Audio Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "155a8a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import whisper\n",
    "import os\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Load Whisper model once\n",
    "model = whisper.load_model(\"large\", device=\"cuda\")\n",
    "\n",
    "# Function to transcribe and save sentence-wise output\n",
    "def transcribe_audio(audio_path, output_dir):\n",
    "    # Extract base name (without extension)\n",
    "    base_name = os.path.splitext(os.path.basename(audio_path))[0]\n",
    "\n",
    "    # Transcribe the audio\n",
    "    result = model.transcribe(audio_path, language=\"en\")\n",
    "\n",
    "    # Combine all text segments\n",
    "    full_text = \" \".join(segment[\"text\"].strip() for segment in result[\"segments\"])\n",
    "\n",
    "    # Split into proper sentences\n",
    "    sentences = sent_tokenize(full_text)\n",
    "\n",
    "    # Ensure output directory exists\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Save transcript (sentence-wise) to a text file\n",
    "    txt_file = os.path.join(output_dir, f\"{base_name}.txt\")\n",
    "    with open(txt_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        for sentence in sentences:\n",
    "            f.write(sentence.strip() + \"\\n\")  # Sentence per line\n",
    "\n",
    "    print(f\"✅ Transcription saved: {txt_file}\")\n",
    "\n",
    "# Iterate through numbered folders (1 to 26)\n",
    "for subfolder in sorted(os.listdir(root_audio_dir)):\n",
    "    subfolder_path = os.path.join(root_audio_dir, subfolder)\n",
    "\n",
    "    # Ensure it's a directory\n",
    "    if os.path.isdir(subfolder_path):\n",
    "        # Define corresponding output directory\n",
    "        output_subfolder = os.path.join(root_output_dir, subfolder)\n",
    "\n",
    "        # Process each audio file in the subfolder\n",
    "        for file in os.listdir(subfolder_path):\n",
    "            if file.endswith(\".wav\"):\n",
    "                audio_file_path = os.path.join(subfolder_path, file)\n",
    "                transcribe_audio(audio_file_path, output_subfolder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40af294b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "# Root directory containing numbered subfolders\n",
    "root_audio_dir = \"Data/Speech-Processing/extracted_text_speech_processing\"\n",
    "root_output_dir = \"Data/Speech-Processing/Transcribe-Text\"\n",
    "\n",
    "# Define folder paths\n",
    "f1 = \"Data/Speech-Processing/Audio-Chunks-Speech-Processing\"\n",
    "f2 = \"Data/Speech-Processing/Transcribe-Text\"\n",
    "\n",
    "# Iterate over directories\n",
    "for subfolder in sorted(os.listdir(f2), key=lambda x: int(x)):  \n",
    "  \n",
    "    source_folder = os.path.join(f2, subfolder)\n",
    "    destination_folder = os.path.join(f1, subfolder)\n",
    "\n",
    "    # Ensure the destination folder exists\n",
    "    if not os.path.exists(destination_folder):\n",
    "        print(f\"Warning: Destination folder {destination_folder} does not exist, skipping...\")\n",
    "        continue\n",
    "\n",
    "    # Copy all .txt files from source to destination\n",
    "    for file in os.listdir(source_folder):\n",
    "        if file.endswith(\".txt\"):\n",
    "            src_file = os.path.join(source_folder, file)\n",
    "            dest_file = os.path.join(destination_folder, file)\n",
    "            shutil.copy2(src_file, dest_file)  # copy2 preserves metadata\n",
    "            print(f\"Copied {src_file} -> {dest_file}\")\n",
    "\n",
    "print(\"All .txt files copied successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19607e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import os\n",
    "\n",
    "# Define paths\n",
    "root_corpus_dir = \"Data/Speech-Processing/Audio-Chunks-Speech-Processing\"\n",
    "dict_path = \"english_us_arpa\"\n",
    "model_path = \"english_us_arpa\"\n",
    "root_output_dir = \"Data/Speech-Processing/complete_timestamp\"\n",
    "\n",
    "# Iterate through numbered folders (1 to 26)\n",
    "for subfolder in sorted(os.listdir(root_corpus_dir), key=lambda x: int(x)):\n",
    "\n",
    "    # if subfolder not in [\"22\",\"23\",\"24\",\"25\",\"26\"]:\n",
    "    #     continue\n",
    "    subfolder_corpus_path = os.path.join(root_corpus_dir, subfolder)\n",
    "    subfolder_output_path = os.path.join(root_output_dir, subfolder)\n",
    "\n",
    "    # Ensure the output directory exists\n",
    "    os.makedirs(subfolder_output_path, exist_ok=True)\n",
    "\n",
    "    # Run the alignment command\n",
    "    cmd = [\n",
    "        \"mfa\", \"align\",\"--clean\", \"--output_format\", \"csv\",\n",
    "        subfolder_corpus_path, dict_path, model_path, subfolder_output_path\n",
    "    ]\n",
    "    subprocess.run(cmd, check=True)\n",
    "    print(f\"Alignment completed for {subfolder}!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a6b5b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import librosa\n",
    "from natsort import natsorted  # To ensure files are processed in correct order\n",
    "\n",
    "def get_audio_length(audio_file):\n",
    "    \"\"\"Get the exact length of an audio file using librosa.\"\"\"\n",
    "    y, sr = librosa.load(audio_file, sr=None)\n",
    "    return librosa.get_duration(y=y, sr=sr)\n",
    "\n",
    "def convert_to_srt_time(seconds):\n",
    "    \"\"\"Convert seconds to SRT time format (hh:mm:ss,ms).\"\"\"\n",
    "    millisec = int((seconds - int(seconds)) * 1000)\n",
    "    hours, remainder = divmod(int(seconds), 3600)\n",
    "    minutes, sec = divmod(remainder, 60)\n",
    "    return f\"{hours:02}:{minutes:02}:{sec:02},{millisec:03}\"\n",
    "\n",
    "def merge_chunks_to_srt(csv_folder,txt_folder,audio_folder, srt_file):\n",
    "    \"\"\"Process all chunk CSVs and TXT files to generate a sentence-level SRT file.\"\"\"\n",
    "    chunk_files = natsorted([f for f in os.listdir(csv_folder) if f.endswith(\".csv\")])  # Sort correctly\n",
    "    # chunk_files = natsorted([f for f in os.listdir(audio_folder) if f.endswith(\".wav\")])  \n",
    "    # print(chunk_files)\n",
    "    total_offset = 0  # Offset to adjust timestamps\n",
    "\n",
    "    with open(srt_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        subtitle_index = 1  # SRT subtitle counter\n",
    "        for chunk in chunk_files:\n",
    "            csv_path = os.path.join(csv_folder, chunk)\n",
    "            txt_path = os.path.join(txt_folder, chunk.replace(\".csv\", \".txt\"))\n",
    "            audio_path = os.path.join(audio_folder, chunk.replace(\".csv\", \".wav\"))  # Assuming audio has the same name\n",
    "\n",
    "            # Get audio length of this chunk\n",
    "            chunk_length = get_audio_length(audio_path)\n",
    "\n",
    "            # Read CSV\n",
    "            df = pd.read_csv(csv_path)\n",
    "\n",
    "            # Filter for words only\n",
    "            df = df[df[\"Type\"] == \"words\"].reset_index(drop=True)\n",
    "\n",
    "            # Read sentences from the corresponding TXT file\n",
    "            with open(txt_path, \"r\", encoding=\"utf-8\") as txt_file:\n",
    "                sentences = txt_file.readlines()\n",
    "\n",
    "            word_index = 0  # Track position in the word list\n",
    "\n",
    "            for sentence in sentences:\n",
    "                words = sentence.strip().split()\n",
    "\n",
    "                if word_index >= len(df):\n",
    "                    break  # Avoid index error\n",
    "\n",
    "                # Get the start and end timestamps for the sentence\n",
    "                start_time = df.loc[word_index, 'Begin'] + total_offset\n",
    "                end_time = df.loc[min(word_index + len(words) - 1, len(df) - 1), 'End'] + total_offset\n",
    "\n",
    "                # Write to SRT\n",
    "                f.write(f\"{subtitle_index}\\n\")\n",
    "                f.write(f\"{convert_to_srt_time(start_time)} --> {convert_to_srt_time(end_time)}\\n\")\n",
    "                f.write(f\"{sentence.strip()}\\n\\n\")\n",
    "\n",
    "                subtitle_index += 1  # Increment subtitle number\n",
    "                word_index += len(words)  # Move to the next sentence\n",
    "\n",
    "            # Update offset for next chunk\n",
    "            total_offset += chunk_length\n",
    "\n",
    "    print(f\"✅ Sentence-level SRT file saved as: {srt_file}\")\n",
    "\n",
    "# Example usage\n",
    "csv_base_folder = \"Data/Speech-Processing/complete_timestamp\"  # Change to your folder path\n",
    "txt_base_folder=\"Data/Speech-Processing/audio_chunks_text\"\n",
    "audio_base_folder=\"Data/Speech-Processing/Audio-Chunks-Speech-Processing\"\n",
    "srt_base_folder = \"Data/Speech-Processing/complete_srt\"\n",
    "\n",
    "for subfolder in sorted(os.listdir(audio_base_folder), key=lambda x: int(x)):\n",
    "    subfolder_path = os.path.join(audio_base_folder, subfolder)\n",
    "\n",
    "    # Ensure it's a directory\n",
    "    if os.path.isdir(subfolder_path):\n",
    "        # Define corresponding output directory\n",
    "        # output_subfolder = os.path.join(root_output_dir, subfolder)\n",
    "        csv_subfolder = os.path.join(csv_base_folder, subfolder)\n",
    "        txt_subfolder=os.path.join(txt_base_folder, subfolder)\n",
    "        audio_subfolder = os.path.join(audio_base_folder, subfolder)\n",
    "        srt_file_path = os.path.join(srt_base_folder, subfolder+\".srt\")\n",
    "\n",
    "        merge_chunks_to_srt(csv_subfolder, txt_subfolder,audio_subfolder,srt_file_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e7588d",
   "metadata": {},
   "source": [
    "## SRT to Sentences generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba7e8ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract combined sentences with timestamps from .srt file\n",
    "def extract_sentences_from_srt(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    sentences = []\n",
    "    timestamps = []\n",
    "    current_sentence = \"\"\n",
    "    current_timestamp = \"\"\n",
    "\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "\n",
    "        # Check for timestamp lines\n",
    "        timestamp_match = re.match(r'(\\d{2}:\\d{2}:\\d{2}[.,]\\d{3}) --> (\\d{2}:\\d{2}:\\d{2}[.,]\\d{3})', line)\n",
    "        if timestamp_match:\n",
    "            current_timestamp = timestamp_match.group(1).replace(',', '.') + ' --> ' + timestamp_match.group(2).replace(',', '.')\n",
    "            continue\n",
    "\n",
    "        # Skip empty lines and cue identifiers\n",
    "        if not line or line.isdigit():\n",
    "            continue\n",
    "\n",
    "        # Add line to current sentence\n",
    "        current_sentence += \" \" + line if current_sentence else line\n",
    "\n",
    "        # If sentence ends, save it\n",
    "        if re.search(r'[.!?]$', line):\n",
    "            sentences.append(current_sentence.strip())\n",
    "            timestamps.append(current_timestamp)\n",
    "            current_sentence = \"\"\n",
    "            current_timestamp = \"\"\n",
    "\n",
    "    return sentences, timestamps\n",
    "\n",
    "\n",
    "\n",
    "# Initialize lists for all sentences, timestamps, and filenames\n",
    "all_sentences = []\n",
    "all_timestamps = []\n",
    "all_filenames = []\n",
    "\n",
    "# Process all SRT files with tqdm\n",
    "srt_files = [f for f in os.listdir(srt_directory) if f.endswith('.srt')]\n",
    "for file_name in tqdm(srt_files, desc=\"Processing SRT files\"):\n",
    "    file_path = os.path.join(srt_directory, file_name)\n",
    "    sentences, timestamps = extract_sentences_from_srt(file_path)\n",
    "    all_sentences.extend(sentences)\n",
    "    all_timestamps.extend(timestamps)\n",
    "    all_filenames.extend([file_name] * len(sentences))  # associate each sentence with its file\n",
    "\n",
    "# print(\"Encoding sentences into embeddings...\")\n",
    "# Uncomment for all miniLM and all mpnet.\n",
    "# sentence_embeddings = np.array(\n",
    "#     model.encode(all_sentences) \n",
    "# ).astype('float32')\n",
    "\n",
    "# sentence_embeddings = model.encode(all_sentences)\n",
    "\n",
    "# Create FAISS index (use Inner Product for cosine similarity)\n",
    "# embedding_dimension = sentence_embeddings.shape[1]\n",
    "# faiss_index = faiss.IndexFlatIP(embedding_dimension)\n",
    "# faiss_index.add(sentence_embeddings)\n",
    "\n",
    "# Save FAISS index\n",
    "# faiss.write_index(faiss_index, \"Data/sentence_embeddings.index\")\n",
    "\n",
    "# Save metadata to file\n",
    "\n",
    "with open(metadata_file, 'w', encoding='utf-8') as file:\n",
    "    file.write(\"filename\\ttimestamp\\tsentence\\n\")\n",
    "    for fname, timestamp, sentence in zip(all_filenames, all_timestamps, all_sentences):\n",
    "        clean_sentence = sentence.replace('\\t', ' ').replace('\\n', ' ')\n",
    "        file.write(f\"{fname}\\t{timestamp}\\t{clean_sentence}\\n\")\n",
    "\n",
    "# Save sentences to a text file\n",
    "\n",
    "with open(sentences_file, 'w', encoding='utf-8') as file:\n",
    "    for sentence in all_sentences:\n",
    "        file.write(sentence.strip().replace('\\n', ' ') + '\\n')\n",
    "\n",
    "# Summary\n",
    "# print(f\"\\nEmbeddings created for {len(all_sentences)} sentences from {len(srt_files)} SRT files.\")\n",
    "# print(\"FAISS index saved as 'Data/sentence_embeddings.index'\")\n",
    "print(f\"Metadata saved as '{metadata_file}'\")\n",
    "print(f\"Sentences saved as '{sentences_file}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f406234c",
   "metadata": {},
   "source": [
    "## Finding Related Sentences to a Question"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "565b77c4",
   "metadata": {},
   "source": [
    "### Grouping Sentences N-gram technique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbdc7e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_list = []\n",
    "with open(metadata_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    reader = csv.reader(f, delimiter='\\t')\n",
    "    next(reader)\n",
    "    for row in reader:\n",
    "        filename, timestamp, sentence = row\n",
    "        metadata_list.append((filename.strip(), timestamp.strip(), sentence.strip()))\n",
    "\n",
    "group_size = 3\n",
    "grouped_sentences = []\n",
    "grouped_sent_to_metadata = {}\n",
    "\n",
    "def extract_start_end(ts):\n",
    "    start, end = ts.split(\"-->\")\n",
    "    return start.strip(), end.strip()\n",
    "\n",
    "# Group by filename first\n",
    "from itertools import groupby\n",
    "\n",
    "for filename, file_group in groupby(metadata_list, key=lambda x: x[0]):\n",
    "    file_group = list(file_group)\n",
    "    for i in range(len(file_group) - group_size + 1):\n",
    "        group = file_group[i:i+group_size]\n",
    "        grouped_text = \" \".join(sent for _, _, sent in group)\n",
    "        first_start, _ = extract_start_end(group[0][1])\n",
    "        _, last_end = extract_start_end(group[-1][1])\n",
    "        timestamp_range = f\"{first_start} --> {last_end}\"\n",
    "        individual_timestamps = [ts for _, ts, _ in group]\n",
    "        grouped_sentences.append(grouped_text)\n",
    "        grouped_sent_to_metadata[grouped_text] = {\n",
    "            \"filename\": filename,\n",
    "            \"timestamp_range\": timestamp_range,\n",
    "            \"individual_timestamps\": individual_timestamps\n",
    "        }\n",
    "\n",
    "# Save pickles\n",
    "with open(grouped_sentences_file, \"wb\") as f:\n",
    "    pickle.dump(grouped_sentences, f)\n",
    "\n",
    "with open(grouped_sent_to_metadata_file, \"wb\") as f:\n",
    "    pickle.dump(grouped_sent_to_metadata, f)\n",
    "\n",
    "first_key = next(iter(grouped_sent_to_metadata))\n",
    "print(\"Grouped Sentence to Metadata First element:\", first_key, \"->\", grouped_sent_to_metadata[first_key])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c6e162",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load grouped sentences\n",
    "with open(grouped_sentences_file, \"rb\") as f:\n",
    "    grouped_sentences = pickle.load(f)\n",
    "\n",
    "# Load precomputed metadata mapping\n",
    "with open(grouped_sent_to_metadata_file, \"rb\") as f:\n",
    "    grouped_sent_to_metadata = pickle.load(f)\n",
    "print(\"Encoding grouped sentences into embeddings...\")\n",
    "\n",
    "batch_size = 16  # you can adjust this based on memory\n",
    "all_embeddings = []\n",
    "\n",
    "for i in tqdm(range(0, len(grouped_sentences), batch_size), desc=\"Encoding batches\"):\n",
    "    batch = grouped_sentences[i:i+batch_size]\n",
    "    batch_emb = model.encode(batch)  # encode the batch\n",
    "    all_embeddings.append(batch_emb)\n",
    "\n",
    "# Combine all batches into one array and convert to float32\n",
    "grouped_embeddings = np.vstack(all_embeddings).astype('float32')\n",
    "\n",
    "# Create FAISS index (Inner Product for cosine similarity)\n",
    "embedding_dim = grouped_embeddings.shape[1]\n",
    "faiss_index = faiss.IndexFlatIP(embedding_dim)\n",
    "faiss_index.add(grouped_embeddings)\n",
    "\n",
    "# Save FAISS index\n",
    "faiss.write_index(faiss_index, grouped_sentences_embeddings_file)\n",
    "\n",
    "print(\"✅ FAISS index created and saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "947e47fe",
   "metadata": {},
   "source": [
    "### QA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c19be7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import torch\n",
    "import pickle\n",
    "import faiss\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "faiss_index = faiss.read_index(grouped_sentences_embeddings_file)\n",
    "\n",
    "with open(grouped_sentences_file, \"rb\") as f:\n",
    "    grouped_sentences = pickle.load(f)\n",
    "\n",
    "with open(grouped_sent_to_metadata_file, \"rb\") as f:\n",
    "    grouped_sent_to_metadata = pickle.load(f)\n",
    "\n",
    "student_question = input(\"Enter your question: \")\n",
    "question_embedding = model.encode(student_question, prompt_name=\"query\")\n",
    "if question_embedding.ndim == 1:\n",
    "    question_embedding = np.expand_dims(question_embedding, axis=0)\n",
    "\n",
    "distances, indices = faiss_index.search(question_embedding, 10)\n",
    "\n",
    "related_results = []\n",
    "for idx in indices[0]:\n",
    "    grouped_sent = grouped_sentences[idx]\n",
    "    meta = grouped_sent_to_metadata.get(grouped_sent, None)\n",
    "\n",
    "    if meta:\n",
    "        filename = meta[\"filename\"]\n",
    "        timestamp_range = meta[\"timestamp_range\"]\n",
    "        individual_timestamps = meta.get(\"individual_timestamps\", [])\n",
    "    else:\n",
    "        filename = \"Unknown\"\n",
    "        timestamp_range = \"Unknown\"\n",
    "        individual_timestamps = []\n",
    "\n",
    "    related_results.append(\n",
    "        (filename, timestamp_range, grouped_sent, individual_timestamps)\n",
    "    )\n",
    "\n",
    "print(\"Question:\", student_question)\n",
    "print(\"\\nTop Related Sentences with Metadata:\")\n",
    "for filename, timestamp, sent, indiv_ts in related_results:\n",
    "    print(f\"- [{filename} | {timestamp}] {sent}\")\n",
    "    if indiv_ts:\n",
    "        print(\"  ↳ Individual timestamps:\", indiv_ts)\n",
    "    print()\n",
    "\n",
    "# Group results by file\n",
    "grouped_by_file = {}\n",
    "for filename, timestamp, sent, _ in related_results:\n",
    "    if filename not in grouped_by_file:\n",
    "        grouped_by_file[filename] = []\n",
    "    grouped_by_file[filename].append((timestamp, sent))\n",
    "\n",
    "# Helper to parse timestamps\n",
    "def parse_ts(ts):\n",
    "    start, end = ts.split(\"->\")\n",
    "    fmt = \"%H:%M:%S.%f\"\n",
    "    return datetime.strptime(start.strip(), fmt), datetime.strptime(end.strip(), fmt)\n",
    "\n",
    "# Helper to clean and normalize sentences\n",
    "def clean_sentences(sentences):\n",
    "    cleaned = []\n",
    "    seen = set()\n",
    "    for s in sentences:\n",
    "        s = s.strip()\n",
    "        while s and s[-1] in \".!?\":\n",
    "            s = s[:-1].strip()\n",
    "        if s and s not in seen:\n",
    "            cleaned.append(s)\n",
    "            seen.add(s)\n",
    "    return cleaned\n",
    "\n",
    "def extract_file_number(fname):\n",
    "    match = re.search(r\"(\\d+)\", fname)\n",
    "    return int(match.group(1)) if match else float(\"inf\")\n",
    "\n",
    "# Sort by file and start timestamp\n",
    "related_results_sorted = sorted(\n",
    "    related_results,\n",
    "    key=lambda x: (extract_file_number(x[0]), parse_ts(x[1])[0])\n",
    ")\n",
    "\n",
    "# Merge overlapping timestamps within same file\n",
    "merged_results = []\n",
    "for filename, ts_range, text, indiv_ts in related_results_sorted:\n",
    "    sentences = clean_sentences(text.split('. '))\n",
    "\n",
    "    if not merged_results:\n",
    "        merged_results.append([\n",
    "            filename, ts_range, '. '.join(sentences) + '.', indiv_ts\n",
    "        ])\n",
    "        continue\n",
    "\n",
    "    prev = merged_results[-1]\n",
    "    prev_start, prev_end = parse_ts(prev[1])\n",
    "    curr_start, curr_end = parse_ts(ts_range)\n",
    "\n",
    "    if filename == prev[0] and curr_start <= prev_end:\n",
    "        new_start = prev_start.strftime(\"%H:%M:%S.%f\")[:-3]\n",
    "        new_end = max(prev_end, curr_end).strftime(\"%H:%M:%S.%f\")[:-3]\n",
    "        prev[1] = f\"{new_start} -> {new_end}\"\n",
    "\n",
    "        prev_sentences = clean_sentences(prev[2].split('. '))\n",
    "        combined_sentences = prev_sentences + sentences\n",
    "        prev[2] = '. '.join(clean_sentences(combined_sentences)) + '.'\n",
    "\n",
    "        # Combine timestamps\n",
    "        prev[3].extend(indiv_ts)\n",
    "    else:\n",
    "        merged_results.append([filename, ts_range, '. '.join(sentences) + '.', indiv_ts])\n",
    "\n",
    "# Print merged results\n",
    "print(\"\\nOrdered & Merged Top Related Sentences (duplicates removed):\")\n",
    "for i, (fname, ts, sent, indiv_ts) in enumerate(merged_results, 1):\n",
    "    print(f\"\\nGroup {i}\")\n",
    "    print(\"Text:\", sent)\n",
    "    print(\"Metadata:\", (fname, ts))\n",
    "    if indiv_ts:\n",
    "        print(\"Individual timestamps:\", indiv_ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f9b6f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "segment_list = []\n",
    "\n",
    "for filename, ts_range, text, individual_timestamps in merged_results:\n",
    "    distance = 0.0  # placeholder for similarity or ranking score\n",
    "    segment_list.append({\n",
    "        \"filename\": filename,\n",
    "        \"timestamp_range\": ts_range,\n",
    "        \"text\": text,\n",
    "        \"individual_timestamps\": individual_timestamps,\n",
    "        \"distance\": distance\n",
    "    })\n",
    "\n",
    "# Now segment_list is ready to use\n",
    "for seg in segment_list:\n",
    "    print(f\"[{seg['filename']} | {seg['timestamp_range']}]\")\n",
    "    print(f\"Text: {seg['text']}\")\n",
    "    print(f\"Individual timestamps: {seg['individual_timestamps']}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07dff472",
   "metadata": {},
   "outputs": [],
   "source": [
    "from moviepy.editor import VideoFileClip, concatenate_videoclips, ColorClip\n",
    "import os\n",
    "import datetime\n",
    "\n",
    "def format_srt_timestamp(seconds):\n",
    "    td = datetime.timedelta(seconds=seconds)\n",
    "    total_seconds = int(td.total_seconds())\n",
    "    milliseconds = int((td.total_seconds() - total_seconds) * 1000)\n",
    "    hours, remainder = divmod(total_seconds, 3600)\n",
    "    minutes, seconds = divmod(remainder, 60)\n",
    "    return f\"{hours:02}:{minutes:02}:{int(seconds):02},{milliseconds:03}\"\n",
    "\n",
    "def create_continuous_srt(clips_info, output_filename=final_video_stitched_output_srt_file, transition_sec=0.01):\n",
    "    srt_lines = []\n",
    "    current_time = 0.0\n",
    "\n",
    "    for idx, (duration, sentence, start, video_file) in enumerate(clips_info, start=1):\n",
    "        start_time = format_srt_timestamp(current_time)\n",
    "        end_time = format_srt_timestamp(current_time + duration)\n",
    "        srt_lines.append(f\"{idx}\\n{start_time} --> {end_time}\\n{sentence}\\n\")\n",
    "        current_time += duration + transition_sec  # Account for pause\n",
    "    with open(output_filename, \"w\") as f:\n",
    "        f.write(\"\\n\".join(srt_lines))\n",
    "\n",
    "def parse_timestamp(timestamp_str):\n",
    "    start, end = timestamp_str.split(\" -> \")\n",
    "    return start.strip(), end.strip()\n",
    "\n",
    "def stitch_video_from_segments(segment_list, srt_filename=final_video_stitched_output_srt_file, pause_duration=0.01):\n",
    "    clips = []\n",
    "    clips_info = []\n",
    "    sources = set()\n",
    "\n",
    "    for idx, (filename, timestamp, sentence, distance) in enumerate(segment_list):\n",
    "        lecture_no = os.path.splitext(filename)[0]\n",
    "        video_file = VIDEO_DIR + \"/\" + lecture_no + \".mp4\"\n",
    "        start, end = parse_timestamp(timestamp)\n",
    "        sources.add(\"Lecture - \" + lecture_no)\n",
    "\n",
    "        try:\n",
    "            clip = VideoFileClip(video_file).subclip(start, end)\n",
    "\n",
    "            # Resize black screen to match clip size\n",
    "            if idx > 0:\n",
    "                black_clip = ColorClip(size=clip.size, color=(0, 0, 0), duration=pause_duration)\n",
    "                black_clip = black_clip.set_fps(clip.fps)\n",
    "                clips.append(black_clip)\n",
    "\n",
    "            clips.append(clip)\n",
    "            clips_info.append((clip.duration, sentence, start, video_file))\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing segment ({filename}, {timestamp}): {e}\")\n",
    "\n",
    "    if not clips:\n",
    "        print(\"No valid clips found.\")\n",
    "        return\n",
    "\n",
    "    final_clip = concatenate_videoclips(clips, method=\"chain\")\n",
    "\n",
    "    final_clip.write_videofile(\n",
    "        final_video_file,\n",
    "        codec=\"libx264\",\n",
    "        preset=\"ultrafast\",\n",
    "        threads=4,\n",
    "        audio_codec=\"aac\"\n",
    "    )\n",
    "\n",
    "    create_continuous_srt(clips_info, output_filename=srt_filename, transition_sec=pause_duration)\n",
    "\n",
    "stitch_video_from_segments(segment_list, srt_filename=final_video_stitched_output_srt_file, pause_duration=0.01)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d92ae79",
   "metadata": {},
   "source": [
    "### Summarisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b2dbf67",
   "metadata": {},
   "outputs": [],
   "source": [
    "from summarizer import Summarizer\n",
    "\n",
    "# Initialize summarizer\n",
    "summarizer = Summarizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a53200b5",
   "metadata": {},
   "source": [
    "### Summarisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c24187d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract texts and timestamps\n",
    "texts = [sent for _, _, sent, _ in merged_results]\n",
    "timestamps = [ts for _, ts, _, _ in merged_results]\n",
    "\n",
    "# Combine all texts\n",
    "full_text = \" \".join(texts)\n",
    "\n",
    "# Run summarization\n",
    "summary_text = summarizer(full_text, ratio=0.6)\n",
    "\n",
    "if isinstance(summary_text, list):\n",
    "    summary_text = \" \".join(summary_text)\n",
    "\n",
    "# Split summary into sentences\n",
    "summary_sentences = [s.strip() for s in summary_text.split('. ') if s.strip()]\n",
    "\n",
    "def find_timestamp(sentence):\n",
    "    # Try to find which merged text contains this sentence or a portion of it\n",
    "    for text, ts in zip(texts, timestamps):\n",
    "        if sentence[:20].lower() in text.lower() or sentence.lower() in text.lower():\n",
    "            return ts\n",
    "    return \"Unknown\"\n",
    "\n",
    "# Print summary with timestamps\n",
    "print(\"\\nSummary with timestamps:\\n\")\n",
    "for s in summary_sentences:\n",
    "    ts = find_timestamp(s)\n",
    "    print(f\"[{ts}] {s}.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
