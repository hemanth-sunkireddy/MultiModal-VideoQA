{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f1c1376",
   "metadata": {},
   "source": [
    "## Qwen 0.6B Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "056097c4",
   "metadata": {},
   "source": [
    "### Install SUMMARISER MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc777f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install bert-extractive-summarizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d657e91d",
   "metadata": {},
   "source": [
    "Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1bfbf162",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import ffmpeg\n",
    "import os\n",
    "import faiss\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import re\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import csv\n",
    "\n",
    "# Paths\n",
    "VIDEO_DIR = \"Data/Videos\"             # Folder containing input video files\n",
    "AUDIO_DIR = \"Data/Audios\"             # Folder to store extracted audio files\n",
    "CHUNK_DIR = \"Data/Audio-Chunks\"       # Folder to save audio chunks after VAD\n",
    "\n",
    "# Create output directories if they don't exist\n",
    "os.makedirs(AUDIO_DIR, exist_ok=True)\n",
    "os.makedirs(CHUNK_DIR, exist_ok=True)\n",
    "\n",
    "# Audio processing\n",
    "TARGET_SAMPLE_RATE = 16000  # or 32000 Hz depending on your use case\n",
    "\n",
    "# VAD settings\n",
    "MIN_CHUNK_DURATION_SEC = 30  # Minimum duration for an audio chunk\n",
    "USE_ONNX_MODEL = False      # Set True to use ONNX version of Silero VAD\n",
    "\n",
    "from silero_vad import (\n",
    "    load_silero_vad, read_audio, get_speech_timestamps, \n",
    "    save_audio, VADIterator\n",
    ")\n",
    "\n",
    "faiss_index = faiss.read_index(\"Data/sentence_embeddings.index\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a321f47e",
   "metadata": {},
   "source": [
    "### Model for Semantic Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "554479d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer(\"Qwen/Qwen3-Embedding-0.6B\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af288996",
   "metadata": {},
   "source": [
    "### Summarisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2e89e92",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from summarizer import Summarizer\n",
    "\n",
    "# Initialize summarizer\n",
    "summarizer = Summarizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d41c6490",
   "metadata": {},
   "source": [
    "## Video to Audio Conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d21fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through all .mp4 files in the input folder\n",
    "for filename in os.listdir(VIDEO_DIR):\n",
    "    if filename.endswith(\".mp4\"):\n",
    "        input_path = os.path.join(VIDEO_DIR, filename)\n",
    "        output_path = os.path.join(AUDIO_DIR, filename.replace(\".mp4\", \".wav\"))\n",
    "\n",
    "        print(f\"Processing: {input_path} -> {output_path}\")\n",
    "        \n",
    "        # Extract audio\n",
    "        input_video = ffmpeg.input(input_path)\n",
    "        output_audio = ffmpeg.output(input_video.audio, output_path, ac=1, ar=TARGET_SAMPLE_RATE)\n",
    "        ffmpeg.run(output_audio, overwrite_output=True)\n",
    "        \n",
    "        # Probe the generated audio file for details\n",
    "        audio_info = ffmpeg.probe(output_path, v=\"error\", select_streams=\"a\", show_entries=\"stream=codec_name,codec_type,sample_rate,channels,bit_rate,duration\")\n",
    "        \n",
    "        codec_name = audio_info['streams'][0]['codec_name']\n",
    "        sample_rate = int(audio_info['streams'][0]['sample_rate'])\n",
    "        channels = int(audio_info['streams'][0]['channels'])\n",
    "        bit_rate = audio_info['streams'][0].get('bit_rate', 'N/A')\n",
    "        duration_sec = float(audio_info['streams'][0]['duration'])\n",
    "        duration_ms = duration_sec * 1000\n",
    "        \n",
    "        print(f\"Audio extracted: {output_path}\")\n",
    "        # print(f\"Codec: {codec_name}, Sample Rate: {sample_rate} Hz, Channels: {channels}, Bit Rate: {bit_rate}, Duration: {duration_ms} ms\\n\")\n",
    "\n",
    "print(\"Video to Audio converted successfully!.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be5d8a67",
   "metadata": {},
   "source": [
    "Voice Activity Detection Algorithm on Audio Files - Converting into Smaller Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "184e3aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Silero VAD model\n",
    "model = load_silero_vad(onnx=USE_ONNX_MODEL)\n",
    "\n",
    "def process_audio_file(audio_path, output_chunk_dir):\n",
    "    \"\"\"Process an audio file, split it into chunks, and save them.\"\"\"\n",
    "    wav = read_audio(audio_path, sampling_rate=TARGET_SAMPLE_RATE)\n",
    "    speech_timestamps = get_speech_timestamps(\n",
    "        wav, model, sampling_rate=TARGET_SAMPLE_RATE, return_seconds=True\n",
    "    )\n",
    "    \n",
    "    # Format timestamps to 4 decimal places\n",
    "    for segment in speech_timestamps:\n",
    "        segment['start'] = float(f\"{segment['start']:.4f}\")\n",
    "        segment['end'] = float(f\"{segment['end']:.4f}\")\n",
    "    \n",
    "    vad_iterator = VADIterator(model, sampling_rate=TARGET_SAMPLE_RATE)\n",
    "    chunks = []\n",
    "    current_chunk_start = 0\n",
    "    \n",
    "    for segment in speech_timestamps:\n",
    "        start, end = segment['start'], segment['end']\n",
    "        if (end - current_chunk_start) >= MIN_CHUNK_DURATION_SEC:\n",
    "            chunk_wav = wav[int(current_chunk_start * TARGET_SAMPLE_RATE):int(end * TARGET_SAMPLE_RATE)]\n",
    "            chunk_path = os.path.join(output_chunk_dir, f\"{len(chunks) + 1}.wav\")\n",
    "            save_audio(chunk_path, chunk_wav, sampling_rate=TARGET_SAMPLE_RATE)\n",
    "            chunks.append((current_chunk_start, end, chunk_wav))\n",
    "            current_chunk_start = end\n",
    "    \n",
    "    # Save the last chunk if necessary\n",
    "    if current_chunk_start < speech_timestamps[-1]['end']:\n",
    "        chunk_wav = wav[int(current_chunk_start * TARGET_SAMPLE_RATE):]\n",
    "        chunk_path = os.path.join(output_chunk_dir, f\"{len(chunks) + 1}.wav\")\n",
    "        save_audio(chunk_path, chunk_wav, sampling_rate=TARGET_SAMPLE_RATE)\n",
    "        chunks.append((current_chunk_start, speech_timestamps[-1]['end'], chunk_wav))\n",
    "    \n",
    "    vad_iterator.reset_states()\n",
    "    print(f\"Processed {audio_path}, saved chunks in {output_chunk_dir}\")\n",
    "\n",
    "def process_all_audio_files():\n",
    "    \"\"\"Process all .wav files in the main audio folder and save their chunks.\"\"\"\n",
    "    if not os.path.exists(AUDIO_DIR):\n",
    "        print(f\"Audio folder '{AUDIO_DIR}' does not exist.\")\n",
    "        return\n",
    "    \n",
    "    for file_name in sorted(os.listdir(AUDIO_DIR)):\n",
    "        if file_name.endswith(\".wav\"):\n",
    "            audio_path = os.path.join(AUDIO_DIR, file_name)\n",
    "            audio_id = os.path.splitext(file_name)[0]  # Extract the number without extension\n",
    "            output_chunk_dir = os.path.join(CHUNK_DIR, audio_id)\n",
    "            process_audio_file(audio_path, output_chunk_dir)\n",
    "\n",
    "# Process VAD on Audio Files.\n",
    "process_all_audio_files()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e7588d",
   "metadata": {},
   "source": [
    "## SRT to Sentences generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba7e8ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract combined sentences with timestamps from .srt file\n",
    "def extract_sentences_from_srt(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    sentences = []\n",
    "    timestamps = []\n",
    "    current_sentence = \"\"\n",
    "    current_timestamp = \"\"\n",
    "\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "\n",
    "        # Check for timestamp lines\n",
    "        timestamp_match = re.match(r'(\\d{2}:\\d{2}:\\d{2}[.,]\\d{3}) --> (\\d{2}:\\d{2}:\\d{2}[.,]\\d{3})', line)\n",
    "        if timestamp_match:\n",
    "            current_timestamp = timestamp_match.group(1).replace(',', '.') + ' --> ' + timestamp_match.group(2).replace(',', '.')\n",
    "            continue\n",
    "\n",
    "        # Skip empty lines and cue identifiers\n",
    "        if not line or line.isdigit():\n",
    "            continue\n",
    "\n",
    "        # Add line to current sentence\n",
    "        current_sentence += \" \" + line if current_sentence else line\n",
    "\n",
    "        # If sentence ends, save it\n",
    "        if re.search(r'[.!?]$', line):\n",
    "            sentences.append(current_sentence.strip())\n",
    "            timestamps.append(current_timestamp)\n",
    "            current_sentence = \"\"\n",
    "            current_timestamp = \"\"\n",
    "\n",
    "    return sentences, timestamps\n",
    "\n",
    "# Directory containing SRT files\n",
    "srt_directory = 'Data/SRT-Files'\n",
    "\n",
    "# Initialize lists for all sentences, timestamps, and filenames\n",
    "all_sentences = []\n",
    "all_timestamps = []\n",
    "all_filenames = []\n",
    "\n",
    "# Process all SRT files with tqdm\n",
    "srt_files = [f for f in os.listdir(srt_directory) if f.endswith('.srt')]\n",
    "for file_name in tqdm(srt_files, desc=\"Processing SRT files\"):\n",
    "    file_path = os.path.join(srt_directory, file_name)\n",
    "    sentences, timestamps = extract_sentences_from_srt(file_path)\n",
    "    all_sentences.extend(sentences)\n",
    "    all_timestamps.extend(timestamps)\n",
    "    all_filenames.extend([file_name] * len(sentences))  # associate each sentence with its file\n",
    "\n",
    "print(\"Encoding sentences into embeddings...\")\n",
    "# Uncomment for all miniLM and all mpnet.\n",
    "# sentence_embeddings = np.array(\n",
    "#     model.encode(all_sentences) \n",
    "# ).astype('float32')\n",
    "\n",
    "sentence_embeddings = model.encode(all_sentences)\n",
    "\n",
    "# Create FAISS index (use Inner Product for cosine similarity)\n",
    "embedding_dimension = sentence_embeddings.shape[1]\n",
    "faiss_index = faiss.IndexFlatIP(embedding_dimension)\n",
    "faiss_index.add(sentence_embeddings)\n",
    "\n",
    "# Save FAISS index\n",
    "faiss.write_index(faiss_index, \"Data/sentence_embeddings.index\")\n",
    "\n",
    "# Save metadata to file\n",
    "metadata_file = 'Data/srt-embedding-metadata.tsv'\n",
    "with open(metadata_file, 'w', encoding='utf-8') as file:\n",
    "    file.write(\"filename\\ttimestamp\\tsentence\\n\")\n",
    "    for fname, timestamp, sentence in zip(all_filenames, all_timestamps, all_sentences):\n",
    "        clean_sentence = sentence.replace('\\t', ' ').replace('\\n', ' ')\n",
    "        file.write(f\"{fname}\\t{timestamp}\\t{clean_sentence}\\n\")\n",
    "\n",
    "# Save sentences to a text file\n",
    "sentences_file = 'Data/sentences.txt'\n",
    "with open(sentences_file, 'w', encoding='utf-8') as file:\n",
    "    for sentence in all_sentences:\n",
    "        file.write(sentence.strip().replace('\\n', ' ') + '\\n')\n",
    "\n",
    "# Summary\n",
    "print(f\"\\nEmbeddings created for {len(all_sentences)} sentences from {len(srt_files)} SRT files.\")\n",
    "print(\"FAISS index saved as 'Data/sentence_embeddings.index'\")\n",
    "print(f\"Metadata saved as '{metadata_file}'\")\n",
    "print(f\"Sentences saved as '{sentences_file}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f406234c",
   "metadata": {},
   "source": [
    "## Finding Related Sentences to a Question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5016a7f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# Load sentences\n",
    "with open('Data/sentences.txt', 'r') as file:\n",
    "    lecture_sentences = [line.strip() for line in file if line.strip()]\n",
    "\n",
    "# Group sentences (sliding window)\n",
    "window_size = 3\n",
    "grouped_sentences = [\n",
    "    \" \".join(lecture_sentences[i:i + window_size])\n",
    "    for i in range(len(lecture_sentences) - window_size + 1)\n",
    "]\n",
    "\n",
    "# Encode grouped sentences\n",
    "group_embeddings = model.encode(grouped_sentences)\n",
    "faiss_index = faiss.IndexFlatL2(group_embeddings.shape[1])\n",
    "faiss_index.add(group_embeddings)\n",
    "\n",
    "faiss.write_index(faiss_index, \"Data/qwen-0.6B-grouped-sentences-embeddings.idx\")\n",
    "\n",
    "# Save grouped sentences and metadata (optional, for lookup)\n",
    "with open(\"Data/grouped_sentences.pkl\", \"wb\") as f:\n",
    "    pickle.dump(grouped_sentences, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "565b77c4",
   "metadata": {},
   "source": [
    "### Grouping Sentences N-gram technique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbdc7e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load grouped sentences\n",
    "with open(\"Data/grouped_sentences.pkl\", \"rb\") as f:\n",
    "    grouped_sentences = pickle.load(f)\n",
    "\n",
    "# Load metadata\n",
    "metadata_list = []\n",
    "with open(\"Data/srt-embedding-metadata.tsv\", \"r\", encoding=\"utf-8\") as f:\n",
    "    reader = csv.reader(f, delimiter='\\t')\n",
    "    header = next(reader)\n",
    "    for row in reader:\n",
    "        filename, timestamp, sentence = row\n",
    "        metadata_list.append((filename, timestamp, sentence))\n",
    "\n",
    "# Create mapping: grouped_sent -> (filename, start_time, end_time)\n",
    "grouped_sent_to_metadata = {}\n",
    "\n",
    "for grouped_sent in grouped_sentences:\n",
    "    matched_metadata = [(fn, ts) for fn, ts, sent in metadata_list if sent in grouped_sent]\n",
    "\n",
    "    if matched_metadata:\n",
    "        # Since your grouped sentences come from **one file only**, pick first filename\n",
    "        filename_set = set(fn for fn, ts in matched_metadata)\n",
    "        filename = filename_set.pop()  # should be only one\n",
    "\n",
    "        # Combine timestamps\n",
    "        timestamps = [ts for fn, ts in matched_metadata]\n",
    "        start_time = timestamps[0]\n",
    "        end_time = timestamps[-1]\n",
    "    else:\n",
    "        filename = \"Unknown\"\n",
    "        start_time = \"Unknown\"\n",
    "        end_time = \"Unknown\"\n",
    "\n",
    "    grouped_sent_to_metadata[grouped_sent] = (filename, start_time, end_time)\n",
    "\n",
    "# Save mapping\n",
    "with open(\"Data/grouped_sent_to_metadata.pkl\", \"wb\") as f:\n",
    "    pickle.dump(grouped_sent_to_metadata, f)\n",
    "\n",
    "print(\"Mapping file created successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "947e47fe",
   "metadata": {},
   "source": [
    "### QA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c19be7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What are Decision Trees?\n",
      "\n",
      "Top Related Sentences with Metadata:\n",
      "- [16.srt | 00:12:20.530 --> 00:12:21.400 → 00:04:38.839 --> 00:04:40.349] So let's have a look. So what is a decision tree? Let's take an example.\n",
      "\n",
      "- [2.srt | 00:00:44.700 --> 00:00:46.929 → 00:04:38.839 --> 00:04:40.349] So what is a decision tree? Let's take an example. These are the photos of two different kinds of mushroom and one of them is edible and the other one is deadly poisonous.\n",
      "\n",
      "- [14.srt | 00:08:37.470 --> 00:08:37.810 → 00:09:34.810 --> 00:09:35.080] All right. Hey everyone. In this video, we're going to talk about decision trees.\n",
      "\n",
      "- [17.srt | 00:00:32.740 --> 00:00:33.329 → 00:01:25.299 --> 00:01:31.549] The decision tree may look like this. Let's say we have different samples of mushroom data and then from this first node it's asking some criteria whether it's large or not. So let's say this one is large and then classify to large equals yes.\n",
      "\n",
      "- [16.srt | 00:00:05.129 --> 00:00:10.500 → 00:00:19.589 --> 00:00:25.250] Hey everyone, in this video we're going to talk about decision tree classifier and their split criteria. So decision tree classifier look exactly like decision tree regressor. This is a representation of the decision tree classifier in the HERT dataset.\n",
      "\n",
      "- [17.srt | 00:02:55.310 --> 00:02:58.530 → 00:03:11.150 --> 00:03:13.060] So it's a little bit fancier but essentially kind of the same. Decision trees, while they are easy and useful to understand, they have some drawbacks. They are very easy to overfit so we're gonna talk about some strategies to prevent overfitting.\n",
      "\n",
      "- [17.srt | 00:00:32.740 --> 00:00:33.329 → 00:02:27.670 --> 00:02:33.230] So decision tree works like this. It splits the samples from each node depending on their criteria. Let's talk about some terminology here.\n",
      "\n",
      "- [16.srt | 00:00:14.070 --> 00:00:18.789 → 00:09:10.969 --> 00:09:11.639] So decision tree classifier look exactly like decision tree regressor. This is a representation of the decision tree classifier in the HERT dataset. It's binary class classification, so at the end of the day in the terminal node we'll have a few samples.\n",
      "\n",
      "- [17.srt | 00:00:32.740 --> 00:00:33.329 → 00:03:52.809 --> 00:03:55.519] Decision tree similarly does not have parameters, however uses other metrics such as MSA for regression task and entropy or Gini for classification task. And they split nodes as we've seen before. So decision tree regressor works like this.\n",
      "\n",
      "- [17.srt | 00:02:44.379 --> 00:02:54.590 → 00:02:59.590 --> 00:03:06.039] So if you see more red and more blue it means that the node is more pure and if you see white node that means it's kind of 50-50 or very mixed there. So it's a little bit fancier but essentially kind of the same. Decision trees, while they are easy and useful to understand, they have some drawbacks.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "# Load FAISS index\n",
    "faiss_index = faiss.read_index(\"Data/qwen-0.6B-grouped-sentences-embeddings.idx\")\n",
    "\n",
    "# Load grouped sentences\n",
    "with open(\"Data/grouped_sentences.pkl\", \"rb\") as f:\n",
    "    grouped_sentences = pickle.load(f)\n",
    "\n",
    "# Load precomputed metadata mapping\n",
    "with open(\"Data/grouped_sent_to_metadata.pkl\", \"rb\") as f:\n",
    "    grouped_sent_to_metadata = pickle.load(f)\n",
    "\n",
    "# Encode question\n",
    "student_question = input(\"Enter your question: \")\n",
    "question_embedding = model.encode(student_question, prompt_name=\"query\")\n",
    "if question_embedding.ndim == 1:\n",
    "    question_embedding = np.expand_dims(question_embedding, axis=0)\n",
    "\n",
    "# Search top-10 results\n",
    "distances, indices = faiss_index.search(question_embedding, 10)\n",
    "\n",
    "# Retrieve grouped sentences + metadata directly\n",
    "related_results = []\n",
    "for idx in indices[0]:\n",
    "    grouped_sent = grouped_sentences[idx]\n",
    "    # Get filename and timestamps from precomputed mapping\n",
    "    filename, start_time, end_time = grouped_sent_to_metadata.get(\n",
    "        grouped_sent, (\"Unknown\", \"Unknown\", \"Unknown\")\n",
    "    )\n",
    "    timestamp_range = f\"{start_time} → {end_time}\"\n",
    "    related_results.append((filename, timestamp_range, grouped_sent))\n",
    "\n",
    "# Print Related Sentences\n",
    "print(\"Question:\", student_question)\n",
    "print(\"\\nTop Related Sentences with Metadata:\")\n",
    "for filename, timestamp, sent in related_results:\n",
    "    print(f\"- [{filename} | {timestamp}] {sent}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a53200b5",
   "metadata": {},
   "source": [
    "### Summarisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c24187d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUJAL IS NOT HERE\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Combine all top-K grouped sentences into one text\n",
    "all_text = \" \".join([sent for _, _, sent in related_results])\n",
    "print(all_text)\n",
    "\n",
    "print(\"SUJAL IS NOT HERE\")\n",
    "# Summarize across groups (num_sentences controls how many groups to pick)\n",
    "summary_text = summarizer(all_text, num_sentences=4)  # pick top 4 important groups\n",
    "print(\"SUJAL CODE IS WORKING\")\n",
    "# Map summary back to original grouped sentences to get metadata\n",
    "important_groups = []\n",
    "for filename, timestamp, grouped_sent in related_results:\n",
    "    # Check if this grouped sentence appears in the summary\n",
    "    if grouped_sent in summary_text:\n",
    "        important_groups.append((filename, timestamp, grouped_sent))\n",
    "\n",
    "# Print important groups with metadata\n",
    "print(\"\\nMost Important Groups with Metadata:\")\n",
    "for filename, timestamp, grouped_sent in important_groups:\n",
    "    print(f\"- [{filename} | {timestamp}] {grouped_sent}\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
