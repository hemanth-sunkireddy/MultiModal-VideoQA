Hello, welcome to the NPTEL online certification course on deep learning.
Now, we are discussing about the discriminant function and the decision boundary among different classes.
So, in the previous class we have considered two simple cases where the covariance matrices of the different classes they are same and in one of the case we have assumed that the covariance matrix is of the form sigma square i where sigma is the variance of all the components of the vectors.
And, i is an unit matrix which indicates that the covariance matrix is diagonal all the off diagonal elements are 0 and all the diagonal elements are same and that is a case of distribution of points where the points are spherically distributed or hyper spherically distributed.
And in the second case we have assumed that sigma or the covariance matrix of all the classes are same, but the covariance matrix need not be of the form sigma square i that means, i also have.
of diagonal elements which are nonzero.
And this is a case of distribution of points where the vectors are distributed in ellipsoidal fashion or hyper ellipsoidal fashion.
So, what we have discussed in the previous class is the decision boundary under various cases of covariance matrices.
And we are going to continue with the same discussion in this class with few more examples.
So, what we stopped in the previous lecture is .
matrix.
We have assumed the case where the covariance matrix sigma i is equal to sigma that means, the covariance matrix of all the classes are same, but here the covariance matrices need not be diagonal only.
And given this case we have in the previous class computed that g i x or the covariance the discriminant function of the ith class is given by w i transpose x plus w i naught where we have seen that this w i is of the form sigma inverse mu i and w i naught was simply given by minus half.
mu i transpose sigma inverse mu i plus log of p of omega i where p of omega i is the a priori probability of class omega i .
So, given this again as we have done in the other case I can compute the decision yeah.
So, here you find that because this g i x of the form w i transpose x plus w i naught.
Again this discriminant function is a linear function because I do not have any quadratic term in x in this expression.
So, the discriminant function here again is a linear function.
So, given such a discriminant functions as we have computed in the previous case, I can compute the decision boundary between the two classes omega i and omega j.
So, what will be the decision boundary?
The decision boundary here again will be given by g x is equal to g i x minus g j x and because on the boundary g i x and g j x they are equal.
So, I will have g i x minus g j x equal to 0 and you find that what was our expression for g i x g i x.
was of the form sigma i mu i sigma mu i transpose sigma inverse x minus half mu i transpose sigma inverse mu i plus log of p of i.
Similarly, when I compute g j x, g j x will be mu j transpose sigma inverse x minus half of mu j transpose sigma inverse mu j plus log of p of omega j.
So, if I simply subtract this g i x minus g j x and equate that to 0, my expression will simply become 1, when I compute.
here g x which is nothing but g i x minus g j x equal to 0 that simply becomes mu i transpose sigma inverse minus mu j transpose sigma inverse x of this.
minus half of mu i transpose sigma inverse mu i minus mu j transpose sigma inverse mu j plus log of p omega i upon p omega j that will be equal to 0 ok. And if you simplify in the same way that you have done earlier, you find that this will lead to an equation of the form W transpose X minus X naught which is equal to 0, where this W in this case will be given by sigma inverse mu i.
minus mu j and x naught will be given by half of mu i plus mu j minus 1 upon mu i minus mu j transpose sigma inverse mu i minus.
mu j here it will be log of p omega i upon p omega j into mu i minus mu j .
So, what does it indicate?
It indicates as before that because the equation of the boundary the decision boundary between the classes omega 1 and omega j .
is of the form W transpose X minus X naught equal to 0.
And if you remember this form is similar to what we have obtained in the previous case where the covariance matrix was a diagonal matrix for all the classes.
But what is the difference?
In the previous case this W was simply mu i minus mu j that means, W was the vector drawn from mu i to mu j.
But in this case W is sigma inverse mu i and j.
mu i minus mu j.
So, which means that W is no longer in the direction of the vector from mu i to mu j, but the direction of W depends upon the covariance matrix sigma because the expression is sigma inverse mu i minus mu j. X naught as before if I assume P omega i and P omega j to be equal that is the classes to be equally a priori.
In that case sigma naught as before becomes half of mu i plus mu j, ok.
So, this expression simply becomes again my decision surface the decision boundary between the classes omega i and omega j is orthogonal to W, but unlike in the previous case it is not orthogonal to the line joining mu i and mu j, right.
So, it is orthogonal to W, but under the situation that p omega i and p omega j will to be equal.
.
The line becomes a bisector of the line the decision surface becomes a bisector of the line joining mu i and mu j because when p omega i and p omega j they are equal x naught is half of mu i plus mu j.
So, it is half way between mu i and mu j.
So, my decision surface is a bisector of the line joining mu i and mu j, but it may not be an orthogonal bisector because W is no longer .
in the direction of the line joining mu i and mu j in general.
So, let us see that what will be the nature of the decision surface in this particular case.
So, again for that I take a number of examples number of feature vectors from class omega 1 and from class omega 2.
So, in this case the feature vectors that I am taking from class omega 1 are all these feature vectors which are marked in blue.
So, these are the feature vectors that I am taking from class omega 1 which are 6 2 9 3 7 5 and 10 6.
Similarly, all these feature vectors which are in red they are taken from class omega 2 the feature vectors are 6 11 9 12 7 14 and 10 15.
So, here you find that unlike in the previous case where the set of feature vectors were spherically distributed in this case they are elliptical.
elliptically distributed, they are not spherical distribution anymore.
So, given this feature vectors now let us see that how we can find out the decision surface.
So, for these two sets of feature vectors I compute the mean vectors mu 1 for class omega 1 and also I compute mu 2 for class omega 2.
Again there is a mistake the second one.
1 here this mu 1 this actually should be mu 2 this is not mu 1, but it is mu 2.
So, mu 1 is equal to 8 4 and mu 2 is equal to 8 13 they are the mean vectors of the feature vectors taken from class omega 1 and class omega 2.
So, once I have this then again as before I can compute the covariance matrices.
So, as we have computed in the previouslecture in the same manner if I compute the covariance matrix, you will find that a covariance matrix for both the classes omega 1 and omega 2 will be half into 5, 3, 3, 5.
So, this is the covariance matrix that you get for both the classes omega 1 and omega 2.
And so, this is a case where my off diagonal elements.
they are not 0 anymore.
So, unlike in the previous case where I had the covariance matrix which was completely a diagonal matrix and all the diagonal elements were equal in this case I have off diagonal elements which are non 0.
So, that means, the feature components the different components are not statistically independent anymore.
However, for both the classes I have the same covariance matrix that is half 5, 3, 3, 5.
So, this comes under the case sigma.
i is equal to sigma.
So, once I have this covariance matrix I can compute sigma inverse that is the covariance matrix inverse ok. And using this as we have seen earlier that I can compute thedecision surface and the decision surface is given by W transpose X minus X naught equal to 0.
where W is sigma inverse mu 2 minus mu 1.
And you remember the sigma inverse that you have computed was 1 by 8 5 minus 3 minus 3 5 and mu 2 minus mu 1 is nothing but 0 1 ok.
So, I get W which is minus 3 5.
So, that simply says that W is in the direction of minus 3 5 W will be in this particular direction.
So, here you find that this dark green line this line represents the direction of w ok. And given this I can also compute what is x naught as x naught is given by minus half x naught is given by half mu 1 plus mu 2 minus this.
And under the situation if I assume that p omega 1 and p omega 2 to be equal log of p omega 1.
upon p omega 2 that will be equal to 0.
That means, this term will be cancelled giving X naught to be half of mu 1 plus mu 2.
So, that simply says that our decision boundary which will be orthogonal to sigma inverse mu 2 minus mu 1 or mu 1 minus mu 2 and it will pass through the midpoint between mu 1 and mu 2.
So, this dark blue line is the line joining mu 1 and mu 2.
dark green line is sigma inverse mu 1 and mu 1 minus mu 2 and this dotted red line is the line which is orthogonal to this dark green line and it passes through x naught where x naught is half of mu 1 plus mu 2.
So, this is mu 1 and this is mu 2 ok.
So, here you find that the decision boundary in this case is also linear.
but the decision boundary is no longer orthogonal to the line joining mu 1 and mu 2.
So, this was our second case where sigma i was equal to sigma .
Now, let us consider that what will be the discriminant function and the decision boundary in the most general case where I have sigma i to be the most general one that is for different classes I can have different sigma y .
It is possible that for some of the classes the sigma i will be orthogonal, but some of the some other classessigma i can be diagonal for, but for some other classes it may not be diagonal.
So, every class or the sample vectors taken from every class they have their own covariance matrix sigma i.
So, to see that what will be the nature of the discriminant function in such case let us go back to the original.
g i x where we have seen that g i x was minus d by 2 log of 2 pi minus half log of determinant sigma i minus half x minus mu i transpose sigma i inverse.
x minus mu i plus log of p omega i .
So, here you find that as before I can remove this term from g i x because this is same for all values of i whereas, in the earlier cases we had ignored this term as well half log determinant sigma i because sigma i was same for all the classes.
Now, sigma i is not same for all the classes, it is different for different classes.
So, I cannot ignore this term anymore.
So, that gives me that g i x will be simply minus half x minus mu i transpose sigma inverse x minus mu i plus log of p of omega i.
minus d by 2 log of mod of determinants in i right .
So, given this you will find that this expression is of the form x transpose a i x plus b i transpose x plus b i.
C i. I can simplify this expression in this form x transpose a i x plus b i transpose x plus c i.
And now, we find that because of the presence of the term x transpose x, I will have if the components are x 1, x 2, x 3 and so on of the feature vector x, I will have terms x 1 square, I will have terms x 2 square, I will have term x 3 square, I will have term x 1 into x 2, x 1 into x 3, x 2 into x 3, x 2 into x 5 and so on.
So, that leads to a situation that the discriminant function g i x does not remain linear anymore, but it becomes a quadratic discriminant function because of the presence of quadratic terms.
And over here, this a i it will be simply minus half sigma i.
I will be sigma I inverse mu I and C I will be minus half mu I transpose sigma I inverse mu I minus half log of p of omega i plus sorry this is minus half log of determinant sigma i plus log of p of omega i.
So, C i will be half mu i transpose sigma inverse mu i minus half log of determinant sigma i plus log of p of omega i.
So, my discriminant function becomes a non-linear or quadratic discriminant function.
And when I have such non-linear discriminant functions for to find out the decision boundary between two classesomega i and omega j.
Now, it is difficult to simplify as we have done in the previous case unlike in the previous case.
So, now, what I have to do is I have to find out what is g i x which will be as we have seen it will be x transpose a i x plus b i transpose x plus c i.
Similarly, I have to compute what is g j x which will be x transpose a j x plus b j transpose x plus c j and to compute g x which is the decision boundary between the two classes I have to compute g j x which is nothing but g i x minus g j x and I have to equate this to equal to 0.
So, whatever expression I get that will be the decision boundary between the classes omega i and omega j.
So, now let us see what will be the decision boundary in thiskind of scenario .
So, for this again I take a set of feature vectors .
So, a set of feature vectors say 9 10, 9 14, 7 12 and 11 12 which are taken from class omega 2 that is this one and I have a set of feature vectors 5 4, 9 2, 9 6 and 13 6 .
are taken from class omega 1 and the feature vectors belonging to class omega 1 are these ones right.
So, as before as we have done before given these feature vectors I can compute what is mu 1 and what is mu 2 that is the mean of feature vectors taken from class omega 1 and mean of feature vectors taken from class omega 2.
And I can also compute the covariance matrix sigma 1 and sigma 2 for these two different classes.
So, over here my mu 1 that is the mean of the feature vectors from class omega 1 will be 12 6 and mu 2 will be 9 4 and the covariance matrix sigma 1 is 2 0 0 2.
So, you find that this 2 0 0 2 is the covariance matrix for this feature vectors I think as we have doing doing earlier this sigma 2 sigma 1 should actually be sigma 2 anyway that does not matter much.
And for the other class your sigma or the coviance matrix is 8 0 0 2 which is the coviance matrix over here right.
So, you find that in the first case in this case the points are spherically distributed and here the coviance matrix is of the form 2 0 0 2 which is nothing but of the form sigma square i.
So, the points are spherically distributed whereas, in the second case where my covariance matrix sigma 2 is 8 0 0 2 it is elliptically distributed and because the variance of the component x 1 component is more than the variance of the x 2 component.
So, obviously, the spread in x 1 direction is more than the spread in x 2 direction and that makes it elliptical.
So, given this two set of feature points feature vectors.
Now, as I find that the covariance matrix for the two classes are different.
So, mydiscriminate function for the two classes for the second class where covariance matrix is 8 0 0 2 will be a quadratic one.
Whereas, the discriminant function for the first case where sigma 1 that is this one because it is of the form sigma square i.
the cov the discriminant function for this class will be linear.
So, over here my cov discriminant function will be quadratic for this the discriminant function will be linear.
So, to find out the decision surface between the two classes over here I simply have to make g i x minus g j x and equate that to 0.
And after equating that to 0 whatever I get that becomes 0.
the decision surface.
So, you find that as we have done before that this is the expression of the covariance of the discriminant function or the covariance matrix is not of the form i square i right and the discriminant function is a quadratic discriminant function and given this particular situation if I try to find out that what will be the decision boundary between the two classes.
So, as we said that For this set of points the discriminant function will be linear one, for this set of points the discriminant function will be a quadratic one and if I if I call it say g 1 x and if I call it g 2 x the discriminant functions the equation of the boundary will be given by g 1 x minus g 2.
x that will be equal to 0.
And if you plot the decision boundary you will find that the decision boundary will have a shape something like this which is not a linear decision boundary anymore that we have obtained earlier, but in this case the decision boundary will be a non-linear decision boundary or a quadratic decision boundary well.
So, with this I come to the end of this lecture.
So, in this lecture and the previous lectures 2 lectures what we have tried to do is we have tried to find out the discriminant functions of different classes assuming that the distribution of points is multivariate normal distribution.
And there we have taken 3 different cases.
In the first case we have assumed that the covariance matrices for all the classes are same and they are of the form sigma square i and which case we have obtained the discriminant functions to be linear.
And, not only that the decision boundary between the different classes they are also linear and under the situation when the a priori probabilities are same we have seen that decision boundary is orthogonal bisector of the line joining mu 1 and mu 2.
In the second case we have assumed that the covariance matrices for different classes are same, but they may not be of simple form sigma square i.
In that case also the discriminant functions we have found to be linear.
Decision boundary was also linear, decision boundary was a bisector of the line joining mu 1 and mu 2 under the situation when a priori probabilities are same, but the decision boundary in general was not orthogonal to the line joining mu 1 and mu 2 because of the presence of the term sigma inverse.
So, the direction of the decision boundary in that case depends upon the covariance matrix.
And, the third case was the more general case where we assumed that the covariance matrix of all the classes are different and in which case we have found that the discriminant function is not linear anymore, the discriminant function is a quadratic discriminant function.
And, given such quadratic distribution the decision boundary to be the discriminant function to be a quadratic one using that if I try to find out .
the separating boundary between the two classes, the separating boundary in general is quadratic, it is not linear anymore.
I will stop here today.
Thank you very much.
Hello, welcome to the NPTEL online certification course on deep learning.
You remember in the previous class we have talked about the discriminant functions and we have also seen the decision boundaries.
So, we when we discussed about the discriminant function and then the decision boundary, we have assumed the vectors representing the objects that follow certain probability density function or certain distribution.
And the distribution that we have assumed in this case.
was a normal distribution.
So, it was a multivariate normal distribution.
And based on that based using this multivariate normal distribution under different assumptions of the covariance matrix we have seen that we can have the discriminant functions which are linear, we can have discriminant functions which are quadratic.
And accordingly when we try to compute the decision boundary between the vectors or the patterns belonging to two different classes the boundary can be either a linear boundary.
or the boundary can be a quadratic boundary that depends upon what type of covariance matrix the distribution exhibits.
Now, today we will talk about the linear classifier and we will alsotalk about the support vector machine.
Before talking about the linear classifier and the support vector machine, we will briefly touch upon another two different types of classifiers which are nearest neighbor classifier.
and k nearest neighbor or k n n classifier.
So, let us first talk about what is nearest neighbor classifier or nearest neighbor rule.
So, before going to that you find that in previous few lectures when we talked about the discriminant function which leads to the decision boundary between the two different classes in a particular case that when the covariance matrix of.
all the different classes are of the form sigma square i.
That means, in this case the different components of the vectors were statistically independent and all the components have same variance which is equal to sigma square.
So, there the covariance matrix for all the classes are same and which is in the form sigma square i where this i is an unity matrix.
Under this And, when the a priori probability of the classes P omega i was equal to P of omega j, there we found that the decision boundary between the classes omega i and omega j was a linear boundary.
And, not only that it was a perpendicular or orthogonal bisector of the vector of the line joining the mean points mu i and mu j.
So, if this is mu i, mu j is and this is mu j then the line joining mu i and mu j is bisected orthogonally by the decision boundary between these two classes.
So, this was one of the case in which I can separate two classes by a linear boundary.
And of course, in this case if p omega i is greater than p omega j then this decision boundary is shifted towards mu j.
in the sense that for unknown feature vectors or decision will be biased towards omega i because p of omega i the probability p of omega i is greater than p of omega j.
In the other case if p of omega i is less than p of omega j in that case the decision boundary shifts towards omega i it remains orthogonal to the line joining omega i and omega j, but it shifts towards .
omega i indicating that our decision for unknown feature vectors will be biased in favor of class omega j.
So, particularly in this case when p omega i is equal to p omega j then my decision boundary is orthogonal bisector of the line joining mu i and mu j which clearly indicates that if i have an unknown feature vector say x over here which i need to classify.
This is the mean of the vectors belonging to class omega i and this is mu j which is mean of the vectors belonging to class omega j.
And here we are assuming that the a priori probabilities p omega i and p omega j they are equal that means the classes are equally probable .
So, under this situation when I have this unknown feature vector x over here which have to be classified what I am effectively doing is because this x falls on this side of the boundary .
So, it is to be classified to class omega i or in other words what I am doing is I am trying to compute the distance between mu i and x and I am also computing the distance between mu j and x.
And here the distance between mu i and x which if I represent as d of x mu i and in other case the distance between x and mu j is d of x mu j, you find that d of x mu i is less than d of x mu j.
And that is true for any point x lying on this side of the boundary.
And all these cases this vector will be classified to omega i.
So, in other words the classification rule that I am applying is a minimum distance classification rule where the distance that you are computing is the Euclidean distance between the unknown vector x and the means of the classes.
Let us see the other case where we have assumed that mu i is of the form mu.
That means, the covariance matrix of all the classes are same, but the components of the feature vectors may not be statistically independent.
That means, the off diagonal elements of the covariance matrix may be nonzero.
So, under that situation again under the assumption of equal a priori probability that is p of omega i is equal to p of omega j, we found the decision boundary is bisector of the line joining mu i and mu j, but the decision boundary is no longer orthogonal to the line joining mu i and mu j.
So, it is a bisector, but it may not be orthogonal .
So, what do we do in this case?
Again here if I have an unknown point x on this side, this unknown point x will be classified to class omega i because it is falling on the side of mu i.
Is it a minimum distance classifier?
Yes, again in this case it is a minimum distance classifier because what we are computing is .
x minus mu i transpose sigma inverse x minus mu i and I am also computing x minus mu j transpose sigma inverse x minus mu j .
So, if x minus mu i transpose x sigma inverse x minus mu i this is less .
than X minus mu j transpose sigma inverse X minus mu j, then the point X will be classified to class omega i.
And you find that this is also a distance measure, but it is not Euclidean distance anymore, but this distance is what is known as Mahalanubis distance ok. And you find that, when this covariance matrix sigma is of the form sigma square i and assuming that sigma square is equal to 1, this Mahalanobis distance will be same as Euclidean distance.
So, even in this case where the covariance matrix sigma i of all the classes is same, but the feature components may not be statistically independent, I still get a minimum distance classifier, but the distance that you compute in this case is not.
Euclidean distance, but the distance that you have to compute is Mahalanovitch distance ok.
So, with this background now let me talk about what is meant by nearest neighbor rule or nearest neighbor classification.
So, as we said before that every object or every signal is represented by a vector or feature vectors whichever way the vectors are computed, it may be computed using some signal processing techniques.
Or given an image, I can simply represent this image as a vector by simply concatenating the columns of the image.
So, if I have an image of size say m by n, I will represent this by a vector having m into n number of components.
So, that becomes that an m into n dimensional vector.
So, once I represent these as vectors that means, the signal and image or whatever is represented by a point in that vector space or feature space.
So, as it is an m into n dimensional vector, so I am defining an m into n dimensional feature space and every image will be represented by a point in that m into n dimensional feature space.
But here I am taking a simplistic view because I cannot represent an m into n dimensional space on a two dimensional plane.
So, what I am doing is I am projecting them into two dimensional space.
So, each of these images that you find in this particular plane they are nothing, but different vectors.
So, this image is a vector, this image is a vector, I am simply projecting them into two dimensional space x 1 x 2 assuming that x 1 x 2 are the feature vectors.
Now, given this let us see what is a nearest neighbor rule.
So, here all these images arethe known images, I know what is the class from which this image comes.
So, for example, I know I have a lot of images which are birds, I have a lot of images which are cars, I have lot of images which are dogs ok. Now, given an unknown image this I have to classify or I have to identify what this image is.
You know that all those previous images that we had those are the training images using which I have to classify this unknown image.
So, one of the approach in which this image can be classified is what I do is.
I simply take the distance of this unknown image or the vector representing this unknown image from all other images which are known.
And once I compute this vector, so if I have say p number of previously known images and I have this unknown image which I want to identify.
So, I have to compute p number of distances, distance from every other image which we have in my knowledge base.
right.
And once I do that after that I find out that what is which is the image which is nearest to it.
So, if you go back to the previous one you find that probably this is the one which is nearest to this unknown image.
So, my classification rule is that whichever image is nearest to it I identify this unknown image to be the image corresponding to that class.
So, over here as this was.
the nearest image.
So, I identify this unknown image to be one of these images.
And obviously, in this case you find that your classification is not correct because the unknown image that I had was the image of a car whereas, my nearest neighbor rule has said that it is the image of a dog which is obviously, incorrect.
So, what is the problem in this case?
The problem is I am trying to find out which is the nearest known image in my knowledge base that is nearest to this unknown image.
So, I am taking my decision based on that nearest image.
So, my decision is based on only one vector and if that vector is an outlier then obviously, my decision is going to be wrong and that is what has happened in this case.
So, an alternative is that instead of considering only one image, if I consider multiple number of images.
So, what I will do is I will take multiple number of images.
So, if I take k number of nearest images then it becomes a k nearest neighbor rule.
So, in k nearest neighbor rule what you do is I have as before all these different vectors or images in my feature space again I have this unknown image I compute the distances from all the known images and out of that I consider only few or k number of images which are nearest.
And then among this k number of images I compute a vote that is whichever class of images is in major majority in that k number of images I consider this unknown image to be classified to that corresponding class.
So, in this particular case you find that the out of all these images which are nearest.
So, all these images.
which were nearest to this unknown image.
I have two images which belong to bird, I have two images which belong to dog, but I have 1, 2, 3 and 4, 4 images of cars.
So, this car images are in majority.
So, I classify this unknown image to be one of the car images and my classification in this case is correct.
So, K-nearest timber rule in general, it is gives better result than nearest neighbor rule because you are taking decision based on multiple number of images or multiple number of feature vectors which takes care of the outlets removal of outlets.
So, this nearest neighbor rules are very simple to apply, but what is the drawback?
The drawback is usually in machine learning applications I have lakhs and lakhs of images given for training.
So, whether I want to use nearest neighbor or k nearest neighbor, I have to save all those images in the memory.
And while taking decision for an unknown image or for an unknown vector, I have to compute all those lot of distance values.
And based on the distance values, I have to take a decision that to which class this unknownimage should be classified.
And that is obviously computationally extensive.
So, the simplest approach is that instead of trying to do this you try to find out the decision boundaries which we have also seen earlier.
So, given a number of classes the given images belonging to two different classes if I can compute a decision boundary between the two classes.
Then for an unknown image if that image falls on one side of the boundary it will be classified to say class omega i if it falls on other side of the boundary then it will be classified to class omega j.
And, for that once that boundary is computed I can discard all those training images or the training vectors.
What I need to stored is simply few number of parameters which identifies or which describes that boundary.
So, let us see how this can be done.
So, let us assume in this case take a let us consider this case that all these feature vectors they belong to one class and these are the feature vectors.
which belong to another class.
So, had we used a nearest neighbor rule and given an unknown feature vector over here in order to classify this unknown feature vector I had to compute the distances from all these feature vectors which are known.
And then based on these distance values I had to classify this unknown feature vector to either this class say omega 1 or this class say omega 2.
But now what I am doing is instead ofcomputing those distances or saving all these feature vectors in computer memory, I am trying to find out a boundary between these two classes.
So, assuming that all these feature vectors are linearly separable, I can separate these two classes of feature vectors by a linear boundary.
So, in a two dimensional case, it will be a straight line, in a three dimensional case, it will be a plane in case it will be hyper plane.
So, given such a linear boundary in this two dimensional case you find that the equation of this boundary will be something like a x 1 plus b x 2 plus c is equal to 0.
So, the parameters representing this particular straight line are the parameters a b and c where we know that a vector a b is orthogonal to this separating line to the line and c represents the position of the line in that is two dimensional space.
So, if I vary a and b in that case the orientation of the line will be different and if I vary c in that case the position of the line will be different.
So, this is what we have in case.
of a two dimensional space.
In case of a multi dimensional feature vector the equation will be a x 1 plus b x 2 plus c x 3 plus something like k that will be equal to 0.
So, if I have d dimensional feature vectors I can write this in the form w i x i where i varies from 1 to d plus w 0 that equal to 0.
So, you remember that this equation is similar to what we had derived earlier the decision boundary between two different classes usingthe normal multivariate distribution which was w transpose x plus w naught that equal to 0.
So, this is same as this when this W transpose X is expanded component wise.
So, this is the expression that I get.
So, this is the equation of the this straight line which or the plane which separatesthe feature vectors belonging to two classes omega 1 and omega 2 or omega i and omega j.
So, given this now I can move further.
So, what I need to do is given a set of feature vectors belonging to two different classes and assuming that the feature vectors are linearly separable, I have to find out a separating plane or hyper plane which separates the feature vectors belonging to these two different classes.
And the equation of that separating hyper plane is simply of the form W transpose x plus w naught is equal to 0.
And you remember that such a plane divides the feature space into two half spaces, one of the half space is positive and the other half space is negative.
So, if we say that the feature vectors taken from class omega 1, they belong to the positive half space and the feature vectors taken from class omega 2, they fall in the negative half space.
And, using this once I design such a classifier or the such a separating plane, then for an unknown feature vector say y, if w transpose y plus w naught becomes greater than 0, then we take a decision that w should belong to class omega 1 because y is sorry this is y naught x.
that, because y is falling on the positive half space.
Whereas, if we find that w transpose y plus w naught becomes negative, it is less than 0, then our decision will be that y should belong to class omega 2.
So, what we have is for all the known samples of the training samples, I if the training sample is taken from class omega 1, I should have w transpose x.
plus W naught greater than 0, when X belongs to class omega 1 and W transpose X plus W naught should be less than 0 when X belongs to class omega 2.
So, whichever W weight vectors W and the bias term W naught I choose that must satisfy these relations, these inequalities for the samples taken from class omega 1 and the samples taken from class omega 2.
Now, we find that this particular expression w transpose x plus w naught, I can put this in an unified representation in the form that I can write this as a transpose y.
How I can do it?
I have to append an additional dimension, I have to append an any additional dimension into this x.
So, if x is of dimension d.
I have to append a 1 to this dimension to this X and make it of dimensional d plus 1 .
So, the way it is done is suppose it is a d dimensional vector.
So, this w transpose t w is also a d dimensional vector.
So, I have w 1 w 2 up to w d this multiplied by X 1 X 2 up to X d .
So, this is the expression which gives me w transpose x plus w naught this equating to 0 gives me the equation of the separating plane.
Now, this equation I can rewrite in the form w 1, w 2, w d, w 0, x 1, x 2, x 3 .
1 this equal to 0.
So, I can call this as a and this as y right.
So, my equation simply becomes equation of the plane as a transpose y equal to 0 where a is w appended by w naught and y is vector x appended by and additional component which is equal to 1.
So, I can represent that equation W transpose X plus W naught equal to 0 as W transpose Y equal to 0.
And as before if Y belongs to class omega 1, I have to have A transpose Y greater than 0.
If Y belongs to class omega 2, I have to have A transpose Y less than 0.
What I can do is for all the Y, I can write which are taken from class omega 2, I simply negate them.
So, for them instead of considering y if I consider minus y for all y which are taken from class omega 2.
And if I do this then if y is correctly classified by a I will have simply a transpose y greater than 0, where y if it is taken from class omega 2 it is negated y.
So, given this I have now uniform criteria of correct classification that a transpose y has to be greater than 0 for all the training samples y whether they are taken from class omega 1 or that again from class omega 2 because for all the y's which are taken from class omega 2 they have been negated.
So, my design approach can be that I can start with any a arbitrarily, but if I find so with this a I test all the training samples.
So, as long as for every training sample A transpose Y remains greater than 0, I know this A correctly classifies all those training samples.
But for if for any training sample, I find that A transpose Y becomes less than 0, then I know that this Y has not been correctly classified or it has been misclassified by this vector A.
So, what I have to do is, I in this particular case this a has to be modified.
So, in order to do this what I can do is I can compute some sort of error terms that is for every vector which is misclassified I will compute an error.
So, the error term because y will be misclassified if a transpose y is less than 0 I can simply compute the error term as minus a transpose y.
So, whenever a transpose y is less than 0 minus a transpose y will have a positive positive term.
So, if I check all the feature vectors all the training vectors which are misclassified by this a I collect all of them compute minus a transpose y for all those feature vectors which are misclassified and take the sum of all of them right.
So, I compute an error say g.
y which is equal to minus a transpose y for all y which are misclassified and j will be equal to 0 if all the samples are correctly classified.
So, you find that if I have the misclassified samples this sample this sum sum of minus a transpose y is going to be positive.
And, if all the samples are correctly classified all the vectors are correctly classified then the value of j will be equal to 0.
So, while designing this a or while designing the boundary between the linear boundary between two different classes or approach should be that we should be able to or we should try to reduce this error term j ok. And, for that the kind of approach that can be taken is.
So, something like this .
So, I have this a I will put an initial a to be equal to 0 ok and then gradually in the kth step I should be able to a get a k from a of k minus 1 that is the previous value of a.
a such that I move from a k plus 1 to a k minus 1 to a k in such a way that this movement will try to reduce the error j a if I represent the error as a function of a that when I move from j a k minus 1 to a k the error a should be reduced.
So, we will talk about this more in our next lecture.
Thank you.
Hello, welcome to the NPTEL online certification course on deep learning.
You remember in the previous class we have started our discussion on optimization techniques and we have talked about the differentoptimizationtechniques like stochastic gradient descent, we have talked about batch optimization and we have also talked about mini batch optimization.
In today's lecture we are going to talk about optimization in machine learning in particular that how optimization in machine learning applications differs from general optimization problems.
We will also talk about linear and logistic regression, softmax classifier and we will also talk about the nonlinearity, how nonlinearity is important in case ofdeep learning or machine learning applications.
So, let us first talk about how machine learning optimization in machine learning is different from general optimization techniques.
So, what you do?
in case of optimization.
The goal of optimization is to reduce a cost function given by say J w where w is the parameters which defines your machine learning algorithm.
So, you optimize this cost function J w or minimize the cost function J w in order to optimize some performance measure of your learning algorithm or machine learning model.
So, what is the performance measure in case of machine learning?
that we have discussed so far.
The problems that we have considered are mainly categorization or classification problems that is given objects or signals belonging to different categories.
We try to identify that which signal or which object belongs to which which kind of category.
So, if your categorization or classification is correct there is no error, if the classification is incorrect you incur some error.
So, the performance measure with respect to machine learning is how accurate your classification or categorization problem is decision is.
And in order to do that you minimize a cost function J w which isthis cost function J w iscomprises of the loss that you incur during classification of the training data.
Now, if you talk about the pure optimization problem.
In case of pure optimization problem theoptimization criteria the cost function j is minimized and that is the goal in and of itself.
In other sense suppose you are given a set of observations in two dimension that is I may have x y pairs where I assume that x is an independent variable and y is a dependent variable.
And, if there is sufficient reason to believe that the relation between x and y is linear, then given a set of observations what we would like to do is we would like to fit a minimum error state line passing through the set of observed data and that is the M. And, when you try to find out this state line which minimizes the error actually you go forminimization of sum of squared error.
And, once the line is fit your goal issatisfied.
But, when you talk about machine learning in case of machine learning we minimize the loss function or cost function J wwhere w is the parameters of your machine learning algorithm or the machine learning model.
But, this minimization is done on the training data or the training samples or in other words what you try to minimize.
is the training error.
But what is our actual aim?
Our actual aim is not to classify the training data because for the training data the classes are already known.
So, what is the great thing about classifying the data which are known already.
So, our aim is not to class correctly classify the training data, but our aim is that the machine that you have trained using the training data or the level data that same algorithm or the machine has to be used for classification of unknown or unforeseen data.
data which the machine has not seen before.
And for this unknown data theclass belongingness is not really known because if it is known then I do not have to classify.
So, for this what my aim is that though I am training the machine or minimizing the error on the training data, but my aim is that the same machine has to perform well on the actual data or the test data.
So, I want that not only the training error to be minimized, but also the test error is to be minimized.
So, how do you know the test error?
Because as I said that for your real life classification problem that class belongingness of the data is not known.
So, what you do is you set aside a set of level data which are known as test data for which the classes are known, but you do not use that set of data or test set for training the machine.
So, in that case I can compute what is the error given by the machine while testing, but while training you use one set of data which is training set for testing you set another set of data which is test set.
But again when you come to actual application when you want to deploy this machine learning model machine learning algorithm for classification or recognition there the class belongingness of the data is not really known.
So, accordingly you incorporate some test error when you are testing the machine on the test.
test set of data which are not actually used for training purpose.
And while doing so you incorporate some error or some loss which are the test which is known as the test error or generalization error.
So, in case of machine learning algorithms or optimization in case of machine learning though I am optimizing the machine on the training data, but my aim is that the same machine should perform well on the test data as well as on the real life data.
So, that is the basic difference between a machine learning algorithm or optimization in case of machine learning algorithm.
So, naturally because the test data has not been used for training the machine, how do we really know or how do we really guarantee that the machine or algorithm which has been trained on the training data will also perform well or give?
error while testing on the test data or it will also give minimum error when you actually deploy the machine for real life classification problem.
So, truly speaking this cannot be guaranteed, but under certain assumptions we cansay with certain degree ofconfidence that the machine will also perform well on the test data.
And for that we make some.
assumptionspopularly known as IID assumptions.
So, what you assume is that both of the training data and the test data they are generated by a probability distribution which is known as the data generation process or data generation model.
We also assume that the data samples in each set in data set are independent that is one sample or one vector is independent of the other vector.
And, we also assume that the training set and test set are identically distributed.
So, when you talk about the data distribution, the distribution of the training set of data and the distribution of the test set of data they are identical and that is what is known as IID assumption that is independent identically distributed set of data.
Similarly the performance of a machine learning algorithm.
is measured by its abilityto perform two tasks.
One of the task is that it has to make the training error small that is while training the machine on the training set of data, the training error that you incorporate that should be small and only when it is very small ideally it should be 0, we assume that the machine or your algorithm is properly trained or it has learned the classes.
And, secondly when you test it so, while testing on the test data and as we said before that the test data is not used during training the machine though we know what are the class belongingness of the test data.
So, once the machine is trained on the training data you use the same machine on the test data.
And, in case of test data ideally we want that the error should be minimum or the error should be 0 in ideal cases, but practically that may not be possible.
So, while applying the same machine on the test data we get some test error.
So, again the performance of the machine learning algorithm is measured in terms of what is the gap between the between the training error and the test error.
So, we always expect that whatever is the training error the test error should also be similar that means, the gap between the training error and the test error should be as small as possible.
So, these are certain assumptions and the under those assumptions we talk about how the machine learning algorithm actually perform.
Now, these assumptions and the performance measures that lead to two very importantproblems to challenging problems, one of them is known as underfitting problem and the other one is known as overfitting problem.
So, what is underfitting problem?
Underfitting problem is when yourmodel or the machine is not able to obtain sufficiently low training error, that means, it is not performing well.
on the training data itself.
So, the training error is notacceptably low.
And as we said that in ideal cases we expect this training error should be 0.
That means, all the training samples which are given for training the machine they should be correctly classified and then only we get the training error to be 0.
And the overfitting says that the gap between training and test error is too large.
So, there may be cases.
that while training we have been able to design a machine or a model where the training error is minimum, but the same machine when it is applied on the test data the test data is quite large that means, there is a gap between the training error and the test error.
So, overfitting says that your model has been able to capture even the minute differences in the test data and while doing so.
the model is so tuned to the test data that it is not it has lost the generalization and it is not being able to perform well on the test data and that is what is known as overfitting problem.
So, this problems of overfitting or underfittingcan be controlled by altering another property of the model which is known as model capacity or capacity of the machine learning algorithm.
And this capacity is nothing but a set of functions the learning algorithm can select as being the solution.
So, we will just see after this and you remember that all the problems that we have considered we have discussed till now they are linear problems.
We have assumed that given data belonging to two different classes in the feature space, I can have a hyper plane in the feature space, I can define a hyper plane in the feature space where all the training data belonging to one class.
hyper plane if I pass a curve or a surface a curved surface then a curved surface may be able to separate those twoset of data belonging to two different classes.
So, that is what is known as the set of functions.
Always the linear function may not be sufficient, I may have to go for functions of higher order, maybe quadratic, maybe cubic or even fourth order, fifth order and so on which is known as the capacity.
So, if I control the capacity or the set of functions the training algorithm can adopt as required, then possibly we can have a control over the overfitting and underfitting problems.
And there also it is not that that simple or that easy because it is quite possible that say for example, a data set which is sampled from a quadratic function.
If I want to fit a quadratic functionit will be properly fit that means, all the data will beproperly fit to the quadratic curve.
If I try to fit a surface, then obviously, I will incorporate some error.
If I want to fit a data of say higher dimension say of a curve of higher dimension say order 5 or order 7 and so on, then it is possible that for the given set of data, I will have no loss there will be no error, but the data is too specific for that set of training samples.
It is not general enough to perform well on unknown data set or test data set.
So, even selecting the capacity of the model or capacity of the learning of the machine learning algorithm isalso a difficult task or a challenging task.
So, here we have talked about that what is the difference between a general optimization task and an optimization task as as applicable to machine learning algorithms.
Because in case of machine learning algorithms though we are on the training set of data, but actually I want to perform on a different set of data which is test set of data right.
So, now I talk about two problems which are known as linear regression and logistic regression and we will see the importance of this.
So, what is linear regression?
You find that so far all the problems of the examples that we have considered as .
as we have just mentioned now that I have considered only the linear problems that is given two sets of data I should be able to find outstraight line in two dimension or a surface or a plane in three dimension or a hyper plane in even higher dimension.
So, effectively there what I want to do isthis model that maps a feature vector of dimension d to a scalar.
ywhich is a linear number.
So, this is actually put in this form that this is a mapping which maps a feature vector x of dimension d. So, we write it this way that x belongs to r d. So, it maps this feature vector x to and y or in the other words that given feature vector x we want to predict what is y, what is the value of y.
So, I can write this in the form of a linear equation of this form that y hat is equal to W transpose X, where W is the weight vector and based on the value.
So, while training what we try to do is, we try to minimize the error between actual y which is the two class that is given for the set of data and this y hat that is predicted.
We want to minimize this error during the training operation.
And during classification what we do is based on this predicted value of y which is y hat we take a decision that whether we should classify x to class 1 or we should classify x to class 2.
So, for a binary classification problem that we have donepreviously we have discussed previously we have seen that if y hat is positive that means if it is greater than 0 then we classify x to class 1 and if y hat is negative that it is less than 0, then we classify x belonging to belong to class 2.
Now, what is this linear regression actually?
If you look at this expression that is W transpose x given this example here we have taken the same example in your previous discussion.
that we assume that this is the set of data the set of feature vectors which belong to class say omega 1 and this is another set of feature vectors that belong to class omega 2 .
So, for every point and this is myhyper plane given by the equation W transpose X equal to 0 .
So, you find that for every point belonging to omega 1 your W transpose X will be greater than 0 .
for every sample belonging to class omega 2 W transpose X will be less than 0.
And in fact, this W transpose X upon mod of W that actually gives you an idea of what is the distance or perpendicular distance of the vector X from the plane W transpose X equal to 0.
So, that measure gives us a confidence of how accurate your classification is.
Because, given these two sides if I have a vector over here which is far away from the separating plane in such cases obviously, I can tell with high degree of confidence that my classification result is correct.
Whereas, if I have a data somewhere over here where the distance of this data of this vector from the separating plane is very small my confidence level is not that high.
though here the distance becomes greater than 0 as w transpose X becomes greater than 0.
So, I classify this to belong to class omega 1, but my confidence level is not that high as I have confidence in this particular case.
So, I have a physical interpretation of the value of w transpose X that it says that how well inside the class the data is.
And if it is very high w transpose X is very high on the positive side.
Then, with guarantee I can say that yes this belongs to class omega 1 and if it is very low that is W transpose X is high magnitude, but it is negative then I can tell with confidence that yes it belongs to class omega 2, but if the value is low my confidence level comes down.
So, I can have another interpretation ofthis measure that is what is given by.
logistic regression.
So, in course of logistic regression I can have a probability measure that is I can compute that what is the probability of Y which is the class index given a feature vector X and the model parameter W which is written in this form P Y given X and parameter vector W .
x that can be written as a function of sigma w transpose x.
So, again you find that this w transpose x what you what you said earlier is a measure of distance this was a distance measure distance of x from the surface w transpose x equal to 0 and using this sigma function I can convert this to a probability measure.
So, as we said that if my data vector x is far away from the separating plane my confidence level is very high.
So, that in other sense I can say that the probability of class Y is very high.
So, I can convert this distance measure into a probability measure and that is what is done by this function sigma and I can write this function as a sigmoidal function which is given like this.
So, this sigmoidal function is nothing, but sigma W transpose X equal to 1 by 1 plus e to the power minus W transpose X.
And if I plot this function the function will have a plot like this which is a sigmoidal function.
So, if I plot sigma W transpose X versus W transpose X I get a curve as shown on this slide and here you find that as W transpose X become high as you increase W transpose X.
So, coming to our previousinterpretation of distance from the separating plane as the distance from the separating plane is very high the probability goes on increasing and asymptotically it goes to 1.
On the other hand if W transpose X is negative you find over here.
So, here you find that if W transpose X is equal to 0 what I get is sigma W transpose A X that is equal to half.
right.
So, from here e to the power minus W transpose X, if W transpose X is 0 that means, e to the power minus W transpose X is 1.
So, I get simply the value of half.
So, as at W transpose X equal to 0, the value of this sigmoidal function is half.
That means, if the feature vector lies on the separating plane, it has equal probability of belonging to class omega 1 and class omega 2.
or in other words I cannot really classify the feature vector x.
Whereas, as W transpose x becomes greater than 0, its probability of belongingness to class 1 goes on increasing and when W transpose x becomes very high that is the distance from the separating plane is very high the probability goes towards 1.
And to the other side as W transpose x becomes negative and becomes lower and lower that more on the negative side asymptotically your sigma W transpose X becomes tends to be 0.
That means, its probability of belongingness to class omega 1 is almost 0.
And when I compute the probabilities because I have I am considering only two classes omega 1 and omega 2.
So, I can say that the probability that it belongs to class omega 1 and the probability of it belonging to class omega 2.
So, what I can write is I can write it in this form.
1 thatprobability y equal to 1 given x and w that is the probability that x belongs to class omega 1 and I can also write the probability that x belongs to class omega 2.
So, probability y is equal to 2 given x and w .
So, if I add these two that should be equal to 1 because I am considering only two classes omega 1 and omega 2.
So, here as the probability belonging to class omega 1 becomes high, the probability of belonging to class omega 2 becomes lower.
And this is the point when both of them are equal that is probability of belongingness to both the classes are halfto omega 1 as well as half to omega 2.
So, that is what is logistic regression that is converting my distance measure or the confidence measure.
to a probabilistic measure that what is the probability that X belongs to class omega 1 or what is the probability that X belongs to class omega 2.
There is another concept which is known as soft max classifier.
So, earlier this linear regression orlogistic regression which we have discussed those are with respect to our binary classifier or two class problem, but when we have a multi class problem.
machine, we have discussed earlier that I can have a linear machine where using a linear machine the parameters set of parameters are represented as a weight matrix W, where vectors in every row of the weight matrix W we have said represents a prototype or represents a class or sample belonging to a particular class that is a representative of a particular class.
So, if I have say k number of classes I will have k number of rows in the weight vector.
And what that set of or matrix ofweight vectors give is when you multiply that with your given feature vector you get a score vector and we have seen that the score vector is a set of real numbers.
So, if I have got k number of classes my score vector x was having k number of elements.
So, if I have a feature vector x which belongs to class omega.
which belongs to class say y i the class index is y i, then the corresponding score is given by S of y i which have we have computed as w x i and the y i th component of that or this also you can write as w y i th rho the vector corresponding to y i th rho take the transpose of that vector and multiply that with x i .
So, that gives you the score fact score of class y i for feature vector x or x i which belongs to class y i that is we already know because this is a training vector.
So, again this is on the output of a linear function as we have done in case of in case of logistic regression I can also convert this to a probabilistic measure.
So, I can say that what is the probability of class y i given sample x i and your parameters w given by the parameter matrix or the weight matrix w. And we can compute this as e to the power s y i upon e to the power s j where the summation sum of e to the power s j in the denominator where the summation has to be taken over all j .
So, you find that this also converts or score factors orthe score values to different classes in a probabilistic measure that what is the probability of class y i given of which a vector x or given a feature vector x i.
So, we will continue our lecture further.
Thank you very much.
Hello, welcome to the NPTEL online certification course on deep learning.
In our previous class, we have talked about the loss function in a multiclass support vector machine which is a linear machine.
And we have also seen that how the loss function can beminimized using the gradient descent approach, so that we get the weight matrix corresponding to the minimum loss on the training vectors.
In today's lecture, we will continue with the optimization techniques.
And, we will talk about three variants of the optimization techniques which are stochastic gradient descent approach, a batch optimization technique and a minima mini batch optimization techniques .
So, just to recapitulate what we have done in the previouslecture, we had defined a loss function for a multi class support vector machine which is given by L is equal to 1 upon N .
Then, summation of maximum of 0 and w j transpose x i minus w y transpose x i plus delta plus lambda times sum of w k L square where w was our weight matrix and the first term in this loss function which is 1 upon N of sum of this, this is what constitutes the data loss component.
So, it is data loss component and the second term lambda times sum of W k L squared, this is what we said is regularization loss .
So, in order to minimize this loss .
using the gradient descent approach, we have to take the gradient of this loss function with respect to w y i and the gradient of this loss function with wj.
So, the gradient with respect to w y i is given by this, it is minus 1 over n sum of all those x i, all those training vectors for which w j transpose x i minus w y transpose x i plus delta is greater than 0.
So, you find that for those training samples for which this component this term W j transpose y i minus W y i transposeW j transpose x i minus W y i transpose x i plus delta is greater than 0 such x i such training samples are not correctly classified or not satisfactorily classified by the weight vector W .
So, for these samples we have to correct the weight vector w. So, that is the reason you take the sum of x i only for those samples which are not correctly classified.
And this is the correction term that comes from the data losscomponent.
Similarly, the other term that comes from the regularization loss component which is eta times w y i.
In the same manner when you take the gradient with respect to w j.
Again, I have a component 1 over 1 over n sum of X i where W j transpose X i minus W y i transpose X i plus delta is equal to is greater than 0 and such X i is obviously not satisfactorilyclassified by the W that I have at that iteration step.
So, you take the summation of all those X i which are not satisfactorily classified.
I do not take any corrective measure for attaining sample X i which is satisfactory classified by W. So, this is a term again coming from the data loss component and the other term that comes from the regularization component is zeta times W j .
So, using this gradient you go for gradient descent approach and your weight updation rules becomes W y i k plus 1 gets 1 minus eta times W y i k .
plus 1 over n sum of all those x i which were not satisfactorily classified.
Similarly, W j is modified as W j k plus 1 is equal to 1 minus zeta times W j k minus 1 over n sum of all those x i which are not satisfactorily classified.
So, that is what gives you the gradient descent approach for .
of the weight matrix and once you reach the minimum of the loss or on convergence the W matrix that you get that gives you the support vector machine or the linear machine which now can be used for classification.
And again you remember that all these X i which are being used for training the support vector machine or for training the linear machine once the training is complete that is once we get.
and weight which satisfies our criteria of correct classification, then we can forget about all those training vectors or X i's which were used for training the support vector machine or which were used for obtaining the value of weight matrix W which is now to be used for classification.
So, once this part of learning is complete, we can simply discard all those training vectors and what I need is only this weight matrix weight matrix W.
for classification of any unknown samples or any unforeseen samples right.
So, coming to the nature ofthe optimization functions or the loss functions.
Earlier in case of support vector machine we have seen that the loss function is convex which isalways we want that the loss function to be convex because then I do not have the problem oflocal minima or global minima.
But, in general we will see later that we can have situations where the loss function is not necessarily convex.
So, I can have a loss function which is given over here the nature of the loss function is somewhere over here.
So, here you find that this loss function has a number of maximas and it also has a number of minimas right.
So, a minima which is minima over all the minimas that is what is the global minima.
Similarly, a maxima which is maximum over all the maximums that is the global maximum.
So, we have local minima and global minimum and we have local maxima and global maxima.
So, the target of any optimization technique is to reach the global minima.
However, there is a possibility that you may be trapped into local minima.
So, in case of machine learning and machine learning applications.
even a solution leading to a local minima is acceptable if the loss or the error given by that is not too much.
However, as we said that it is possible that you may be trapped into local minima and obtaining a solution corresponding to a global minima may not be possible always.
So, this is in general is what is the loss function and as I said that we .
always target to have to achieve the global minimum.
Now, I talk about three different optimization variants, one is the stochastic optimization, the other one is batch optimization, the other one is mini batch optimization.
So, if you look at the previous optimization techniques is somewhere over here.
So, you find that whenever I compute the loss function or let me come to the definition of the loss function So, whenever I come to or try to define the loss function, the loss function is defined in terms of all those samples, all those training samples which are not correctly classified or which are not satisfactorily classified by yourclassifier or by the support vector machine.
So, I have to collect all those samples which are not satisfactorily satisfied,satisfactory classified.
That means, for my training algorithm to work I must have access to all the training samples in one go right.
So, I must have access to all the training samples and then you have to try the classification algorithm with all the training samples, we have to identify all the training samples which are not correctly classified and then we have to compute the loss function for each of those individual training samples which are not correctly classified.
And, then you have to combine all such loss functions to get the overall loss function and that is what this expression tells you.
So, wherever this term is greater than 0 that is W transpose W j transpose X i minus W y i transpose X i plus delta if this term is greater than 0, then I assume that this corresponding X i is not correctly classified and this leads to an error term.
So, I have to compute this error term for all those training samples, sum them up and take the take the average to compute the overall loss function.
And that is in some cases problematic if your training sample size is very very large.
And this is the approach which is known as batch optimization technique, because you are considering all the training samples together.
On the other side the so, one of the problem of batch optimization technique is that I must have access to all the training samples.
I have to identify all the training samples which are not correctly classified and then I have to compute the loss function which is the sum of loss functions of all individual training samples, the loss functioncontribution of all individual training samples which are not correctly classified.
So, that needs huge amount of memory and sometimes the computation is also not inefficient.
So, the other variant of that is stochastic optimization techniques.
So, in stochastic case of stochastic optimization, Instead of considering all the training samples together, you take the training samples one by one.
And the moment you find that the training sample is not correctly classified, immediately you compute the loss function corresponding to that training samples and use that loss function to update your weight vector or weight matrix immediately.
If the training sample is correctly classified at by a given W.
Then as the training sample is correctly classified, so far as that training sample is is concerned I need not update W at that moment.
So, you take the training samples one by one, every time a training sample is correctly classified simply skip that sample go to the next sample.
If the sample is not correctly classified immediately you update the weight vector or the weight matrix and then you take the next training sample which has to be tied to the with this updated weight matrix.
So, if the next training sample is correctly classified by this updated weight matrix, then you skip that sample go to the next one.
If the next one is again correctly classified, skip that sample again go to the next one.
If that sample is not correctly classified, then immediately using the corresponding loss function following the gradient descent approach, you update yourweight vector of the weight matrix.
And it continues like this until you come to a situation.
that your error is less than certain acceptable limit or in a single pass over all the training samples all the training samples are correctly classified.
And this is what is known as stochastic optimization or stochastic gradient descent.
When you come to batch optimization or batch gradient descent it is somewhere in betweensorry the mini batch optimization or mini batch gradient descent it is somewhere in between.
So, in case of batch optimization we have considered all the training samples together and then identified all the samples which were not correctly classified, computed the error function, then accumulated all those error functions together to give you the overall error function.
And then you go for a optimization technique to reduce the error.
In case of stochastic optimization you have considered the training samples one by one.
So, that is on other extreme.
In one extreme in case of batch optimization you consider all the training samples in one go.
together.
In case of stochastic optimization which is the other extreme you are considering one training sample at a time.
If it is correctly classified you skip, if it is not correctly classified you update the weight vector.
In case of mini batch optimization you take the training samples in mini batches.
So, there I have to decide that what should be my batch size, whether I should consider 10 10 training samples at a time or I should consider 100 training samples at a time which is by mini batch size.
So, once I once I have this mini batches, then the approach that I have taken for batch optimization I do the same approach for this mini batch.
So, if a mini batch contains say 50 training samples, I have to consider all those 50 training samples together, identify that out of this 50 what are the training samples which are incorrectly classified.
So, you compute the loss function using those incorrectly classified training samples out of your mini batch.
And, using that loss function you go for updation of the weight vector or the weight matrix.
So, in case of mini batch optimization I still take the samples in batches, but not all the samples together.
I divide the samples the training samples into smaller batch size and I go for optimization using those smaller batch sizes that is what is mini batch optimization.
So, So, the mini batch optimization is somewhere in between your batch optimization and stochastic optimization.
So, given this now let us try to see that what are certain advantages or disadvantages of this different approaches of optimization techniques .
So, coming to your stochastic gradient descent approach the advantages areyou are frequently updating the weight vector or the weight matrix .
So, as I said that any moment a sample vectoris misclassified immediately you go for updation of the weights or updation of the weight matrix.
And this gives you an insight into the performance of the model, how the model is performing and howthe improvement in the loss function is taking place.
So, that tells youan insight that gives you an idea of the rate of learning of the model.
And this gradient descent is the simplest one to understand also the simplest one to implement because I do not have to accumulate all the training vectors together or I do not have to identify all the misclassified samples together and combine them collect them and then compute the loss function.
So, the implementation and understanding of thisstochastic gradient descent approach is also simpler than in case of batch gradient descent approach.As we are updatingthe weight vectors quite frequently every time a sample is misclassified you are updating the weight vector or the weight matrix.
So, as a resultyour learning or learning updating of the weight vectors or learning ofthe algorithm that may become faster, but this is not always guaranteed, but in some cases it may become faster.At the same timesince your update process is noisy in the sense that with one training example which is misclassified I can be on one side.
of the optimizationsurface, the error surface with the other one I may simply go to the other side of the error surfaceover while doing that you may simply overstep the global minima or local minima.
So, as a result it may be possible that you may be able to avoid the local minima.
and convergence may be at the global minima .
These are the advantages of the stochastic gradient descent approach.
However, it has got disadvantages also.
Say for example,as the model is updated quite frequently.
So, your computation may be more expensivethan what you do in case of batch optimization.
See Here, this optimization techniques it involves a number of stepsfor optimization.
The first step is for the training samples I have to compute whether it is correctly classified or it is misclassified.
If it is misclassified then I have to compute the loss function.
Once I compute the loss function then I have to compute the gradient and once I compute the gradient I have to update the weight vector.
So, the complexity depends upon the what is the dimensionality of the feature vectors or the dimensionality of the weight vectors.
It depends upon the number of training samples that I have because for every training sample I have to compute whether the training sample is correctly classified or the training sample is not correctly classified.
The complexity also depends upon the number of iterations over whichthe computation has to be made the updation has to be made.
Now, one of the problem of this stochastic gradient descent is though we have said that it is a very very simple approach, very simple to understand, very simple to implement.
So, But, one of the problem may be that suppose for a kth sample vector, I find it is correctly classified by the weight vector at a certainin a certain iteration.
But, as the weight vector is being updated for every sample which is misclassified, this kth vector which in one instant I have found to be correctly classified by the weight vector, but in the next pass the same sample.
may be misclassified by the updated weight vector because the weight vectors are being continuously updated and that may lead toincrease in computational complexity.
Not only that once I find an weight vectoronce I find a sample vector is misclassified by a weight vector in a particular iteration step immediately you are updating that weight vector and using that updated weight vector you go to the you go for testing the next training vector and this continues.
And finally, you have to come back to the same weight vector using which you had updated the weight vector once.
And now you may find that this weight vector again is not correctly classified by the updated weight vector.
So, the same operation has to be done once more and this may repeat many times and that may lead to the computational complexity or increase of computation time of the gradient descent approach.
And this may be significantly longer.
to train a model on a very large data set.
The other problem may be that frequent update can result in noisy gradient signal and that may cause the model parameters and in turn the model error to jump around and this is what I said that I may overstep the minima which I want to reach because I am frequently updating the model parameters.
The other problem may be that the noisy learning process down the error gradient can also make hard the algorithm to settle on an error minimum for the model.
It is again for the same one that even if certain training vector a particular training vector is correctly classified by one weight vector at a particular iteration in the next iteration the same training vectors, the sample vector may be misclassified by the updated weight vector and that does not allow the weight vectors to settle to a minimum of the error.
So, these are the problems of the stochastic gradient descent.
Now, against this when you go for batch gradient descent, the batch gradient descent approach again has got certain advantages, it also has certain disadvantage.
The advantages are that as your updates are not that frequent or if a fewer number of updates because here for every sample we are not updating the weight vector rather we are collecting all the samples which are misclassified then computing the loss function and then you are computing the weight vector.
So, that means, this is usually computationally more efficient than the stochastic gradient descent approach and this decreased update frequency results in a more stable error gradient and may result in a more stable convergence on some problems.
Again you note the term that it is on some problems it is not guaranteed that in every case this may not be true.
And the next one is as your calculation of errors and the update these two are twoseparate processes.
For calculation of error or the loss function what we are doing is we are collecting all the samples which are misclassified.
then you are completing the error and once your error computation is complete, then you are using that error to update your model parameters.
So, these two processes are separate and as the two processes are separate they can be implemented in parallel as well in a pipeline right.
So, these are theadvantages of the batch optimization or batch gradient descent approach.
On, the downside or coming to the disadvantages that as the error gradient is very stable this may result in a premature convergence in the sense that there is a high risk of converging to a local minima because of stable gradient.
The other disadvantage is that the updates at the end of every training epoch require additional complexity of of accumulating prediction errors across all training samples.
And this is what I said that I have to identify all the training samples which are misclassified and using these misclassified training samples I have to get the overall error.
So, that is also a disadvantageous.
It also requires that the entire training data must be available in memory and to the algorithm for processing which requires huge amount of memory and as in case of machine learning applications or deep learning applications.
the number of training samples is huge of the order of maybe lakhs of samples are used for training your machine learning algorithm.
So, this is a problem that for batch optimization techniques or batch batch gradient descent techniques, I require that all the training samples must be available in memory and must be available to your updation algorithm.
And of course, as we are working on all the data at a time.
The model updates and the training speed may become very slow for the data sets we are very very large.
So, these are the advantages and disadvantages of the batch optimization or batch gradient descent approach.
Then coming to the mini batch gradient descent as we said that mini batch is somewhere in between your stochastic optimization and batch optimization.
So, the advantages of mini batch optimization is that the model update is frequency is higher than that in case of batch descent optimization which allows for morerobust convergence avoiding local minima.
So, in case of batch optimization we have seen that as your error rate as the training rate is very very slow, so is more stable.
So, there is a risk of convergence to a local minima in case of batch optimization that is that risk is less.
And, this batch updates provide more efficient process than in case of stochastic gradient descent because in stochastic gradient descent we had to workevery sample at a time in batch optimization we are taking mini batches.
So, here thecomputation computation is more efficient than in case of batch optimization.
And, the other advantage for batch optimization is that in case of mini batch.
I do not need all the training samples to be available in memory and the algorithm.
Rather I need all the training samples in the mini batch to be available in the main memory whose size is much lower than the size of all the training samples.
And coming to the disadvantages here again how do you decide what should be the mini batch size that is mini batch requires configuration of an additional hyper parameter.
which is the size of the mini batch.
And the other disadvantage which is of course, not as severe as we had in case of batch optimization that the error information must be accumulated across batch mini batches of training examples for batch gradient descent.
So, I can have these three variants of the optimization techniques or gradient descent techniques the batch optimization.
the stochastic optimization and mini batch optimization.
And just as a comparison of the performance of these three you find that this shows a set of curves on certain examples forbatch optimization, mini batch optimization and stochastic gradient descent.
So, here you find that this black curve this is for the batch optimization and here you find that this curve is very smooth.
On the other extreme we have this stochastic gradient descent where every sample is taken one at a time ok. And here you find thatthe error reduction over iteration is very easy to read.
And using the batch optimization techniques we have performance in between which is as expected and depending upon the mini batch size your rate of convergence.
is different.
So, this is the one which tells you where mini batch size is 10 and this is the one which tells you whenwhich gives you the performance when mini batch size is 100.
And you find that interestingly that if you in go on increasing the mini batch size finally, it becomes a batch optimization technique.
So, with increase of mini batch size or or in other words with increase of number of samples to be considered to.
together for batch optimization techniques, you find that gradually you are moving towards the performance of your batch optimization technique.
So, in today's lecture we have discussed about the gradient descent and the three different variants of gradient descent, the batch gradient descent, the mini batch gradient descent and the stochastic gradient descent.
So, with this I complete this lecture.
Thank you.
Welcome to the NPTEL online certification course on Deep Learning.
O'Reilly Corporation you just try to recapitulate what we have done in the previous class.
In the previous class we have talked about the linear classifier and then we had moved on to the linear machine.
In today's class we will continue with the linear machine and then we will move to what is known as multi class support vector machine.
So, as you remember in the previous class that a linear classifier.
when I extend that to a multi class problem because linear classifier is usually in case of a two class problem where we try to find out what is the boundary between the two different different class and we try to design that boundary using the feature vectors or the training vectors given from the two different classes.
When it comes to multi class problem then what we have a linear machine.
So, if you remember from the previous class A linear machine tries to map a d dimensional feature vector say r d to a k dimensional feature vector which is known as which is r k. So, this r k is what is known as the score vector that is given a feature vector x i when the linear machine maps that vector to a k dimensional score vector s.
every component of S tells us that what is the score for that particular category the corresponding to the component of S for the given input vector x i.
So, the first question is that how do we get this d dimensional vector r d. Let us take a very simple case that we have talked about of the feature vectors in conventional machine learning techniques, where the feature vectors are handcrafted and they can be generated using the properties of the boundary of the object or using the properties or regional properties which contains the color information, texture information and intensity information and all that.
But when we talk about deep learning, we do not depend upon the handcrafted feature vectors, we want that the machine will learn the features as well based on which it will classify or it will understand the data.
So, for specific example when we have image as the input data.
So, suppose we have an image of size say m by n that means, m number of rows and n number of columns which is a matrix of integer numbers.
So, this is say an m m by n image which will have n number of columns and m number of rows.
So, the way I can convert this into a feature vector is first you take this column and form a part of the vector, then you concatenate the second column that makes another part of the vector.
Like this you continue the last column forms.
the last portion of the vector.
So, when I have an image of size m by n having total number of pixels which is m into n, I have a feature vector of generate a feature vector from this image which has got m into n number of components.
So, this is an m into n dimensional vector.
Likewise if I have a color image then in color image we have three different planes.
I have red plane, I have green plane and I have blue plane.
Each of these color planes let us assume have got m by n number of pixels, there being m number of rows and n number of columns.
So, from first r component r plane I generate a vector having 3.
This is the total number of elements in the switcher vector.
So, we were talking about cipher 10 database yesterday in the previous class and in cipher 10 database we have assumed that there are n is equal to say 50,000 images.
Each image is of size 32 by 32 pixels and those being color images there are 32 pixels 3 planes red green and blue for each of the images.
So, if I convert this into a feature vector of this form every image will be converted into a column vector having say this is 32 by 32 into 3.
So, that is equal to 3 0 7 2 number of elements.
So, one element corresponding to every pixel.
So, when I convert an image into a vector of this form So,this vectorization the way I have shown is not the only option available.
What I can do is first I can have one column of component R. So, using this column I make a part of the feature vector, then you take one column of green make another part of the feature vector, then you take another column from B or the blue component make another part of the feature vector and this you continue.
for every columns taken from red, green and blue components.
So, this can be another way of making converting a color image into a vector form.
And another thing you notice that one I have once I have converted an image into a vector of this form.
So, this vector is of dimension m by n by 3 where the images are of size m by n and the images are are are color image .
For cifar database this will be having 3072 number of components.
So, that means, I am defining a space or 3072 dimensional space.
And every image is now represented by a point or by vector into that 3072 dimensional space.
In this case every image will be defined by vector or it will be represented by a point into m into n into 3 dimensional space.
So, that is what the vector representation of an image.
So, once I have such a vector representation .
So, here this iswhat I was telling that I have this d dimensional vector which is represented which is generated out of an image and the linear machine maps this d dimensional vector into a k dimensional vector where k .
is the number of categories or the number of classes that we have.
And if I expand this expression this expression can be written in this form that f x i w b where x i is my input vector, w is a matrix of dimension k by d having k number of rows and d number of columns and b is a bias vector having again k number of elements.
So, this is a k dimensional bias vector.
This w and b they are the parameters of the linear machine they define the linear machine.
So, this functional form f x i w b is nothing, but w x i plus b where w as I said that it is a parameter matrix or weight matrix.
of this matrix W represents classifier of a particular class.
So, if I take jth row of this matrix W this represents the classifier of the jth class and this is my input vector x this is the bias vector and after computation I get the score function k right.
So, now let us .
take an example to see what it actually means.
Suppose I have somehow I have obtained this matrix W, this is the vector which is generated out of an image.
Let us assume that this vector for simplicity is a vector of dimension 4.
So, I have got 4 elements in this particular vector and there are.
that each of this each element of this scorevector is gives you the score for a particular class.
Here the first element let us assume that the first category is cat category.
So, the first score corresponds to cat score, the second category is bird category in this particular case.
So, the second score corresponds to bird, the third score corresponds to dog and so on.
And here as my input vector is generated from a bird, so you find that the bird score is maximum, it is maximum of all other course.
So, my for my classification purpose whenever an in unknown input vector is presented to the classifier, the classifier will generate or the linear machine will generate a score vector and in that score vector whichever component comes out to be maximum, I have to classify that input vector to that corresponding category.
Now, here comes the interpretation of .
the rows of weight matrix W. You find that effectively what I am doing is this matrixcalculation is doing is that it is taking the dot product of the input vector .
It is taking dot product of the input vector with say jth row .
vector of weight matrix W suppose this is the jth row.
So, it is taking the dot product of the input vector with the vector corresponding to the jth row ofthe weight matrix of the parameter matrix and it is generating the jth component of the score vector S. And as we said that this being a vector that is my input image represented as a vector or as a point in d dimensional space.
and this jth row of matrix W is also a point in the d dimensional space and I am taking the dot product of these two and you know that the dot product of two vectors tells you what is the degree of similarity between the two vectors.
So, if the two vectors are very similar say I have one vector in this and another vector like this the dot product of these two will be quite high whereas, if I have one vector here and another vector here.
are widely different the dot product of these two will be quite low.
So, this jth component of my score vector or S j in other words tells me that what is the similarity between the input vector which I want to classify and the vector corresponding to jth row of the weight matrix W .
So, having thisinterpretation, so, I am going of the weight matrix, I can say that every row of the weight matrix is nothing, but a template of the corresponding category.
So, the j th row is represents a template of the j th category, the k th row represents a template of the k th category.
So, if you again coming to the example of CIFAR database wealso mentioned in our previous class that the CIFAR database contains images of 10 different categories like plane, car, bird, cat, sheep and so on.
And after training the rows of the weight vector the rows of the weight matrix as I said that they representthe weights of different categories.
So, if I convert those rows into the to the form of images every image will be a template of the corresponding class of the corresponding category.
So, how do you convert this weight vector into an image?
So, it is just the reverse process that as we haveshown in the previous example that the way we convert an image into a vector is that I have an image I take every column of this image one by one.
those columns to get the vectors.
So, this is how I form a vector from an image.
Similarly, if I have a vector how do we convert this into an image?
So, if you look at the way we have done this multiplication that is when we have done w.
Pth component of my weight vector W of the jth row corresponds to Pth pixel in the vector representation of my input image.
So, as I have converted an image into a vector by unfolding the columns and concatenating them together to form a vector of dimension d, when I do the reverse process that is given a vector, I want to form a matrix out of it.
So, first you take the first m number of elements of this matrixmy image was I assume to be of size m by n. So, I take first n number of elements of the matrixof this vector and form the first column of the image.
Take the second n number of elements of this vector and use that to form second column of the image.
Similarly, at the end I have mth n number of elements of the vector and use that to form the last column of the matrix, it should be the other way.
I have m number of rows.
So, instead of n number of I have to take m number of elements.
So, here I take first m components of the of the vector from the first column of the matrix, then you take the second m number of elements of the vector from the second column and in the same manner you take the nth m number of elements of this vector from the last column.
If I have color images obviously, I will have other color components according the I form.
other color planes of the input image.
And this is how I convert the vector w in the form of an image.
So, this is what we have the way those vectors can be converted into an image gives me the template.
So, you find that these are the templates corresponding to different classes.
The next we come to the case of we have a bias vector.
matrix, it is also possible to include that bias vector in the weight matrix itself.
So, for that what I have to do is I have to simply increase the number of columns of the weight matrix by 1 and that additional column or the last column is actually the vector column vector corresponding to the bias vector.
And for doing that the modification that I have to do in my vector data vector is that I have to add and additional element and make that equal to 1.
So, this gives me the same matrix equation only difference is now I do not have the bias vector separately, but it is included in the weight matrix itself.
And accordingly the function that I compute is f x i w where w includes the bias vector v b and you can verify that f x i w is same as f x i w b when the bias vector was kept separate.
So, both these computations are same.
So, accordingly the score vector that you get the score vector will also be same.
So, what I get is I get for every vector I compute the function as given by the linear machine.
and the linear basin gives me a score vector which is S. So, if I take S j or the jth component of the score vector as is shown over here.
So, this S j which is the jth component of the score vector which is nothing, but when I do this matrix multiplication W x i and taken the jth component of that which is nothing, but the score for jth class for the ith vector x i .
And ith vector x i is actually given in the form x i y i indicating that this x i belongs to category y i.
You remember that what we are talking about are all training vectors that means, for the training vectors we know the category to which the training vector belongs .
So, I know that the training vector x i belongs to category y i .
And, given this to obtain the score for y i th category for the vector x i, I have to consider I have to look at what is the y i th component of the score vector.
And, y i th component of the score vector as before is nothing, but whatever I get by this matrix multiplication and take the y i th component of that.
because we know that this X i belongs to category Y i.
So, this S Y i that is Y i th component of my score vector X must be maximum among all other components of the score vector because this X i belongs to class Y i ok.
So, the next question is should we be satisfied just with the condition that S Y i is maximum.
or I want that this S y i should be more than all other components at least by some margin delta.
So, that I can say with confidence that whatever classification I have got that is correct.
You remember with what we did in case of support vector machine that in support vector machine your W transpose X plus some bias B, we assumed that this have to be greater than some minimum threshold D. And what is this W transpose X plus B?
This tells you that an idea of what is the distance of X vector X from the plane W transpose X plus B equal to 0 .
And with normalization this we can always write in the form W transpose X .
plus b have to be greater than or equal to 1 after normalization we can always do that.
So, this says that from this boundary W transpose X plus b equal to 0, the normalized distance of my training vector must be greater than or equal to 1 and that is the margin.
Similarly, in this case to have confidence over the result that we get the classification result that we get I should have I should impose that the score for class omega i must be greater than score for any other class j at least by margin delta.
And accordingly I can define a loss function l i which is given by 0 maximum of 0 or S j minus S y i plus delta.
And this should be I have to take some of this for all j which is not equal to y i.
So, here you find that what I get is if S j minus y i this particular value equal to 0, then delta which is a positive constant is greater than 0.
So, output will be 0.
Whereas, if S j minus y i this is minus delta that means, y i minus S j is equal to plus delta then S j minus y i plus delta will be equal to 0, if you take the max function the output will also be 0.
And, it will remain 0 as long as S y i minus S j is greater than delta that is S j minus y i is less than minus delta.
So, for all those cases this max function will be giving an output 0 and the loss component the contribution to this loss function L i in that case will also be equal to 0 and L i will be positive as long as the margin is less than delta.
So, this particular loss function confirms ensures that score for category omega i will always be greater than the score for any other category S j at least by a margin delta right.
So, if I ah.
So, this is an example that suppose I have taken an y i which belongs to class 2 and the score function is given by 10 30 minus 22 minus 20 25 and if you compute the loss function then you find that for the first one the loss function will be equal to 0 because 30 is greater than 10 by a factor by a margin which we have in this case we have assumed to be 10.
byit is 30 is greater than 10 by a margin which is more than 10.
Coming to this one again 30 is more than minus 20 which is score for the third category by more than 10.
Come to the third category fourth category for which the score as 25.
Now, you find that the margin for category 2 to which my input vector belongs.
So, this margin from category 2the difference between the score for category 2 and category 4 is only 5.
1 which is not sufficient because I have assumed margin to be 10.
So, this fourth component that actually gives me a non-zero loss function over here and that non-zero loss function is equal to 15.
Whereas, the others gives me a loss value which is equal to 0 that gives me the overall loss function L i which is equal to 15 ok. And if you plot this loss function I plot this loss function which is max of 0 S j minus S y i plus delta versus S j minus S y i .
You find that as long as S j minus S y isorry it should be just negative of this S y i minus S j if I take the negative value of this that means, S y i minus S j as long as it is greater than delta then your loss function is 0 the moment it becomes less than delta.
loss function goes on increasing which is in this direction.
And because of nature of this variation of loss function this is what is known as hinge loss.
So, in case of a linear machine which minimizes hinge loss you find that we are talking about a margin.
So, the score between the correct class and any incorrect class the difference of these codes must be greater than margin delta which is similar to the two class support vector machine that we discussed earlier this is what is known as multi class support vector machine.
So, in case of multi class support vector machine it tries to minimize the hinge loss as shown in this case .
Now, this hinge loss is not sufficient the reason being .
If I multiply my weight matrix W by a constant say lambda which is greater than 1, you find that the difference between S j and S y i is nothing butif I take the difference of the dot products of the j th row of weight matrix W and y i th row of j th matrix W. So, S j minus S y i is nothing but the difference of these two dot products.
So, accordingly if I scale of W by lambda this difference is also going to be scaled up .
So, if for some W my S j minus S y i was say 15, now if I scale of W by 2 then this S j minus S y i will be 30 .
So, for different values of W I will have different differences of W.
the score function.
So, I have to choose that which value has to be proper among all these different possibilities of W, I have to choose a proper W. That means, I have to have impose a condition on W itself and that is what is known as regularization.
So, when you go for regularization, you include a regularization term in your loss function.
So, this regularization term is what is R w.
and usually the regularization term which is included is the L 2 norm of this weight matrix.
So, accordingly you modify your cost function as L is equal to this was the function corresponding to your hinge loss and you find that the hinge loss depends upon the data along with your parameters of the linear machine.
So, this is a component which is called data loss and I also impose a condition a regularization term which is L 2 norm of my weight matrix which is delta W delta times sum of WKL, WKL is the KLth element of the weight matrix square take the sum of this over L and K and this is what is known as regularization.
loss .
So, now overall loss function includes two terms one is the data loss and other one is the regularization loss.
So, while designing this multi class support vector machine I have to optimize or we have to minimize this overall loss function .
So, with this I will Thank you.
Hello, welcome to the NPTEL online certification course on deep learning.
Till our previous class we have talked about the back propagation learning and we have seen that how the back propagation learning is actually implemented or takes place in a feed forward neural network at the network level and also how the gradient is back propagated within a particular node or different layers or different circuits within a particular node.
And as we said in the previous class.
Now, onwards I will assume that you know back propagation learning and whenever the learning isdiscussed I will simply refer that back propagation learning algorithm is used.
I will not go into details of the learning algorithm until and unless some details is essential.
So, in today's discussion we are going to start to discuss on autoencoders.
So, today subsequent few lectures, we will talk about under complete autoencoder.
We will try to find out what is the relationship between an autoencoder andprinciple component analysis or PCA.
Maybe I will discuss something about PCA for those of you who are not aware of this.
We will talk about the other variants of the autoencoder namely sparse autoencoder, denoising autoencoder.
interactive autoencoder and so on.
And then we will also talk about convolution autoencoder, but not as a continuation of thisseries on autoencoders, but we will come back to convolution autoencoder after we discuss about convolution and convolution neural network.
So, today what we are going to talk about the autoencoder and under complete autoencoder.
Now, what is this autoencoder?
As the name suggests that autoencoder is nothing, but an algorithm that codes itself or that encodes itself.
So, you can say that autoencoder is an unsupervised learning algorithm, where the neural networks are subject to the task of representation learning and what is this representation?
The representation is nothing, but how you code or how you encode the input data that is fed to the network.
And, learning this representation learning this code is what is known as representation learning.
And we say auto encoders are unsupervised learning because when you train an auto encoder for coding an input or forencoding an input we do not use datas which are level datas.
unlike in case of classification problems that we have discussed earlier.
So, if you remember what we discussed in case of classification that for training the network or for training of your classification algorithm machine learning algorithm, we need a lot of training data.
And what that set of training data tells you is that it tells you that what is the class belongingness of a particular training data.
And it is only from that information of class belongingness I can compute the error.
Because, if my machine says that a training data or if my machine infers that training data belongs to some category say 5, whereas the ground truth says that that particular training data belongs tocategory 1.
So, there is a mismatch my machine says it is category 5, whereas the ground truth is category 1.
So, there is an error and your learning algorithm as we said using back propagation tries to minimize this error.
That means, as the machine has interpreted to be 5, what modifications or what updations in the weight vectors we have to do, so that the machine really interprets this data to belong to category 1.
So, those are the supervised learning algorithms, because the data that you use for training or learning are the labeled data.
But in case of autoencoder, the data that we use are not labeled data.
However, we still have back propagation algorithm because we want that whichever way the machine represents or the autoencoder represents the input data from that representation it is possible it should be possible that we should be able to reconstruct the input data.
That means, I have to find out that after encoding whether the encoded data can be reconstructed.
So, if for encoding I call it a forwardencoding algorithm.
So, some mapping function f that gives me the encoded data, then another mapping function g should be able to convert or transform that encoded data to my original input.
And for learning what you do is you compare the original input and this reconstructed input and try to minimize the error between these two.
So, this autoencoder as we said that it is an unsupervised learning and the task of the neural network in this case is to go for representation learning or try to encode or learn how to encode or how to code the input data.
And in order to do this what you do is you introduce a bottleneck in the network.
Because as we said that our learning algorithm will be that I have an input, I have some coding in between then I have a reconstructed output.
And I want that the reconstructed output should be similar to the input or they should be identically possible.
So, there is a possibility that the network may eventually learn an identity mapping ok.
So, if the network learns an identity mapping it does not learn it does not really learn the representation.
So, in order to enforce or in order to force that the network learns the representation or network learns what is the inner structure of the data, it is necessary that in the network you impose a bottleneck layer.
We will come to a bit later details of how this is done.
And this bottleneck actually forces a compressed knowledge representation of the input.
That means, if my input vector input data is of dimension say d, in this compressed knowledge representation it will be mapped to a vector of dimension say m, where m is much much less than d. So, how it is done we will come to this a bit later.
So, for this we have certain assumptions, the assumption is there is a high degree of correlation or structure that exists in the data.
If the components of the input data are not correlated that means, if the features are independent of one another, then this compressed domain representation and subsequent reconstruction of the original input will be difficult in fact, it may not be possible at all.
the neural network goes for representation learning or tries for representation of the input data in compressed domain, what it tries to do is it removes the correlation or the redundancy present in the data.
And what is what it preserves is only the uncorrelated part and from this uncorrelated part then it should be subsequently possible to reconstruct the original input data.
So, that is what an autoencoder is and as we said that the name autoencoder indicates that it encodes the data or it codes the data on its own.
And this is an unsupervised learning because for training an autoencoder we do not use any label data.
What we want is whatever is fed to the input the autoencoder outputs the same thing.
So, for this I need two different functions one is the encoding part one is the decoding part.
So, the encoding part will encode the input data.
to ancompressed domain representation knowledge representation.
And the decoder part will decode the data from that compressed representation from theencoded output to your original input original input.
So, if my input was x it will reconstruct x hat I want that x and x hat should be similar or the error between x and x hat should be minimum.
And that is what is given by the decoder part.
So, I should have half, an encoder half I should also have a decoder half.
And this is the structure the base structure of an autoencoder.
So, you find that in this autoencoder I have an input layer, this is the input layer and I have a hidden layer and I have an output layer.
So, this hidden layer is actually the bottleneck layer.
So, in the bottleneck layer, I have what you are doing is you are compressing the data, you are going for compressed domain representation.
So, you find that the number of nodes in the hidden layer or the number of nodes in the bottleneck layer is much less than the number of nodes in the input layer.
And also you find that the number of nodes in the input layer is same as the number of nodes in the output layer, because finally at the output I want that whatever was the compressed domain representation in the hidden layer from this compressed domain representation it should be possible to reconstruct my original input.
So, original input was x I should be able to reconstruct this x which is x hat.
So, obviously, the dimensionality of x and the dimensionality of x hat should be same.
So, what does it mean?
Say for example, I use this network for compressed domain representation of an input image.
And, suppose image is of size m by n. So, there are m into n number of pixels.
So, as we said before that by column concatenation this m by n matrix can be represented by an one dimensional vector you see having m into n number of elements that we have to do by column concatenation concatenation.
So, if each of the n each pixel isfade to one of the nodes on the input layer.
So, the number of nodes in the input layer has to be m into n because every node input in the input layer gets one pixel.
In addition we have to have one more node to represent the bias.
So, the number of nodes in the input layer has to be m into n plus 1.
Whereas, on the reconstruction side, the I do not need any bias, I just want my x back.
So, that is what I need an x hat.
So, the number of nodes in the output layer has to be m into n as against m into n plus 1 on the input side.
Now, suppose I want that this entire image should be represented by a d dimensional vector.
or a vector having d number of components, where d is much less than m into n .
So, in that case the number of nodes in the hidden layer has to be equal to d, which is my bottleneck layer.
But the reconstruction purpose I need one more additional node to take care of the bias.
So, the number of nodes in the hidden layer for the decoding side or for the reconstruction side will be m into n plus 1 .
So, this is the base architecture of an autoencoder.
Now, you find that so, I have an input layer, I have an output layer, I have an hidden layer or a bottleneck layer.
Now, this same architecture now onwards for simplicity I will represent like this that input layer will be an array of nodes, hidden layer will be another array of nodes where array of array size in the hidden layer.
will usually be less than the array size in the input layer.
Similarly, on the output side also I will have an array of nodes.
And I will have a set of weights say W 1 and W 1 dash.
So, this W 1 connects the input layer to the hidden layer and W 1 dash connects from the hidden layer to the output layer right.
It is also possible that instead of having just one hidden layer, I can have layers.
Now, before that this side when you are going from input to the hidden layer, the hidden layer actually gives you the encodedinformation in a lower dimension in general.
Later on when we talk about sparsity or sparse auto encoder we will see that it is not necessary that I will have to go for dimensionality reduction.
I can have a dimension explosion as well or maybe of the same dimension, but your compressed representation is done by some other mechanism.
However, for the timing let us assume that we are going for compressed domain representation knowledge representation.
So, from input to the reduced dimensional representation that is a part which is the encoding part.
So, this is the encoder side.
So, this is what is your encoder and similarly from compressed domain representation to the reconstruction of the original signal this is what is the decoder part.
So, I have an encoder, I also have a decoder.
It is also possible that I may not have only one hidden layer or only one bottleneck layer.
I can have multiple number of hidden layers.
So, I can have a situation something like this.
So, on the encoder side I have a number of hidden layers, on the decoder side also I will have a number of hidden layers.
I will come back to this one a bit later.
Now, in this case as we said that when I have an autoencoder, what I have is I have input data and output of the autoencoder is also a data and I want that the output should be a faithful reconstruction of the input.
And while doing so, the information passes through the bottleneck layer, where in the bottleneck layer I have a compressed domain representation or coded version of the input data.
Now, I my expectation from such an autoencoder is twofold.
Firstly, I want that the autoencoder should be sensitive enough to the input for accurate reconstruction because what we said is my reconstructed vector x hat should be as close as possible to input x.
So, that means my autoencoder should be accurately should be able to accurately reconstruct my input signal.
So, that is what is it is sensitive enough to input for accurate reconstruction.
Now, if accurate reconstruction is my aim not the representation then an identity function is sufficient.
So, it might be possible that autoencoder simply learns the identity function.
So, if it simply learns the identity mapping it will always reconstruct your output faithfully as the input, but that is not our aim.
Our aim is actually what is happening at the bottleneck layer that is how the input data is represented in in the compressed domain that is what is my interest.
And this encoded data will be useful for some later applications because I have the original if my reconstruction is the is the only aim why do I need it I have the original why do I why do I have to go through the bottleneck and then a reconstruction.
So, the other expectation from an autoencoder is it should be insensitive enough that it does not memorize or over fit the training data.
That means, it does not learn the identity function.
So, I have two conflicting requirements or two conflicting expectations from an autoencoder that it should be sensitive and at the same time it should not be sensitive enough.
So, how do you impose or how do you try to satisfy both these requirements, both these conflicting requirements simultaneously.
So, that is actually done by designing your loss function which takes care of both of them.
And this loss function as we said that the loss function will be used for back propagation learning algorithm when you train the autoencoder.
So, the loss function in this case will be given bythis that this loss function will have two components one is L x x hat which is the error between the input x and the reconstructed x hat .
So, if I want to minimize this, so minimization of this the error takes care of this that the autoencoder is sensitive for faithful reconstruction of the input.
Whereas, the second requirement which conflicts the previous requirement is through a regularized term regularized term.
So, in the loss function you have an error function between x and x hat.
And, you also have a regularizer term.
So, the regularizer term will try to make it insensitive to the input and it will force the autoencoder to learn the low dimensional representation where it learns the salient features of the input.
So, that using the salient features the decoder will be able to reconstruct the data.
So, it does not simply learn the identity function.
So, both theserequirements conflicting requirements are made are satisfied by defining a loss function of this form .
So, this is what I said that the way a variant of the autoencoder which is known as under complete autoencoder takes care of the regularization is by introducingor by introducing restriction on the number of nodes in the hidden layer or the bottleneck bottleneck layer.
So, as we said usually number of nodes in the bottleneck layer or the hidden layer is much less than the number of nodes in the input layer or similarly the number of nodes in the output layer.
So, in such caseswe can consider the The network is made insensitive to the input by restricted number of nodes in the hidden layer.
And when I have an architecture of this form this is what is known as an under complete autoencoder architecture.
And for training such an autoencoder you simply minimize the loss function which is given by L x x hat which is half of x minus x hat take the L 2 norm of that.
And, some of this over all the training vectors ok. And, you find that here as we said that this is an unsupervised learning because I do not need to have a knowledge of the class belongingness of the input vector x.
What I simply only need is that the x and x hat should be similar that is x hat should be a faithful reconstruction of my input x.
So, this is what is my under complete autoencoder.
Now, as we said before that it is not necessary thatthe autoencoder have to have only one hidden layer orthe bottleneck layer.
I can have and stacked autoencoder where a number of hidden layers are stacked one after another.
So, this diagram shows that I have a stacked autoencoder where obviously, x isfade to the input layer.
I have the firsthidden layer in the autoencoder on the encoding side, the connection weights between the input layer and the first hidden layer autoencoder layer is given by the weight vectors or weight matrix W 1.
Then I have the second hidden layer and in this case the second hidden layer happens to be the bottleneck layer and the connection weights between the first autoencoding layer.
to the second hidden layer the second auto encoding layer is the set of connection weights W .
So, this completes my encoding part.
Then on the decoder side from the encoded data it goes to the first hidden layer on the decoding side and the connection weights from this to this is given by W 2 dash.
And similarly from this hidden layer to the output layer the connection weight is given by W 1 dash .
And, what I simply want is that the x hat should be a faithful reconstruction of x that is the input data.
And, for that the loss function that you have to minimize is the L 2 norm between x and x hat.
And, this has to be summed over all the training data and you minimize this L 2 norm using back propagation learning algorithm.
And, while this is minimized, you find that these weights w 1 w 2 similarly w 2 dash and w 2 w 1 dash they will be modified they will be updated until and unless the L 2 norm or the error between x and x hat is 0 or it is within an acceptable limit.
At that point we say that my auto encoder is properly trained.
Now, what you do after the auto encoder is properly I said earlier that reconstruction is not my aim, I want to reconstruction I want to reconstruct the input data from the encoded data just to ensure that my encoding is proper.
That is whatever my is my encoded data from the encoded data I should be able to reconstruct my encoding input data, which ensures that in the process of encoding I have not lost any information.
this decoding part of the reconstruction part is not the one that I am interested in.
What I am interested in is simply this encoding part, my interest is up to this.
So, for subsequent applications if I want to apply this even for classification purpose.
What I can do is after this autoencoder is trained that means my encoding part.
is proper, I can simply forget about this part of the network.
You simply cut it off, you feed your input, I have the encoded output and feed this encoded output to your other application modules.
This is what is my aim, my aim is not the decoding.
Decoding is only a help to ensure that my encoder is working properly.
So, this is whatthe autoencoder will do and for this again we said that what we want is that autoencoder will be trained using the back propagation learning and for during this back propagation learning we will make use of the errors or the gradient of the error that you get at the output layer between the input and the reconstructed output.
And, based on that you modify all the connection weights so that your encoder is properly trained.
So, I will stop here today we will continue with our discussions on autoencoders in subsequent lectures.
Thank you.
Hello, welcome to the NPTEL online certification course on deep learning.
So, since our previous class we have started discussion on auto encoders.
So, what we discussed yesterday or in our previous class is what is an auto encoder and a particular variant of auto encoder that we have introduced is what is known as under complete auto encoder.
Today in this lecture we will discuss about the auto encoder versus auto encoder.
principal component analysis that is whether principal components and the autoencoder outputs they are related, if they are related, how they are related, what is the similarity and what is the dissimilarity between these two.
And then we in subsequent lectures we will discuss about otherautoencoder topics like training of autoencoders, sparse autoencoder, denoising autoencoder, contractive autoencoder.
convolution autoencoder and all that .
So, before we start today's topic on autoencoder versus PCA, let us just briefly recapitulate what we have discussed in our previous class .
So, we have said that autoencoder is an unsupervised learning technique.
So, a learning technique which forces the feed forward or deep neural networks .
to learn what is known as representation learning.
That is given an input vector or an input signal the network or autoencoder learns a compressed domain representation or learns a structure which is present in the input data.
And the way the neural network or the autoencoder learns this representation data representation or data structure is by imposing a bottleneck layer in the network.
And this bottleneck layer actually forces a compressed knowledge representation of the input and that is what the autoencoder learns and this compressed domain knowledge representation is subsequently used for other applications.
So, while doing this we assume something the assumption is the degree of correlation or the structure.
that exists in the input data is quite high.
And in fact, if the input data or the input feature vectors are uncorrelated or they are statistically independent, then compression and subsequent reconstruction would be difficult.
Of course, we will be able to compress, but the compression will be highly lossy compression if there is no redundancy.
Because whatever information is present in the input data unless there is redundancy or there is correlation, then going for any sort of compression leads to loss of data.
And, once in that compressed domain representation the data or the information is lost whichever way I try to reconstruct my signal the original signal from that lossy compressed representation my output will always be lossy.
That means, the decompressed data or reconstructed data cannot be identical to the input.
So, the basic assumption in use of autoencoders when you go for encoding in compressed domain or representation in compressed domain the So, basic assumption is the data is highly correlated.
So, based on this we have seen a basic autoencoder architecture which is something like this that you have an input layer, you have an output layer.
So, input layer actually accepts the input data.
So, if the dimensionality of the input data is n at the input layer I will have n number of nodes.
In addition there will be one more node to take care of the bias.
And, in fact we have seen earlier that addition of this bias in the input vector allows us to go for an unified file representation that is the bias term can be taken can be considered as an additional term in the weight vector.
So, number of nodes in the input layer will be n plus 1 if the input data vector has dimensionality n. Similarly, in the output layer which reconstructs the input as x hat.
So, our input is x and the output is x hat.
So, the output layer will consist of n number of nodes because we want that the input x should be reconstructed at the output.
And in case of a basic model of an autoencoder, we have a hidden layer in between input layer and output layer.
And what we have said in case of under complete autoencoder that the number of nodes in the hidden layer is much less is less than the number of nodes in the input layer or the number of nodes in the output layer .
So, this is what is known as a bottleneck layer.
So, in bottleneck layer as the number of nodes is less than the input layer nodes.
So, what this network does is the network passes the input information through a restricted layer where the number of nodes is much less.
And then subsequently as this information passes through this restricted layer then the decoder side that is the output layer tries to reconstruct.
the original input from this restricted output.
So, as the information passes through this restricted layer, the network tries to learn a compressed domain representation of the data.
So, imagine what will happen if I do not have this bottleneck layer that is if the number of nodes in the hidden layer is same as the number of nodes in the input layer that is the size of the data or even more than the number of nodes in the input layer.
In that case it might be possible that the network will simply learn an identity function that is given an input, it goes to an intermediate representation and then knows how to reconstruct the same output.
And in the process if I have large number of nodes in the hidden layer, the network eventually may not learn the compressed domain representation or the structure present in the data which is not our aim.
So, that is the reason that in the hidden layer or in the bottom layer you put some restriction on the number of nodes that you can have.
Later on we will see that when we talk about the sparse autoencoder that it is not even necessary to have the restriction on the number of nodes, but we can add some other regularization term where the rednodeactivations will be restricted.
So, instead of trying to restrict the number of nodes you try to restrict the node activations.
So, this is the basic structure of an autoencoder and as we said that all subsequent representations we will use this form of representation.
to represent an autoencoder.
So, I have an encoder part which is thefrom input layer to the hidden layer and I have a decoder part which is from the decoder to the output layer and this whole thing taken together the encoder decoder together is known as autoencoder.
And as it is an under complete autoencoder that we are trying to depict the number of nodes in the hidden layer which is the bottleneck layer is less than the number of nodes in the input layer or the number of nodes in the output layer.
And, while training this autoencoder the loss function that youtry to minimize is the squared error loss between the input and output.
So, that is x minus x hat L 2 norm of that x minus x hat square take the summation over all the data points all the training samples that you are feeding for training this network.
So, this is the basic structure of an autoencoder which has got one input layer, one output layer with a hidden layer or a bottleneck layer in between.
Now, when you go for so, what does this auto encoder try to learn?
So, here what has been shown is that if I feed an input to an auto encoder which is an image.
So, in our case x is an image and the output that is x hat which is reconstructed is also an image right.
And in ideal case if the auto encoder is properly trained then x hat will be same as x.
Now, once this auto encoder is properly trained.
what does this encoding layer or the bottleneck layer actually learn.
So, this is an example which has beenobtained from training such an autoencoder with large number of input images.
So, you find that on the right hand side the example image set that we have.
So, this particular image this particular sub image is actually the image or the structure which is learned by the first autoencoder.
Now, here you find that this output is not exactly from this particular network that we are showing here, here you find that there are 100 such sub images or 100 structures.
That means, the autoencoder which has been trained with for this kind ofexample has 100 number of nodes in the middle layer or in the bottleneck layer and every node learns some structure.
So, this first node learns this structure, similarly second node may learn this structure and so on.
And, how this image has been formed?
It is nothing but these weights from the input which are connected to the first node.
You remember the way we have got this vector is by concatenating the columns of the input image.
So, when you form thesestructures when I reconstruct the structures which are learned by these hidden layer nodes, these are these weight vectors which are folded back.
in the form of an image ok.
So, you find that if there are n number of nodes in the input.
So, I have an image consisting of n number of pixels.
Here also I have n number of vectors of course, n plus 1 considering the bias term.
Now, when you form this you remove that bias term.
So, among the remaining vectorsremaining components of the weight vector I fold it back in the form of an image.
So, this is such an image.
So, these are the structures as shown in this set of images which are learnt by this encoding layer.
And as you see over here these sub images appear to be edges oriented in various directions.
So, edges are nothing, but the detailed informations which are present in the image.
So, this simple example shows that this input layers or nodes in the input layer actually learn the structures which are present in the image.
it does not simply pass the input image to the output layer.
And then what this decoder side does is the decoder side use makes use of these structures which which are present in the image and from these structures it tries to reconstruct the original image.
So, when this network is properly learnt this is the form of structure which will which is actually learnt by this encoding layer right.
So, this is how .
an encoder learns the structure which is present in the data.
In case of deep autoencoder, so earlier what we have shown is a basic structure of an autoencoder where I have an input layer, I have an output layer and I have one hidden layer which is a bottleneck layer.
In a deep autoencoder, I can have a number of such auto encoding layers which are stacked one after another.
So, in this diagram what has been shown is This is your input layer, this is the input layer, this is the autoencoder layer 1.
So, I put it as A e 1, this is A e 2 or this is actually the coding layer in this particular diagram.
I can have A e 1, A e 2, A e 3, A e 4 and so on I can go on stacking such autoencoder layers, ok. Then accordingly on the decoder side also I will have stacking of a number of such decoding layers.
So, in case of deep autoencoder the number of layers, number of encoding layers and the number of decoding layers that you make part of the autoencoder that decides what is the depth of the autoencoder that you are going to design or the autoencoder that you are going to use.
However, for training the autoencoder we still use the squared error loss between the input and the output for training the autoencoder.
So, given thishere you find that what this autoencoder does as we have already said that given an input data of dimension say n. So, x having dimension n if in the encoding layer or in the bottleneck layer I have the number of nodes which is equal to d where d is much much less than n.
So, here this autoencoder learns a compressed representation of this n dimensional data to a d dimensional representation which is the latent also called as latent space representation.
And it is expected that if the autoencoder is properly trained then from this latent space representation the decoder will be able to decode this latent space representation of the data to give you the reconstructed data exact.
data which is almost a replica of your input data x.
So, in other sense we can say that while coding the autoencoder actually gives you a transformation that transforms the data from a higher dimensional space to a lower dimensional space.
And while doing so it ensures that your reconstruction error end to end reconstruction error when the data will be reconstructed that is minimized.
So, this lower dimensional representation indicates that the loss it tells that the loss that you incur while compressing the data or while trying to extract the structure from the data the loss incurred will be minimum.
So, in other case you can consider the function of the autoencoder is to go for dimensionality reduction of the input data.
And if I consider the function of the autoencoder as a dimensionality reduction function.
Then, we have to see that what is the other dimensionality reduction function that we have and it is known that traditionally the dimensionality reduction is done by an algorithm known as principal component analysis.
So, naturally then the question comes that how does principal component analysis compare with autoencoders.
So, in order to do that before going to that comparison.
for the benefit of those who does not know what is principal component analysis, let me briefly say what is principal component analysis.
So, in case of principal component analysis our input is let us assume that input is a set of vectors x.
So, I put this as set of vectors x 1, x 2, x 3 up to say x .
x n. So, assuming that we have n number of input vectors and each of the input vector may be of dimension say let me put as capital D .
So, capital D is the dimension of the input vectors that means, each of x 1, x 2 up to x n each of them has D number of components.
Now, once I have such a collection of vectors x, you define the covariance matrix as C x which is defined as expectation value of x minus mu x into x minus mu x transpose .
So, what is mu x?
Mu x is nothing but mean of the input vectors.
So, I have n number of input vectors.
So, this will be 1 upon n sum of x i where i varies from 1 to n. So, this is my mu x and x is each of these individual vectors.
So, I define the covariance matrix of the set of input vectors as the expectation value of X minus mu X into X minus mu X transpose.
And now if you analyze this covariance matrix, so what will be the size of this covariance matrix?
As the vector is n dimensional, so this covariance matrix will be a D by D matrix as D is the dimension of the feature vectors.
So, this covariance matrix will be a d by d matrix.
And in this covariance matrix the diagonal elements will give you the variance of the individual components of the vectors.
That means, if I take the first vector first component of x 1, first component of x 2, first component of x 3 and so on and I compute the variance of all those first components that variance will be my sigma 1 1.
1 which is a first component in this diagonal vector.
Similarly, sigma 2 2 will be the variance of the second component sigma d d will be the variance of the d th component of the last component.
And all the off diagonal elements in this matrix will give you the covariance of different components.
So, sigma 1 2 is the covariance between the first component and sigma second component.
1 2 up to sigma 1 d, sigma 2 1, sigma 2 2 up to sigma 2 d and so on this is sigma d 1, sigma d 2 up to sigma d d. So, this is say my covariance matrix and I want to compute the eigenvectors i and the eigenvalues lambda.
So, the way you compute this eigenvalue is from each of these diagonal elements.
you subtract lambda and then make a determinant.
So, the determinant will be sigma 1 1 minus lambda sigma 1 2 up to sigma 1 d sigma 2 1 sigma 2 2 minus lambda sigma 2 3 goes on sigma d 1 sigma d 2 sigma d d minus lambda make a determinant and equate this to 0 .
So, once you put this you find that this determinant will give you a polynomial of degree d and it will be a polynomial in lambda.
So, when I solve this I will get d components or d values of lambda.
So, I will get lambda i where i varies from 1 to d and then for each of this lambda i, I can compute the corresponding eigenvector.
So, the way you compute eigenvector is if for lambda i the corresponding eigenvector is say e i, then the equation that has to be satisfied is C x e i have to be equal to lambda i times e i.
So, I know what is C x, I know what is lambda i, you solve this equation I get the ith eigenvector which is e. So, this is how.
So, given a set of vectors I can compute the covariance matrix from the covariance matrix I can compute the eigenvectors or eigenvalues and for every eigenvalue I can compute the eigen corresponding eigenvector.
And you see that if this covariance matrix is real and symmetric which usually is then the eigenvectors are orthogonal.
And what this lambda tells you or the eigenvalue tells you.
it simply tells you that what is the scatter or what is the variation of the data in the direction of the corresponding eigenvector.
So, if lambda 1 is very high that in that indicates that the variation of data in the direction of the corresponding eigenvector which is E 1.
So, lambda 1 very high indicates that the variation of data in the direction of E 1 is very high right.
So, given this .
Once, I have this eigenvectors then I can define a transformation.
So, how do you define this transformation?
For defining a transformation you make you form a transformation matrix A, this transformation matrix A is formed using the eigenvectors as the rows in the transformation matrix.
So, the first row in this transformation matrix is E 1, the second row is E 2.
So, the last row is E d you remember that we had d number of eigenvectors as our input vector is of dimension d. And how I get this transformation matrix or how I arrange such eigenvectors into rows of this transformation matrix is in this transformation matrix E 1 corresponding to that my eigenvector is lambda 1 and for vector E 2.
my corresponding eigenvalues eigenvalue is lambda 1 and corresponding to this I have my eigenvalue which is lambda 2.
So, I arrange this eigenvectors as rows in this transformation matrix in descending order of the corresponding eigenvalues.
So, here E 1 is the first row, E 2 is the second row that indicates that I have lambda 1 greater than lambda 2 right.
2 for a pair of eigenvalues say lambda i lambda j where both i and j varies from 1 to d because I will have lambda varies from lambda 1 to lambda d. So, for this pair of eigenvalues lambda i and lambda j if lambda i is greater than lambda j that indicates that E i the eigenvector E i will occupy a higher row than E j in this transformation matrix.
So, for this E i I have the corresponding eigenvalue lambda i for E j I have the corresponding value lambda j.
So, as lambda i is greater than lambda j in this transformation matrix A E will E i will occupy a higher position than E j.
So, that is how this transformation matrix is formed.
So, once I have this transformation matrix, then I can define a transformation.
My input vectors are x, I can have a transformation which is given by a times say x k the kth vector minus mu j mu x which is the mean of the vectors.
So, this defines a transformation.
So, this gives me a transform vector which is y k .
So, for kth vector this transformation gives me a transformed vector y k. So, if you look at this transformation what this transformation is doing?
If I take the first component of a x k, so difference of first component of x k and first component of mu x right, this is transformed or the vector x k minus mu k is being projected onto vector E 1 because this is nothing, but the dot product of E 1 with x k minus mu x that gives that gives me the first component of y k. Similarly, the dot product of x k minus mu x with E 2 which is the second row in my transformation matrix gives me the second component of y k.
right.
So, this transformation that you get this is what is popularly known as K L transformation.
And I can use this K L transformation for data reduction in the sense that if I want to reduce the dimension from D to 2.
What I will do is in this transformation matrix A that I form this A instead of considering all the eigenvectors I will only consider E 1 and E 2 the eigenvectors E 1 and E 2.
And, the transformation will be same as this a times x k minus mu x, where a is now this is actually 2 by d matrix, I have 2 rows and d number of columns right.
So, this is a 2 by d matrix.
So, when you go for this transformation this y k you find that it will be a 2 by d 1 vector that means, it is a 2 dimensional vector.
So, just by truncation of this transformation matrix, I can transform the data from n dimension to 2 dimension or d dimension to 2 dimension.
So, that is what gives me a reduction in the dimensionality of the input data.
And this is what is popularly known as KL transformation and the components of this transform vector Y k that you get that is Y k.
1, the first component and y k 2 that is the second component after this transformation, these are what are known as principal components and the eigenvectors are the principal directions.
So, effectively what you are doing is you are transforming your input data into a space which is known as eigenspace and the eigenvectors being orthogonal, the eigenspace is also orthogonal and the projections in the eigenspace in every eigen direction are the principal components of the input data.
And by arranging the transformation matrix A in this form that is arranging the rows as eigenvectors in descending order of corresponding eigenvalues ensures that the error that you encounter by in by truncating some of the rows from the lower side ensures that the error that you encounter will be minimal.
So, let me stop here today, I will take up this illustrations with principal component in the next class.
Thank you.
Hello, welcome to the NPTEL online certification course on deep learning .
In the previous class we have talked about the linear machines and we have also discussed started our discussion onmulti class support vector machine.
Today's lecture we willdiscuss on multi class support vector machine loss function and also the optimization techniques.
So, what we have done in case of linear machine is that we have seen that linear machine is a function that transforms or that maps a d dimensional feature vector r d into a score function of dimension k. So, score function s that you get is also a vector of dimension k where k is the number of categories or the number of classes.
So, if I expand this the function looks like this that f given an input vector x i f x i w b where w is the weight matrix and b is the bias vector is given by w x i plus b which is equal to the score function s and we have seen that the score function s has got k number of components where k is the number of classes or the number of categories.
So, given this we have seen that the score for the jth class or the jth category is the jth component of the score function S which in this case we have written as S j and this S j is nothing but W x i the jth component of this.
So, we can write this as f x i w the jth component.
So, the jth component of this gives me the score for the jth class of the ith vector x i.
which belongs to class y i.
So, these are the training vectors and because this x i the input vector belongs to class y i.
So, the score function component component y i must be maximum.
So, when this linear machine gives you the score function the score function the y i component of the score function must be maximum because we have taken x i belonging to class.
y i.
And we are not only satisfied with the score function to be maximum, we also want that the score function should be more than the score function of other classes by at least a threshold delta.
That means, taken any other class S j or S of y i the score function of the class y i must be greater than S j, where j is any other category other than y i.
So, this difference must be greater than some threshold delta.
So, accordingly for the ith component the loss function that we get L i which is nothing, but maximum of 0 and S j minus S y i plus delta.
So, you find that as long as S y i is greater than S j, then it is greater than S j by an amount delta.
So, this amount S j minus S y i plus delta will be less than 0.
Whereas, if S j is equal to S y i, then this function will be equal to delta and if S j is greater than S y i, this function will be greater than delta, where delta is a positive threshold.
So, in that case when you take max of 0 and this output will be this only if it is greater than 0.
And, the output of this max function will be equal to 0 only when S y i is greater than S j by at least this threshold amount delta.
And, you sum it over allj not equal to i, I get the loss component S i.
And, the overall loss that you get isgiven by sum of all these loss components.
So, to explain this we had taken an example that suppose I have some x i y i where this y i is equal to 2 that means, this x i belongs to category 2.
And the suppose the score function that is computed is given by this s equal to 10, 30, minus 20 and 25 and let us assume that we have a threshold delta which is equal to 10.
So, if I compute L i in this case i is equal to 2 because we have taken x i from category 2.
So, if I compute L i you find that it will have these three components one is maximum of 0, 10 minus 30 plus 10 this 10 is delta, the first 10 is the score function for category 1 and 30 is the score function for category 2.
So, this is 0.
10 minus 30 plus 10 which becomes minus 30.
So, the corresponding max function gives you an output 0.
Similarly, for the second case it is max of minus 20 which is core function for category 3 minus 30 that is core function for category 2 plus 10 again this part becomes negative.
So, output is 0.
And for the other one the score function is 25 which is the score function for category 4.
So, it is 25 minus 30 plus 10 and that gives you an output 15.
So, as a result L 2 is equal to 15.
So, when I compute this core functions as I said before that if I take summation of this core function of over all value of i for all value of j not equal to y i, I getthe overall score function.
Now, this is how we compute the score function and if you look at the nature of this score function you find that as long as this component S i S j minus S y i ispositive, right.
So, this ok. My score function will be equal to 0 only when this term s j minus s y i as long as this term is less than delta .
delta, my output will be 0.
If it is if it is S y i minus S j this is greater than delta, then output will be equal to 0 because my classification is correct otherwise the output will be high.
So, if I plot this score function with respect to S j minus S y i.
the nature of the score function or theplot of the loss function will be something like this and this loss function is what is known as hinge loss .
So, now we have also talked about a term called regularization because you find that we decide about the classification to be correct or not or whether we are satisfied with the classification output or not depending upon the difference S j minus S y i .
And, which is nothing but W j transpose that is W j is the jth row of the weight matrix W that transpose X i minus W y i transpose X i.
And, if you find you find that if I scale up this W by a factor lambda then the score the difference S j minus S y i will also be scaled up by the same factor lambda.
Say for example.
For some w if the difference S j minus S y i is equal to 15 and if I multiply w or scale up w by a factor 2 then the same difference S j minus S y i will be 30.
So, there are many possible values of w for whichthe value ofthe difference of S j minus S y i can be greater than delta right.
So, what I need is I need to find out an optimum W or best value of W which will satisfy all my requirement.
So, as a result I have to include a regularization termwhich is a function of W or the weight matrix and this regularization term is usually taken to be a L 2 norm.
So, for regularization term L 2 R w becomes lambda times W K L square where you take the summation of W K L square over all value of K and L .
And, as a result our overall loss function becomes L is equal to sum of L ithat divided by n where n is the number of training samples we have plus lambda times R w or the overall loss function in the expanded form is given by this .
So, given this overall loss function now what we .
need to do is we need to optimize this loss function or minimize this loss function.
And in addition to that what we need to do do is we have to also choose the hyper parameters.
So, if you look at the previous expression you find that we have got two hyper parameters over here one of the hyper parameter is delta which is the threshold that we have used and other hyper parameter is lambda.
term which is in the regularization term.
So, if you remember this first term in this loss function we told this we defined this as data loss and the last one is what is known as a regularization loss .
So, we have got two hyper parameters over here one is the threshold delta and other one is this lambda .
So, I need to choose that what should be the proper values of these two hyper parameters.
However, if you look carefully you find that both lambda and delta they control the same tradeoff that is if the lambda is more the difference of S j and S y will also be more, if lambda is less the difference of S j and S y i will also be less.
So, accordingly I will have an effect of lambda.
And, due to this we can safely choose the value of lambda is equal to 1.
So, that is what we have done over here the value of lambda is taken to be 1.
And, accordingly when you go for minimization of this loss function the value of lambda will be chosen.
So, we have taken value of delta equal to 1 and while do your minimization accordingly the value of lambda will be chosen.
And, you find that we have talked about the binary support vector machine where we have said that given the separating plane between two classes omega 1 and omega 2 during training I will be satisfied only when we find that W transpose X i for a training vector X i is more than a normalized distance one.
So, accordingly a loss function for a binary SVM can be defined like this it is max of 0 minus y i W transpose X i .
So, you find that as long as this W transpose X i is lose less than 1 that is the normalized distance from W transpose X equal to 0 that is the separating plane is less than 1 your loss function will be positive.
If it is greater than 1 then only loss function will be 0.
And the regularization term in case of two class support vector machine if you remember we had put this as half of W square that was the regularization term in case of the two class support vector machine.
So, you find that this binary SVM or two class support vector machine is nothing, but a special case of a multi class support vector machine right.
So, now let us go for how to see how this loss functionwhat is the nature of this loss function.
So, for illustrating this I consider a three class problem and stress of vectors that I consider is one dimensional vector.
So, suppose I have got a three classes given by the weights W 1, W 2 and W 3 and as I said that I am considering onedimensional vectors.
So, each of this W 1, W 2, W 3 are scalars ok and I also take three one dimensionaltraining points x 1 taken from class 1, x 2 taken from class 2 and x 3 taken from class 3.
So, given this situation, you find that the loss functions thatwe will get is given by L 1 is equal to max of 0 and W 2 transpose X 1.
So, this W 2 transpose X 1 is nothing, but score of class 2 for this training vector X 1 and this is the score of class 1 for training vector X 1.
So, the loss function L 1 will be max of 0 W 2 transpose X 1 minus W 1 transpose X 1 plus 1 .
plus max of 0 and W 3 transpose X 1.
So, this W 3 transpose X 1 is the score of class 3 for training vector X 1 minus W 1 transpose X 1 plus 1.
Similarly, we define loss functionfor class 2 L 2 for the support vector 2, we also define the loss function L 3 for support for the training vector.
x 3 and the overall loss is given by one third of L 1 plus L 2 plus L 3 .
So, given this now if we try to visualize how this loss function is.
So, here you find that in case of L 1 if W 1 is very small if this weight vector W 1 is very small then W 2 transpose X 1 minus W 1 transpose X 1 .
1, this term will be positive assuming that W 1 transpose X 1 is less than W 2 transpose X 1.
So, output loss function will be positive which is max of 0 and this.
And this particular term will be 0 only when W 1 transpose X 1 is more than W 2 transpose X 1 at least by W 1.
That means, as long as W 1 is very small your loss function given by this value of loss is positive.
Similarly, in this case as long as w 1 is less than w 3 here also it will be positive.
However, as w 1 goes on increasing the loss function gradually reduces and ultimately it becomes 0 and remains 0 for this component L 1.
Similarly, for component L 2 you find that when w 1 is.
very small compared to this.
So, that this term becomes negative the output will be 0 and as W 1 goes on increasing eventually this term becomes positive.
So, output L 1 will also become positive and it will have a certain value it is not 0 and same is for L 3 .
So, by this understanding having this understanding now we can try to plot the different loss functions L 1, L 2 and L 3.
So, if I plot L 1.
with W 1 you find that as we said that when W 1 is very small the loss component L 1 is positive and is it goes on reducing as value of W 1 increases.
Similarly the other component L 2 initially it remains 0 with respect to W 1 initially it remains 0 and when W 1 becomes very high in the sense that W 1 x 1 becomes more than W 2 x 2 by a factor 1 by the threshold 1 then the loss function becomes positive and it is like this .
And, same is the case with component L 3 and my overall loss function is average of all these three components L 1, L 2 and L 3 and the overall loss function is given by this.
So, by looking at this figure on the right which gives you the overall loss function you find that the loss function is convex right.
So, this can be solved using convex optimization problems.
because the loss function that you get is convex.
Now, here the situation is very simple we can visualize it very easily because I am considering W 1 to be a scalar or a one dimensional vector.
Now, what happens in case of multiple dimensions?
Usually our weight vectors orthe samples sample vectors they are of very very large dimension maybe of the order of thousands.
So, the visualization of the loss function in such cases is very very difficult.
However, we can try to visualize thatsection wise.
So, what I do is now I know that my loss function is defined in a high dimensional space.
So, I can take a single point in that high dimensional space at random which is a W and I take a direction W 1 which is also at random.
So, this w 1 I take as a direction passing through the selected selected point w. And as we move along the direction of w 1 you go on recording the loss function or effectively what you do isevery point in the direction of w 1 passing through w is represented by this expression w plus a times w 1 where a indicates that what is the position of the point on the line w 1 passing through w.
And, we are taking the loss function L at those different points by varying a I get different points and you take the loss function W at all those different points.
So, what I can do is I can now plot the loss function loss as a function of a or as a varies I get different points on the line and now if I record the loss functions I get a loss function which is of this form.
which is L w plus A w 1, A is the parameter which defines which determines the points on line w 1 in the direction of w 1 passing through w and you find that the loss function which will which will be of this form.
I can also try to define loss function on a plane if I take the section on a plane.
So, in that case what I have to do is instead of taking just w 1 a single line I have to take w 1 and w 2 as two different lines.
ok. And by giving that every point on that plane defined by these two directions W 1 and W 2 now can be determined by this expression W plus a times W 1 plus b times W 2.
And again if I record the loss value for different values of a and b which are the parameters in this particular case I get a loss function which is given in this form.
So, this is the plot of the loss function in 2 dimension .
So, in both the cases whether I take the previous one like this where you find that again the loss function is a convex function it has a minimum somewhere over here or I take the next one that is visualization of the loss function in a plane you find that here again the loss function is a convex one.
where the blue that is in this particular case at the center here it is minimum and the red represents maximum loss function.
So, I have the minimum of loss over here.
So, the loss function in in two dimension again shows that it is a complex one.
And this is the plot that I get if I consider or if I plot the loss function only for a single sample or a single vector.
vector.
And when I average this over multiple number of training vectors the loss function becomes something like this.
So, you find that this averagingover all the training samples smooths the nature of the loss function.
So, once I have this loss function next what I need to do is I have to optimize this loss function or I have to minimize the loss function.
So, for minimization gradient as we have done earlier we take the gradient descent approach.
So, I have to take the gradient of the loss function that I have and a modify w in the direction of the negative gradient.
So, here what I have is as we have already said that the overall loss function is given by this where this is the data loss component and this is the regularization loss component .
If I take So, I have to optimize this loss function in order to find out the value of w for which this loss function will be minimum.
So, I take the gradient of this loss function with respect to w y i, I also take the gradient of this loss function with respect to w j.
So, when you take the gradient of this loss function with respect to w y i, you find that the expression of the gradient will be.
that it is sum of x i that is gradient of loss function with respect to w y i is nothing, but sum of x i only in those cases where w j transpose x i minus w y i transpose x i plus delta is greater than 0.
And because in all the cases where w transpose j transpose x i minus w y i transpose x i plus delta is less than 0.
the loss function was 0.
So, for those cases those x i's are correctly classified by or w. So, in such cases I need not modifythe weight matrix w. So, this gradient only takes the sum of all those x i all those training vector for which w j transpose x i minus w y i transpose x i plus delta is greater than 0.
That means these are the that x i leads to an error.
plus if you take the gradient of this term this gradient will be actually twice lambda timesW y i.
So, in this case it is written with respect to anotherconstants eta eta times W y i.
In the same manner if you take the gradient with respect to W j then it also becomes sum of y i where W j transpose x i minus W y i transpose x i plus delta is greater than 0 .
So, we will take the sum of only those training vectors for which this condition is true, we will not consider those training vectors for which this condition is not true.
That means, those training vectors are correctly classified for the by the current W. And plus from this regularization term when you take the derivative of this regularization term with respect to W j or takethe gradient of this regularization term with respect to W j.
the term that I get is zeta times W j .
So, these are the gradients and using these gradients we go forour optimization step or gradient descent step where we get the gradient descent as at the kth instant if my weight vector was W y i k then the nextiterated value of W y i is becomes W y i k plus 1.
which is 1 minus zeta times w y i k plus 1 over n sum of x i for all those x i which satisfies this condition.
Similarly, the iterated value of w j at instant k plus 1 from instant k from the iterative state k is given by w j k plus 1 is equal to 1 minus zeta times w j k minus 1 over n sum of all those x i for which this condition is satisfied.
So, we will we will not consider those sample vectors which does not satisfy this condition because we assume that those vectors are correctly classified by the W j at the kth instant ok.
So, if you look at these two gradient descent steps you find that what it indicates is first you are modifying.
W y i which was there at the kth instant by the regularization term.
So, this is a component which is coming from your the regularization error part or regularization loss part and this is the component which comes from your data loss part.
So, that is how iteratively you go on optimizing or minimizingthis loss function and.
when it converges the value of weight matrix that you get gives you the linear machine or the support vector machine for multiple classes.
So, we will stop heretodays lecture we will come back in the next day.
Thank you.
Hello, welcome to the NPTEL online certification course on deep learning.
In our previous lecture, we had talked about linear discriminator and we had also talked about the perceptron algorithm in which a linear using which a linear discriminator can be designed.
Today we are going to discuss about the support vector machine.
And if you remember that the classes that we have considered in the previous day are actually two classes.
So, we wanted to have a linear discriminator.
which discriminates the vectors belonging to two different classes.
And when we talk about support vector machine, we will continue with two classes initially, but later on we will move to multi class classification problems.
So, before we go for support vector machine, I will just quickly recapitulate what we have done in the previous class in the linear discriminator.
So, we what we had done in the previous class is we have taken two sets of feature vectors.
So, this is a set of feature vector which belong to class omega 1 and we have taken another class another set of feature vectors from class omega 2.
And then what we tried to see is we assumed that these two classes these two sets of feature vectors are linearly separable and assuming linear separability we have tried to find out.
a linear boundary or a hyper plane which separates these two classes of each of vectors.
So, you find that an equation of such a linear boundary will be given by this a transpose x plus b equal to 0, where you find that this vector a is a vector which is orthogonal or normal to the separating plane.
So, the vector a will be So, this will be the direction of vector a and as we have assumedthe existence of a separating boundary which is linear, we know from ourschool level mathematics that such a linear boundary divides the feature space into two half spaces, one of the half space is positive half space, the other half space is negative half space.
So, for any feature vector x.
which belongs to positive half space, I must have a transpose x plus b which is greater than 0.
And for every feature vector lying on this plane, so if I take a feature vector lying on this plane for this feature vector a transpose x plus b will be equal to 0.
So, as we have taken a number of feature vectors from the two classes omega 1 and omega 2, if I take any vector x from the class omega 1, so this is the set of vectors belonging to class omega 1.
And, this surface this being the linear boundary having equation A transpose X plus b equal to 0 for every X belonging to class omega 1 which are my training vectors this condition must be satisfied that A transpose X plus b have to be greater than 0.
In the same manner if I take a feature vector X from class omega 2 where this feature vector X falls on the negative side of the linear boundary.
this condition that A transpose A x plus B less than 0 must be satisfied.
So, for this surface for this linear boundary which satisfies this equation A transpose B equal to 0 as we have said that the vector A is orthogonal to the surface.
So, if I modify vector A that means, the orientation of this linear boundary will change and the value of B which is nothing, but a bias it decides the position of this separating plane in the feature space.
So, A gives you the orientation and b gives you the position.
Now, given this I can also represent this equation a transpose b a transpose x plus b equal to 0 in an unified form a transpose x equal to 0.
And for writing this equation a transpose x plus b equal to 0 in the form a transpose x equal to 0 I have to do certain modifications.
So, what are those modifications?
I have to modify a like this that now this modified a contains all the previous components a 1 to a d of my initial feature vector a, initial solution vector a and this bias term b is also now included in the same vector a in this modified vector a.
And in order to do this .
the feature vector x has to be modified as all the components of x that is x 1 to x d are remaining as it is.
Now, what I have to do is I have to append an additional component to x which becomes 1.
So, with this modification your a transpose x plus b equal to 0 now gets modified to a transpose x equal to 0.
So, the my bias term b is included in the solution vector a .
So, you find that the implication of this equation is now this separating plane always passes through the origin in my d plus 1 dimensional space.
In the earlier case depending upon the value of bthe separating plane might have been anywhere within my feature space, but in this modified form as I am increasing the dimension of the feature vector by 1 in this modified feature space the separating plane always passes through the origin.
So, given this my situation will now be of this form the classification will rule now will remain that for every x sorry this should be x for every x belonging to class omega 1 I must have a transpose x greater than 0.
So, this y have to be x right.
For every feature vector x in this modified form taken from class omega 1 I must have a transpose x greater than 0.
and if it is taken from class omega 2, I must have a transpose x less than 0.
So, for correct classification the classification rule remains the same.
And I can do another modification that is so, what does it mean is this that coming over here you find that as one of the components of the feature vector x I have made equal to 1.
So, in the modified form the feature vectors will appear like this .
So, if this is the x 1 component and this is the x 2 component, x 2 component has been made equal to 1.
So, this will be the arrangement of feature vectors in this case and a being the orthogonal to my separating plane, these vectors which belong to class omega 1 appear in the positive half space of the of the separating plane and these vectors which belong to class omega 2 appear in the negative half space in my feature space.
I give another modification that is all the feature vectors which come from plus omega 2 I negate them.
That means, a feature vector x if it is taken from class omega 2 I make it minus x.
What is the advantage?
By negating all the feature vectors coming from omega 2 negating and negating them my classification rule becomessame irrespective of whether x is taken from class omega 1 or x is taken from class omega 2 .
0, because in every case my classification rule becomes a transpose X greater than 0.
And if I find that given a solution vector a for any X irrespective of whether this X belongs to class omega 1 or X belongs to class omega 2, if I find that for any such X a transpose X is less than 0, immediately I can say that this solution vector a misclassifies the corresponding X.
And, whenever there is a misclassification I must try to update a such that the modified a or update a will correctly classify x.
Let us see how we can do it.
So, my situation is something like this.
Now, that in the previous case you find that all these feature vectors belonging to omega 2 they were on the negative side ofthe separating plane.
Now, after negation the negated feature vectors belonging to class omega 2 now comes over here and you find that after negation all the feature vectors the negated feature vectors belonging to class omega 2 and also the original feature vectors belonging to class omega 1 all of them fall on the positive side of the separating plane.
Now you find that I can observe one more thing.
plane, that is what is the limit of the separating plane or what is the limit of the solution vector a.
So, you find that if I rotate the separating plane in the anti-clockwise direction, then the limit to which I can rotate this is given by this.
Because if I rotate it further in the anti-clockwise direction, then this feature vector belonging to class omega 1 is going to be misclassified.
In the same manner, if I rotate it in the clockwise direction, then this is the limit that I can have.
because, if I rotate it further in the clockwise direction then this feature vector belonging to class omega 2 that is going to be misclassified.
So, these two this one this position and this position gives me limit of the separating plane.
And as I have limit on the separating plane in the same manner I can have limits on the correspondingsolution vectors.
So, you find that as this position is the limit of plane, the corresponding limit on the solution vector is this which is orthogonal to the separating plane.
Similarly, here the corresponding limit on the orthogonal or the solution vector is this.
So, it clearly says that I must have an a which correctly classifies all the training vectors must be within this region which is my solution region.
So, the approach for designing a linear classifier should be such that I must get a solution vector lying within this solution region.
So, in order to do this what we can do is for every x which is misclassified as I know that for every x which is misclassified by a I should have this condition that a transpose a x less than 0.
You remember that all these x's that we are talking about all these feature vectors x that we are talking about these are all training vectors that means, for all the vectors I know to which class they belong.
So, as we have said so far that given any a if I find that a transpose x becomes less than 0 that means, that a misclassifies that x and this misclassification leads to an error.
So, I can have an error measured for this misclassified sample which I will put as minus a transpose x.
So, as a transpose x is less than 0, so minus a transpose x is positive that means, if I have a misclassified sample that leads to a positive error.
So, whenever a correctly classifies all the samples then this error will be equal to 0.
So, what I do is for given a you identify all the feature vectors which are misclassified that means, all the feature vectors for which a transpose x becomes negative.
And then you define an error function which is called the perceptron criteria function.
So, I write this as J P a which is a function of now the solution vector a.
which is assum of minus a transpose x and this summation has to be taken over all x which are misclassified.
And once I have this error measured, then I can modify a following gradient descent algorithm, we will discuss more about gradient descent algorithm later.
So, this gradient descent algorithm says that I have toshift a in the direction of the negative gradient.
So, for updation of a I have this updation rule that a gets a minus gradient of J p a and this gradient is scaled by a scale factor eta which is known as rate of convergence.
So, this will be my weight updation rule using gradient descent procedure.
So, in this particular case where I have this perceptron criteria function J p a.
given by minus a transpose x for sum of that for all x which are misclassified .
So, using thiserror function the gradient descent procedure now becomes if you take the gradient of JPA the gradient of JPA becomes minus sum of x .
for all x which are misclassified.
And accordingly my training or learning algorithm will be like this that initially you choose weight vector a 0 at random and then I will go on updating this weight vector a iteratively.
So, in any kth iteration if a k is the weight vector using this a k you try to identify all the feature vector x which are misclassified.
And, once you identify all such feature vectors which are misclassified, then for the next iteration or the next updated weight vector a k plus 1, I can get it from a k by modifying a k as a k plus 1 gets a k plus eta times sum of x for all x which are misclassified.
So, this is my weight updation rule.
Let us say with diagram what does it mean?
So, as we have seen before that given these two sets of vectors belonging to class omega 1 and omega 2.
I have the region solution region which is this.
So, that means, any solution vector must lie within this region.
So, what I do is initially let us assume that we have a separating plane which is given as this.
And here you find that this separating plane misclassifies these two samples which belong to class omega 1.
And, the weight vector a is this which is obviously, outside the solution region.
So, I have to update this vector a by adding to it the sum of these two misclassified vectors.
And when you add the sum of these two misclassified vectors is obviously, in this direction.
So, a has to be moved in this direction.
And if I have sufficient eta eta isproper, then possibly we will stop within this solution region and I get the solution.
But, if eta is large then we will cross the solution region and we will move somewhere over here .
So, here in this case after adding modifying this A using some of these two misclassified samples suppose the next separating plane comes out to be this.
And, again this separating plane as you see misclassifies these two samples and the corresponding solution vector A 1 is over here .
So, again to a 1 you add some add some of these two vectors and some of these two vectors is in this direction.
So, a 1 has to be moved in this direction.
So, at the next level my separating plane will be somewhere like this by updating a 1 and now you find that I have a vector which falls within the solution region and this is my a 2.
And, as it falls within the solution region at and it correctly classifies all the samples whether it belongs to class omega 1 or belongs to class omega 2.
So, I am satisfied with this solution vector.
So, this is the approach for designing a linear classifier.
Now, as we have seen that as the solution region is this.
So, any vector within this region should satisfy my purpose.
But, if the solution vector comes very close to this solution boundary possibly that is not a good solution because it is prone to error.
So, I would like to have a vector which is well within this solution region so that the classification my classifier becomes very very robust.
So, let us see how we can do it.
So, what I mean by this is say given all these different are vectors coming from two different classes and earlier as we said earlier that my solution classification rule is A transpose A x plus b b greater than 0 if x belongs to class omega 1 and A transpose A x plus b is less than 0 if x belongs to class omega 2.
And this is one such separating plane which satisfies this criteria, but is it unique?
Obviously, not.
Let us see.
So, this is one such separating plane that we have seen which satisfies this region.
I can have another separating plane which is given by this blue line that also satisfies this criteria.
This is a separating plane which also satisfies this criteria, this is a separating plane which also satisfies this criteria.
So, I have infinite number of such possibilities.
So, I have to identify that out of all these different possibilities which one is .
should be the preferred solution and that is where the support vector machine comes into picture.
So, to illustrate this let us take a very simple case.
So, the case is like this I assume that these are the vectors which belong to class omega 1 and these are two vectors which belong to class omega 2 and I have a separating plane which separates in these two classes.
And, now as I have shown previously that for any x belonging to class omega 1, my correct classification criteria is a transpose x plus b greater than 0 and for x belonging to class omega 2, I had a transpose x plus b less than 0.
Now, I can take another strategy.
Let us assume.
that every training vector is given as a pair that means, along with the training vector we also have its class level right.
So, I can put it this way that a training vector x i is given as a pair x i y i where this y i indicates that what is the class or to which class x i belongs.
So, I will put y i 1 as plus 1 if x i belongs to omega 1 and y i will be is equal to minus 1 if x i belongs to omega 2 .
So, if I assume this then you find that both of these conditions that a transpose x i plus b greater than 0 for x i taken from class omega 1 and a transpose x i plus b less than 0 for x i taken from class omega 2, both of them can be written in asingle form that is y i times a transpose x i plus b it has to be greater than 0.
So, this becomes my unified representation.
And if you compare this with what we discussed previously that we negated all x and had an unified representation all x taken from class omega 2 and had an unified representation or unified classification rule of a transpose x greater than 0 for correct classification which is .
exactly same as this.
It is just another way of representation that you multiply by y i a transpose x i plus b, where y i is plus 1 if x i is taken from class omega 1 and y i is minus 1 if x i is taken from class omega 2.
And by doing that I get an uniform classification rule that y transpose y i into a transpose x.
plus b will be greater than 0 whenever X i is correctly classified by this vector a and the offset of the bias term b ok.
So, now as we have seen previously that I can have different options of the separating plane and what I have to do is I have to choose out of all those options which is the correct option.
So, if I take this particular separating then you find that my margin is given by this.
You remember one more thing that when I take a transpose x plus b for any feature vector x this is an indication of what is the distance of the feature vector x from the separating plane a transpose x plus b equal to 0.
So, given over here given in this equation if this is my separating plane.
my confidence of correctly classifying a feature vector x which is lying over here is more than my confidence of correctly classifying a feature vector over here or correctly classifying a feature vector over here.
And, what is this A transpose x plus b?
A transpose x plus b as I said it is the distance of vector x from the separating plane A transpose x plus b equal to 0 or in other words.
a transpose x plus b upon mod of a this is the distance of x from the separating plane.
So, more the distance more is my confidence that I have correctly classified this sample x.
Now, going by this if I take the featurethe separating plane to be this I find that my margin is classification is given by this much .
That means, this feature vector the confidence level of this feature vector being classified correctly is this and the confidence level of this feature vector being correctly classified is given by this.
On the other hand if I take some other orientation of the separating plane say this, now the margin or my confidence level in classification is this much.
Let us take another say this one.
the margin is this.
So, you find that for every orientation of the separating plane I have different margins.
So, if my classifier is proper or the separating plane is proper or it is robust, then I must take that particular separating plane which tries to maximize the margin or in other words the separating plane which I use for classification of or the classification of the feature vector belonging to two different classes, this separating plane must be at a maximum distance from all the feature vectors belonging to belonging to two different classes.
That is this separating plane the distance of this separating plane from the feature vectors belonging to class omega 1 and the distance of the separating plane from the feature vectors belonging to class omega 2 .
From both sides this should be maximized.
So, this is just the introduction of support vector machine and the machine which gives such aseparating plane is nothing, but a support vector machine.
So, we stop the today's lecture over here and in the next lecture we will come across that what should be my strategy for designing such a support vector machine.
Thank you.
Hello, welcome to the NPTEL online certification course on Deep Learning.
In our previous class, we have talked aboutwe have recapitulated the discriminant function and the decision boundary.
We have talked about the nearest neighbor and K n n or K nearest neighbor classifier and we had started our discussion on linear classifier.
So, today let us continue our discussion on linear classifier and then we will move on to support vector machine.
So, you see here that the linear classifier that we are discussing over here is for a two class problem that is we have samples belonging to two different classes say omega 1 and omega 2 and I assume that the samples are linearly separable and in which case given these two training samples from classes omega 1 and omega 2, I can separate these samples using a linear boundary.
y is equal to 0, where this a is a d d plus 1 dimensional Fisher vector, y is also a d plus 1 dimensional Fisher vector.
So, as we have said in our previous lecture that this a includes the vector which is perpendicular to the plane separating the feature vectors belonging to two classes and also the bias term.
And, y is obtained asappending 1 to the feature vectors of dimension d. So, that is how we get d plus 1 dimensional vector for a and also d plus 1 dimensional vector representing y that is the feature vectors.
And, we have also assumed that for all the feature vectors taken from class omega 1 y remains as it is whereas, for all the feature vectors which we taken from class omega 2 y is negated.
it, this is for the design purpose.
Why we have done it?
Because while designing the separator A transpose y equal to 0 for correct or incorrect classification I have an uniform decision rule that I should have always A transpose y greater than 0 if attaining vector y is correctly classified.
So, our original classification rule was if y is taken from class class omega 1 that then a transpose y will be greater than 0.
If y is taken from class omega 2 then a transpose y should be less than 0.
So, what we have done is we have simply negated y for all the samples taken from class omega 2.
So, for after negation whether the same samples are taken from class omega 1 or class omega 2 I should always have a transpose y greater than 0.
So, this is simply for the convenience of designing the decision boundary.
So, given this now let us see that we have seen that as we have our decision boundary given by a transpose y is equal to 0.
That means, the vector a is orthogonal to the plane separating the two feature vectors right .
So, can we have can we try to find out what should be should be the limits on a .
So, in order to see that Let us take a very small example, a simple example say something like this.
Here I have samples taken from class omega 1, I have taken a smaller sample size for clarity of the pictures.
So, these are the feature vectors which are taken from class omega 1 and these are the feature vectors which are taken from class omega 2.
So, what I want to have is I want to have a plane which separates these two sets of feature vectors one from class omega 1 and one from class omega 2.
So, how we do this?
So, I am trying to find out a plane something like this.
So, you find that this plane clearly separates the feature vectors from class omega 1 and the feature vectors from class omega 2.
Now, you find that there is a limit of orientation of this particular plane.
If I rotate this plane in say anti-clockwise direction.
So, this is my new position of the plane, this is the limit to which I can rotate because if I rotate it further then this is the point which is going to be misclassified.
Similarly, coming to the other side if I take this plane this is a limit on the other side because if I rotate it further in the clockwise direction then this is a vector which is going to be misclassified.
So, the range in which I can have this separating plane between these two classes is given by this.
So, this is the separating plane.
Given this separating plane what happens to our solution vector a because this solution vector a as we have seen is orthogonal to the plane a transpose y equal to 0 where y's are the feature vectors taken from class omega 1 and omega 2.
So, if I look at that you find that over here what I have done is in earlier case our feature vectors these were the feature vectors corresponding to class omega 2 on this side.
And what we said is for design purpose we want to negate this feature vectors.
So, that always I will have a transpose y greater than 0 while designing this is my .
decision for correct classification.
And I can always do it because now the feature vectors are the training vectors.
So, for every vector I know what is this class belongingness.
But you remember once your classifier is designed that means, once yourseparating plane is designed my decision rule will always be or the classification rule will always be that if A transpose Y is greater than 0, then Y should belong to class omega 1.
if it is less than 0 then y will be classified to class omega 2.
This is what we will do while classification or during testing because in that case the feature vector y that we want to classify I do not know from which class this has been taken because that is what I have to decide.
But during design with the training vectors I know from which from which class which training vector has come.
So, I can negate y in that case for all the y taken from class omega 2 and that is what has been done.
over here.
So, given this you find that when I have this limiting position of the separating plane, the vector a is this one which is orthogonal to this particular plane.
Similarly, on the other limiting side when this is the position of the separating plane, then my solution vector a is this one.
And you find that if the solution vector a is rotated towards left from this limiting position, then this is a vector which is going to be misclassified.
Similarly, if the feature if the solution vector a is rotated in the clockwise direction from this position, then this is a feature vector belonging to class omega 2 which is going to be misclassified.
So, that clearly tells me that I have a solution region and my solution vector.
A vector which is perpendicular or orthogonal to the separating plane must lie within this solution region.
If it is outside this then vectors belonging to class omega 1 will be misclassified, if it is outside on this side then the vector belong to class omega 2 are going to be misclassified.
So, my solution vector must lie within this conical region.
a feature vector of this a solution vector of this form.
So, for doing this I start my algorithm like this.
So, initially I assume that at zeroth instant my feature vector a 0 is chosen arbitrarily right and we have I have also said.
that at any kth step in my iteration I have a solution vectors say a k. And it is possible that this solution vector a k at the kth step will correctly classify for some of the samples and will be mis correctly classify some other samples.
So, for all the samples which are correctly classified I will always have a k transpose y greater than 0.
And, for all the samples which are incorrectly classified I will have a k transpose y less than 0 irrespective of whether the vector is taken from class omega 1 or taken from class omega 2 because all the training vectors taken from class omega 2 have been negated.
So, during design or during training whenever I come across this type of situation that a k transpose y less than 0 immediately I can conclude that.
that current a k has misclassified the vector y and for this misclassification I have to generate an error term and the error term that can be can be generated is a k transpose y and as it is negative I negate this.
So, whenever a k transpose y is negative I generate an error term which is minus a k transpose y which will always be positive.
That means, if I have any misclassified sample by a k I will have a positive error and this I do for all the samples which are misclassified by a k. So, you take the summation of this over all y which are misclassified and I call it the error j as a function of a.
a, because as all the y's are fixed because those are my training samples, but what I can vary is a.
So, I represent this as an error j which is a function of a and this is an error which is normally called perceptron criteria.
So, sometimes it is also written as jpa.
So, what is my solution approach?
If you look at the figure over here, you can You find that this was my solution region.
So, if I have at any instant of time a solution vector a over here, my approach should be that I should be able to push this a towards this side.
So, that it moves inside the solution region or at any instant of time if my solution vector a is on this side, I should push it in this direction or push it towards the solution region.
So, that eventually my vector will land in the solution region.
and I get the proper separating boundary.
So, what is the approach that I should take for this?
The approach can be that as I said that my error J a is given by a transpose y minus sum of this.
So, I can take an approach to reduce this error or to minimize this error.
by following an algorithm known as gradient descent algorithm.
What is gradient descent algorithm?
Let us take a very simple case something like this.
Say I have a variable x and I have a function f x and the function f x is something like this.
And iteratively I want to get a value of x for which f x is minimum.
So, somehow over here.
.
As in this case I want to get a value of a which will minimize this error j a .
So, you find that here what I can do is if I start somewhere over here .
So, this is my x naught , I will take the gradient of f x with respect to x.
So, what I will compute is del f x upon del x and over here the del f x upon del x will be this .
What I do is I move x in the negative direction of del f x del x.
So, I will move in this direction.
So, my x next time will be the previous x minus del f x del x and if I take this movement if I change x in the negative direction of the gradient I am moving in the direction of minimum f x.
So, this is what is known as gradient descent algorithm.
and we will talk more of this in details later.
But for the time being let me take that I will use this gradient descent algorithm to minimize my error term j.
So, what I have is I have j a is equal to a transpose y negative of this sum of this over all y which are misclassified and what I want to perform is I want to take the gradient of j with respect to a and which is nothing, but from here it is y minus take the summation over all y which are misclassified right.
So, my decision rule can be as we said said before that initially.
at the 0th instant I assume that a 0 will be chosen arbitrarily or at random.
So, the better term is I choose a 0 at random and then if I have a k at kth iteration So, from there I want to find out a k plus 1 which should reduce my error j a which was of the form minus a transpose y and for that I go for the gradient descent approach.
So, gradient of j a we have seen was minus sum of y for all y which are misclassified.
So, I can get a k plus 1 from the previous value a k as a k minus minus sum of y and I can put a rate of convergence which is eta.
So, that simply becomes my weight updation rule or vector updation rule as.
a k plus 1 same as a k plus eta times sum of y for all y which are misclassified.
And you remember that when I am talking this y talking this feature vector y the y will be as it is for the samples taken from class omega 1 and y will be negated it is negated vector for all the vectors taken from class omega 2.
So, given this.
Now, let us see how this ruleupgradation rule actually tries to refine a k plus 1 or the weight vector.
So, that eventually the weight vector pushesfalls in the solution region ok.
So, this is what we were talking about this is my solution region solution region is within this ok.
Now, if I take an initial separating plane something like this.
So, I am assuming that this is my initial separating plane and the weight vector a or a 0 is this one which is orthogonal to the separating plane.
So, here you find clearly that the initial solution vector which I have chosen at random.
because this separating plane has been chosen at random, it falls outside the solution region because the solution region is this .
So, naturally I have to upgrade or I have to modify this solution vector a 0, so that it is moved towards the solution region over here .
And for that what updation rule we have used?
The updation rule was a k plus 1 .
is a k plus eta times sum of y where y is misclassified.
So, what is the misclassified sample in this case?
I have these two samples which are misclassified by this separating vector right.
So, if you take the sum of these two feature vectors which are misclassified, I will get a feature vector, I will get the sum of the misclassified y which is in this direction.
And, this sum is scaled by some eta which is known as rate of convergence, I will come to what is the importance of this rate of convergence later.
So, this is a k or a 0, you add this sum of y or sum of these vectors this particular vector scaled by eta to this vector to get your a 1.
So, a 1 will be moved in this direction because sum of y misclassified y is in this direction.
So, eta times this is added to this initial vector which was chosen at random and the vector moves towards this direction.
Now, what is the importance of this state of convergence eta?
If the value of eta is very high then it is possible that I will jump this solution region and my modified vector will be somewhere over here.
So, you find that you have crossed the solution region for larger value of eta.
If the value of eta is very small maybe I will be landing somewhere over here which is again not in the solution region.
So, if the value of eta is very small then your rate of convergence so the rate at which you are moving towards the solution is small.
If the value of eta is very high then the risk is that you may overshoot the solution region and to go to the other side of the solution region.
So, your number of iterations to reach the solution will be larger.
However, if the value of eta is appropriate and then it is possible that your next modified solution vector the vector that you obtain will be in your solution region and which is the solution that you are looking for.
Now, whatever it is once I have this misclassified samples using misclassified samples you refine the weight vector a ok. And for by this refinement it is possible that the next weight vector.
the next separating plane that you will land that you will achieve is this one.
And you find that for this separating plane the weight vector is this which is again as we said that if the value of eta is very large in that case you can cross the solution region you can over shoot the solution region and that is what exactly has happened over here.
So, this is my weight vector which is falls in which falls on the other side of the solution region and the sample.
which is misclassified by this separating plane is this one right.
So, at the nextweight vector that I should get which is a 2 that gets from a 1 that was the previous solution vector which is this will be a 1 plus eta times this misclassified sample y.
And as this misclassified sample is in this direction this solution vector as I add eta times y to this will also move in this direction.
So, it is possible that the next solution that I obtain will be within this solution region is somewhere over here.
So, this approach clearly says that by taking the gradient descent approach to reduce the error, it is possible that I will get eventually I will get a separating plane which separates the two classes of the feature vectors the samples belonging to class omega 1 and class omega 2.
And that is possible when I have the samples which are really linearly separable.
If the samples are not linearly separable then it is not possible to obtain a separating boundary something like this.
And the approach that we have to take in such cases is what is known as minimum error criteria.
So, that the separating plane that you obtain does not totally remove the error, but the plane will try to reduce the squared error or the minima or it will try to minimize the squared error ofclassification.
So, in today's lecture what we have done is we have tried to find out a boundary between the samples taken from the two different classes, the samples which are provided for designing the separating plane are the training samples.
And for designing the separating plane what we have done is we have appended one to all the training vectors and the vectors taken from class omega 1 which actually falls on the negative negative half of the boundary we have negated them for design purpose only.
But, you keep in mind that once your separating plane is properly designed or once I get this equation A transpose y equal to 0 which is the equation of this separating plane by using the training vectors for classification of the unknown sample now my classification rule will be.
for an unknown sample say Y unknown, if it is greater than 0 then Y unknown is to be classified to class omega 1, if it is less than 0 then Y unknown has to be classified to class omega 2.
So, with this we will stop today's lecture, next class we will talk about the support vector machine.
Thank you.
Hello, welcome to the NPTEL online certification course on Deep Learning.
Thank you for joining me.
You remember in the previous class we started our discussion on the support vector machine.
So, in today's lecture we will continue with the same discussion.
So, in the previous class we have just introduced or gave a brief introduction of what the support vector machine is and today we are going to talk about what should be the design approach of a support vector machine.
So, we have seen that.
So, in our case we will assume again a two class problem.
So, we have the feature vectors given from two classes omega 1 and omega 2.
And all the training vectors we assume that are given as leveled pair in the sense that a training vector x i which is the ith training vector will be given as a pair x i y i where.
this y i indicates the level.
So, if the training vector y i is taken from class omega 1 that is if y i belongs to class omega 1, then we will setsorry x i belongs to class omega 1, then we will set the level y i to be plus 1.
And if x i the training vector x i is taken from class omega 2.
then we will set y i the level to be equal to minus 1.
So, that indicates that given a separating plane with an equation a transpose x plus b is equal to 0, if this is the separating plane between the feature vectors belonging to class omega 1 and class omega 2, then our classification rule was a transpose x plus b greater than 0 for x taken from class omega 1 or if x belongs to class omega 1 and a transpose x plus b will be less than 0 for x taken from class omega 2.
Now, by introduction of these levels.
that is y i is equal to plus 1 for if x i belongs to class omega 1 and y i equal to minus 1 if x i is taken from class omega 2 then I have a uniform classification rule that is I can write y i a transpose x i plus b will be greater than 0 if x i is correctly classified by the separating plane A transpose X plus b equal to 0 and this will be less than 0 if X i is misclassified by the separating plane A transpose X plus b equal to 0.
And, we have also seen as in this equation A transpose X plus b equal to 0, A is a vector which is orthogonal to the separating plane and b is a bias which indicates what is the position or location of the separating plane.
So, as as a is orthogonal to x if I modify a that means, the orientation of the separating plane will be different whereas, if I change b then the position or location of the separating plane will be different in my feature space.
So, for different values of a and b I have got I I can obtain different separating planes.
And maybe many of those separating planes will satisfy the same condition that is a i y i into a transpose x i plus b to be greater than 0.
Now, for different values of a.
the vector a and for different values of the bias b I get different such planes, but for each such plane I will have the different margins or different confidence level of classification.
So, what is that?
So, here I take this particular separating plane which separates between this set of feature vectors which belong to class omega 1 and these two feature vectors which belong to class omega 2.
Now, given this you find that if I take this particular separating plane, this separating plane gives me a margin which is given by this.
So, that the distance between these two planes gives me the margin or what is the confidence level ofthe confidence level given by this particular classifier .
Similarly, if I take another separating plane say this one, here again you find that the margin is given bythis much ok.
So, obviously, the margin given in this option is less than the margin given in the previous option.
To continue further, if I take this separating plane.
then again the margin is given by this.
So, out of so many options which one should be preferred and that is the scope of the support vector machine that is what the support vector machine does.
The support vector machine tries to get a separating plane which maximizes the margin and for such a separating planethe separating plane should be at a maximal distance from the vectors belonging to both the classes.
That means, the vectors belonging to class omega 1 should try to maximize the distance of the separating plane from the vectors belonging to class omega 1 and it also try to maximize the distance from the vectors belonging to class omega 2 right.
So, I should get that particular separating plane I should try to obtain that particular separating plane which maximizes this margin.
And for classification, my rule is that I must have y i times a i a transpose x i plus b that should be greater than 0 this is for the classification.
But as I am talking about the margin, I want that for correct classification or for reliable classification for every x i, the distance from the separating plane.
must be more than a certain threshold.
So, that distance as we said earlier that a measure of the distance is given by a i transpose X i plus b.
So, if a i transpose X i plus b equal to 0 that means, X i falls on the separating plane in which case the distance of X i from the separating plane is 0.
For any non-zero value if X i is taken from class when we got 1.
2, then I must have a transpose x i plus b to be greater than certain threshold say d and if x i is taken from class omega 2, then I should have a transpose x i plus b should be less than minus d. And this should be true for all the training samples, whether the training samples are taken from class omega 1 or the training samples are taken from class omega 2.
So, if X i is taken from class omega 1, then this should be satisfied that is a transpose X i plus b should be greater than d. And if the training sample X i is taken from class omega 2, then this one should be satisfied that is a i a transpose X i plus b must be less than d less than minus d. And by taking this particularoption, I have and uniform criteria that is a y i a transpose x i plus b should always be greater than d irrespective of from whichever class this training sample x i has been obtained.
What I can do is I can always normalize this expression.
So, while designing I can have the condition that y i a transpose x i plus b should be greater than or equal to 1.
And, I will use this approach while designing the classifier or while choosing the separating plane, but for classification my rule will be once I fix what should be a and what should be b after designing the separating plane or choosing the separating plane using the training vectors.
Then, for any unknown x my classification rule can be that A transpose X plus B greater than 0 indicates that X belongs to class omega 1 or if A transpose X plus B becomes less than 0 then my decision will be that X should be classified to class omega 2.
So, right now our aim is .
that I should choose this separating plane A transpose x plus b equal to 0 which satisfies the condition that y i A transpose x i plus b must be greater than 1.
So, that is after normalization.
So, how I can do that?
So, what I am saying is that this particular equation of this particular separating plane I should take that particular separating plane which maximizes this margin.
So, how I can obtain this margin and how I can maximize this margin.
So, for that let us take one vector on this margin which is say x plus and I have take I will take another vector on this margin.
which is say x minus.
So, x plus is taken within the class omega 1 region and x minus is taken within omega 2 region.
So, a vector x plus minus x minus is a vector drawn from x minus to x minus to x plus.
And once I have this vector then from here you find that I can obtain.
the margin which is given by this as a dot product of the vector x plus minus x minus with the unit vector in the direction of w right.
So, the situation that I have over here is I have taken a vector x plus in omega 1 region, I have taken a vector x minus in omega 2 region drawn a vector from x minus to x plus and then from this I have to find out the margin which is nothing, but dot product of the vector drawn from x minus to x plus with the unit vector in the direction of w which is nothing, but orthogonal to the separating plane and the unit vector in this direction is given by.
A upon mod of A.
So, the margin that you get is x plus minus x minus take the dot product of this or A transpose into x plus minus x minus upon mod of A.
This is what is the margin given by this particular sephatic plane .
And, now you remember that we had the situation because this x plus is on the margin.
So, I have a transpose x plus plus b is equal to 1 and because x minus is on the margin on the negative side or on the margin into omega 2 side.
So, I have this particular equation a transpose x minus.
plus b is equal to minus 1.
So, from here you find that a transpose x plus minus x minus just subtracting if I call it equation a from equation b, I get a transpose x plus minus x minus is 2.
So, by using this you find that the margin A transpose upon mod A into X plus minus X minus sorry is given by 2 upon mod of A .
So, as we said earlier .
that I should choose or I aim to choose that particular separating plane which maximizes the margin and the margin comes out to be 2 upon mod of a.
So, I should choose that particular a which maximizes this and here you find that obviously, as mod of a comes in the denominator, I can maximize this term indefinitely by making a smaller and smaller, but that is not the solution because the a and b that I choose also must satisfy the requirement that y i a transpose x i plus b that has to be greater than or equal to 1.
So, I have to minimize a subject to the constraint that a transpose y i .
into a transpose x i plus b have to be greater than or equal to 1.
So, it becomes a constrained optimization problem and as you know that to solve a constrained optimization problem we have to make use of Lagrangian.
So, here what I have to do is I have to form a Lagrangian using this particular constant.
So, the Lagrangian can be formed like this I form L as, as I have to minimize mod of w. So, the Lagrangian that I form there I write half of mod of w square, why I am taking at taking this as half of mod of w square will be clear very soon.
And then minus alpha i y i times a transpose x i plus b minus 1 take the sum of this over all i.
So, this becomes my Lagrangian for constant optimization problem.
So, this L of the Lagrangian has to be minimized with respect to sorry.
I am using the term a not w. So, let me put it like this that my Lagrangian L will be half of mod of a square minus sum of alpha i y i times a transpose x i plus b minus 1.
i and the summation has to be taken over all i.
So, this Lagrangian has to be minimized with respect to a and it has to be maximized with respect to our Lagrangian multipliers which are alpha i.
So, first for this optimization problem as you know that we have to make use of the differential operators.
So, first let us try to differentiate L.
with respect to a and if I do that it simply becomes a minus it becomes alpha i a transpose sorry it simply becomes alpha i y i x i sum of this over all i.
So, when I differentiate L with respect to a it becomes a minus sum of alpha i y i x i and that has to be equated to 0 which gives me the solution vector a or the orientation of the separating plane to be equal to sum of alpha i y i x i summation has to be taken over.
all i that is all the training vectors which are given for designing the support vector machine .
In the same manner if I take the differential of a l with respect to b what do I get?
The first term because there is no b over here this becomes 0 over here it becomes minus sum of alpha i y i yeah.
So, it is sum of alpha i y i b.
So, if I differentiate this with respect to b it simply becomes sum of alpha i y i and that I if if I equate to 0 this simply gives me that sum of alpha i y i has to be equal to 0.
2 intermediate solutions that is a is equal to sum of alpha i y i x i summation over all i and the other I get is sum of y i alpha i y i that is equal to 0 .
So, now let us see what Lagrangian that we had.
We had Lagrangian equal to half of mod a square minus sum of alpha i y i a transpose x i plus b minus 1.
This was the Lagrangian.
And over here a is nothing but sum of a alpha i y i x i.
So, putting that in this expression it simply becomes half of alpha i y i x i into I can write the other y as alpha i or alpha j .
j x j minus what I have over here.
So, I will put a transpose a is nothing, but a dot a.
So, let us put it as dot product .
So, over here again it becomes sum of alpha i y i x i again dotted with sum of .
alpha j y j x j.
So, that takes care of alpha i y i a transpose x i plus or minus b times sum of alpha i y i and sum of alpha i y i equal to 0 and then I get plus sum of alpha i right.
And, this simply gets gives me sum of alpha i minus double summation alpha i alpha j y i y j x i dotted with x j .
So, the final Lagrangian that I have is L is equal to sum of alpha i .
half alpha i alpha j y i y j then x i dot x j or x i transpose x j .
So, this is the final form of Lagrangian and you find that under .
we get as a, a is equal to sum of alpha i y i x i summation over all i and here it has to be summation over all j and summation over all i.
So, thus solution vector a is given by this expression alpha i y i x i .
taken summation over all i and what should be the values of alpha?
The values of alpha will be should should be those alphas which maximizes this expression of this Lagrangian.
So, now you can make use of any of the optimization tool to optimize L with respect to alphas and the set of such alphas that you get which maximizes this L.
can give you what is my solution vector a.
And once you have the solution vector a you get your separating plane and this is the separating plane which maximizes the margin or in other words this separating plane will give you a robust linear classifier.
So, today what we have done is we have tried to find out a linear boundary between the feature vectors taken from two different classes omega 1 and omega 2.
And, using support vector machine we have tried to find out one suchlinear separator or plane between the two separating planes in such a manner that this separator maximizes the margin between the vectors belonging to class omega 1 and the vectors belonging to class omega 2.
So, so far whatever we have discussed whether it is a linear discriminator or a support vector machine.
problem, we have considered a problem which is only two class problem.
So, next we will generalize this and try to find out that how we can obtain or how we can extend similar concepts to multi class problems.
With this I stop here today.
Thank you.
Hello, welcome to the NPTEL online course on Deep Learning.
You remember in the previous class we have talked about the linear classifier and the support vector machine.
So, what we have done in case of a linear classifier is that we assumed that we have a number of training samples.
So, training samples Y i which were assumed to belong to class omega i.
In the sense that I had a number of training samples from two different classes the class omega 1 and class omega 2 and using this training samples we wanted to find out a separating plane which separates the samples belonging to class omega 1 and class omega 2.
Now, before that if you remember when we talked about the discriminant function.
For every class we defined a discriminant function which was given by g i x and in case that the covariance matrix of the training samples coming from all different classes are same this g i x came out to be a linear one which is of the form W transpose X plus W naught .
Of course, if the covariance matrices of the training samples coming from the different classes they are not same, then we have seen that this g i x or the discriminant function of the different classes that does not become linear anymore, but it becomes a quadratic function.
So, in case of linear classifier what we tried to find out is we tried to find out the boundary between two different classes.
class omega j in that case g i x minus g j x becomes greater than 0 and our conclusion is that x belongs to class omega i.
Whereas, if g j x is less than g i x then obviously, x belongs to class omega j and the boundary between the two classes is given by g x is equal to 0 when g i x and g j x are both of them are same.
So, in such case if this g i x or g j x they are linear as given in this case then g x also becomes linear which is the separating boundary between .
2 different classes.
So, given that if you are given a set of samples say belonging to class omega 1 and a set of samples belonging to class omega 2, then the boundary between these 2 classes omega 1 and omega 2 becomes a linear one.
So, when we talked about the linear classifier we assumed that the boundary is linear that means, the training samples or the samples belonging to class omega 1 and omega 2 they are linearly separable.
And, while designing linear classifier we did not really think of what is the distribution of the vectors belonging to class omega 1 and class omega 2.
We simply assumed that they are linearly separable.
And, we have seen while designing linear classifier is that as over here you find that this straight line is not unique, I can have line over here, I can have a line here, I can have a line here and so on.
So, there are multiple number of solutions possible.
And, out of this multiple number of separating planes some of the separating planes gives you lessermerging, some of the separating planes gives you higher margin.
So, there we had gone for support vector machines which ensures that the margin that you get is maximum.
That means, your separating plane given say a set of samples to omega 1 and another set of samples to omega 2, it gives you .
a maximum margin from the samples belonging to class omega 1 and the samples belonging to class omega 2 and this is what was the aim of support vector machine or SVM.
So, for all these cases what we have seen is that you are given a set of training samples for designing the linear classifier or for designing the support vector machine and those training samples are actually labeled.
That means, for each sample I know that from which class or from which category that sample has been taken.
Or in other words every sample or feature vector x i comes with a level y i.
So, your samples are given in the form x i y i.
And in case of a linear machine or support vector in case of a linear classifier or a support vector machine that we have discussed so far because we are concentrating on only two class problem.
So, this y i which actually tells you from which category this sample x i has been taken this y i can have one of the two values.
So, we assumed that y i was is plus 1 if x i is taken from class omega 1 and y i is minus 1 if x i is taken from class omega 2.
And accordingly we could have a single classification rule or unified classification rule which we said that W transpose .
x plus x w naught of this form.
So, this vector x is x i this whole thing multiplied by y i that should be greater than 0 if x i is correctly classified by the weight vector w and the margin w naught.
This is obvious because if it is if the sample x i is taken from class omega 1.
Then, my condition is W transpose X i plus W naught should be greater than 0 for correct classification.
If it is taken from omega 2 as the sample as omega 2 is on the negative side of the separating plane.
So, I have to have W transpose X i plus W naught less than 0 and for samples taken from class omega 2 because Y i is minus 1.
So, if I multiply this expression W transpose X i plus W naught by Y i which is minus 1 in this case.
that will be greater than 0.
So, I get a uniform classification rule.
So, while designing the support vector machine or while designing the linear classifier we have taken advantage of this.
So, for any x i when y i w transpose x i plus w naught was less than 0 wehad taken that that particular w and w naught could not classify x i properly.
So, the vectors have to be modified and that is how we have designed the linear classifier and also the support vector machine.
So, in this case also we assume that we are given a set of feature vectors for training purpose.
So, in a multiclass problem now w i will not take only two values now because now we have multiple number of classes.
So, what we assume is that suppose I have got n number of feature vectors.
And, I have say k number of categories or k number of classes.
So, a training vector which is given in the form x i y i as I have got n number of feature vectors given for training.
So, this i will vary from 1 to n as there are n number of feature vectors.
And my categories there are k number of categories and this y i is an index to the category.
So, this y i will vary from 1 to k .
So, y i is an index to the category.
So, for example, if I have 5 categories corresponding say corresponding to say apple, bird,then cat, dog, car and so on.
If apple is the first category for apple y i will be is equal to 1.
If cat is the third category, then for all the vectors.
belonging to cat class will have y i value is equal to 3 and so on.
So, for example, there is a public database called c for 10.
So, this c for 10 database it contains 50,000 images.
So, n is equal to 50,000 and there are 10 categories.
So, k is equal to 10 and the categories of images are images belonging to car, images belonging to cat, images belonging to dog, images of sheep, images ofhorse and so on.
So, there are 10 such categories and there are 50,000 images and each of this image is of size 32 by 32 pixels and these are color images.
That means, there are 3 planes red green and blue.
So, the number of pixels is 32 by 32 by 3.
So, for discussion we will assume that every vector or feature vector given forclassification purpose is of dimension D. So, our X i the feature vectors will be D dimensional and the number of categories will beand there are k number of categories.
So, I have d dimensional feature vectors and I have k number of categories.
So, using this now you find that if I go for the same discriminant function we had the discriminant function of the form g i x for category omega i or i th category and this g i x.
As, we are assuming that our discriminant functions are linear discriminant functions.
So, this g i x is nothing, but of the form w i transpose x plus w i naught where this x is a d dimensional vector w i is also a d dimensional vector.
And, you find that if I expand this expression then my g i x becomes of the form w i 1.
1 plus w i 2 x 2 plus as there are d number of components.
So, I will have w i d x d plus I can write this as w i 0 1 which is the bias term.
So, this is what is my g 1 x for category omega 1.
Similarly, So, for category 2 G 2 x will be W sorry I am taking i is equal to 1.
So, this expression will be different let me put it like this.
So, I want to find out G 1 x which is nothing but W 1 transpose X plus W 1 naught.
As W 1 has got d number of components X also has got d number of components.
So, I can expand this expression in the form W 1 1 X 1 plus W 1 2 X 2 plus W 1 d X d plus W 1 0 .
This is what is my G 1 X.
Similarly, G 2 X becomes W 2 1 x 1 plus W 2 2 x 2 plus continue like this W 2 d x d plus W 2 naught which is the bias forcategory 2.
And as I have got k number of classes I will have k number of discriminant functions.
So, g k x will be of the form W k 1 x 1 .
plus W k 2 x 2 W k d x d plus W d W k 0 .
So, this is your discriminant function g 1 x, this is the discriminant function g 2 x .
this is the discriminant function g k x .
Now, given this k number of linear equations, you find that I can represent this k number of linear equations in the form of matrix equation.
So, the matrix equation simply becomes W x plus W naught where this W is a matrix having k number of rows.
and d number of columns.
So, this W is a K by d matrix and W naught is a bias which is a column vector having d number of components.
So, a linear machine is a one which is specified by this matrix W and this bias vector which is W naught.
So, they are the parameters of a linear machine.
So, once I have this.
for given any unknown vector x i or if I take the training vector x i which belongs to omega isorry which belongs toomega i.
So, corresponding that my index is y i.
So, for this x i y i pair if I compute this expression w x i plus w 0 this will give me a column vector let me call that column vector to be S. So, what this column vector gives?
You find that here what you are computing is you are multiplying X i with every row of W. So, if I take the jth row of W to be W j .
So, this X i is you take the dot product of X i with the jth vector in this matrix W and multiply and add to that the jth component of your bias vector W naught to give the jth component of S or S j .
So, we call this S to be a score function .
So, what this linear machine is doing?
The linear machine is nothing, but a function f which operates on an input vector i, the parameters of the linear machine as a W and W naught and this gives you a score function which is S again a d dimensional vector.
So, this operation I can say that it is a mapping which mappings map my d dimensional feature vector x d into a score which is of dimension k. And every component of this score vector of this score gives you the score for the corresponding class.
So, given my input vector x i the score component S j tells me that what is the score of this input vector x i.
to a category omega j as given by the linear machine decided by which are having the parameters w and w naught.
So, this is what the linear machine does.
And now if my training vector says that this x i is taken from class omega i or the corresponding class index is y i, then obviously, what I would like to see is that for the score function S, the y i th component of this score function should be maximum.
So, that the classification or the categorization as given by this linear machine is correct.
And if it is not correct, then I have to go for correction.
ofthe parameters or updation of the parameters of the linear machine, so that the output classification becomesor the error in the categorization or the classification is minimized.
So, what I want to do is I want to now define something called a loss function .
So, one thing we have seen is the score function .
The linear machine gives you the score function and the score function indicates that for a given feature vector what is the core for what is the score for different classes or for different categories.
And I want that this score should be similar to the ground truth, it should be same as the ground truth.
That means, if my feature vector belong to class omega 1 or it is from category 1 in that case in this core vector that I get the first component should be maximum and all other components should be less than the first component.
So, that my classification is correct.
If it is not then I have to go for updation of the parameters of the linear machine and this parameters are nothing but w 1 w naught and by updation I should be able to tune the parameters so that I get a correct classification ok.
So, what I want is ok before I define this loss function.
I want to show this with an example say for example, over here.
Here what we are doing is we have taken an image and this image is the is flattened out to convert this to a vector.
So, how do you flatten it out?
You can take every column of the image and concatenate the columns to get a vector right.
So, if I assume that this image is converted to a vector having four components.
And, this is my linear machine the linear machine takes care of four different categories the categories are cat, bird, dog and car.
So, in this case cat is the first category, bird is the second category, dog is the third category and car is the fourth category.
So, for cat y i will be equal to 1, for bird y i will be taken as 2, for dog it will be taken as 3 and for car it will be taken as 4.
which are the indices to all these different categories.
So, you find that when you multiply this vector with thismatrix w and this is my bias vector w naught, I get this output which is the score.
And over here because this isan image of a bird you find that the bird score is maximum.
If these parameters are not properly tuned, then it it is possible that the score for car will be might be more than the score of bird in which case my classification that I get the linear machine the classification it gives that is an erroneous classification and I should be able to modify the parameters or update the parameters.
So, in order to do this what I go for is I define a loss functionok, I will come to this a bit later.
So, what is the loss function?
So, I will As I said that if my classification is not correct or even if my classification is correct, but I want to have better confidence over the classification or in other words I want that if the vector x i actually belongs to category omega i or the index is y i, then in the in the score function S y i should be greater than S j for all j not equal to y i this is what I want and just that S i is more than S j I may not be satisfied with this because I want to have more confidence over this classification.
So, for that I may like to have S y i minus S j should be greater than some delta.
So, that is your confidence factor.
That means, it is not only that S i S y i should be more than S j, but S y i should be more than S j at least by a factor of delta.
And only then I will be satisfied and in such case I will assume that my loss function will be equal to 0.
In other cases I will assume that my loss function will not be 0, but I will have a finite loss function and that loss value I want to minimize to get the correct classification.
I will come to this loss function in detailsin our previous class, but before that let me just tell you what isthe interpretation of this linear machine.
So, as we have seen so far that in this linear machine what we are doing is for every class or for every category.
I have a vector which is a row inthe vector w and for this input vector I am taking the dot product of this input vector with the ith row of my matrix w the parameter matrix w and this dot product gives me the score along with the bias term the score for the ith class of the given vector I .
And, as you know that when you take the dot product of two different vectors, the dot product gives you a measure of similarity.
That is if the two vectors are similar, then the dot product will be higher, if the two vectors are dissimilar, then the dot product will be lower.
So, going by that I can consider this linear machine the operation given by a linear machine is something like a template matching operation.
Or in other words, if row in the parameter vector in the parameter matrix W is a template of that corresponding class.
And you are trying to find out you are trying to match your input vector with that template.
And in fact, in CIFAR 10database when you train the linear classifiersyou can see that these are the different classes different categories whichthe CIFAR 10 database contains you have the category of plane, car, bird, cat, deer, dog, frog, horse, sheep and truck.
And, once the linear machine is properly trained it isproperly trained then all the vectors of your weight matrix W if you fold it back to form an image then you will find that those vectors represent the templates of these different categories.
Say for example, here this is the a plane, this is the template of a car, this is the template of horse, this is the template of truck and so on.
So, these weight vectors that I get for different classes are nothing but these templates.
And what the linear machine is doing is it is matching your input vector with these templates.
So, I will stop here today in your in our next lecture we will talk about the loss functions details of the loss functions.
and how these loss functions can be used for designing of the linear machine.
Thank you.
Hello, welcome to the NPTEL online certification course on deep learning.
.
We are discussing about the multilayer perceptron or feed forward neural network.
So, in your previous class we have given an introduction to the multilayer perceptron or feed forward neural network and we have also started our discussion on training a neural network or the algorithm which is known as back propagation learning.
So, in the previous class we have considered a back propagation learning algorithm or back propagation training procedure for a single layer neural network having only one neuron that is a function which is having only one output.
And while discussing that we also have assumed that the neuron does not impose any nonlinearity.
That means, given the training vector x and w being the weight vector of the neural network the output simply becomes W transpose X and because the neural network had only one output.
So, the output could be either positive or negative.
So, if it is positive thenthe sample input vector is classified to one class, if it is negative it is classified to another class.
And we consider the case that the training vectors are given as pair ordered pair given in the form x i y i where x i is the training vector and y i indicates the index to the class to which the x i belongs and because it was a two class problem.
So, we assumed that y i could assume either a value 0 or a value 1.
Whereas, W transpose X when I compute W being the weight vector and X being the feature vector or the input vector, it is not necessary that W transpose X will always be either 1 on 0 or 0.
In fact, it will be a real number, it may be 0, it may be greater than 0, it may be less than 0.
So, our classification rule was that if it is greater than 0, it is belonging to one class, if it is less than 0, it belongs to another class.
But, coming to the neural network the output should be either 1 or 0.
That means, if W transpose X is greater than 0, the output should be 1 indicating that it belongs to class say omega 1.
If the output is less than 0, then it should be truncated to 0 indicating that it belongs to another class.
So, that our class index 0 and 1 matches with whatever you get from the as output of the neuron .
And, in order to do that we also have seen before that when we have an implemented an OR function or AND function or XOR function with the help of neurons that the kind of nonlinearity that we have used were simple threshold nonlinearity.
That means, if W transpose X beta is greater than or equal greater than 0, we have put the output to be 1, the moment it is less than 0.
the output was clamped or truncated at 0.
That means, the threshold value was set at W transpose X equal to 0.
So, the moment the output of the neuron becomes more than 0, it is clamped at 1, if it is less than 0 the output is set to 0.
Now, here you remember that when we talk about the training of the neural network or updating the weights of the neural network, our training procedure makes use of the gradient descent procedure.
That means, we have to take the gradient of the error function or the gradient of the loss function and the loss functions of the error functions are computed based on the feature vectors which are misclassified.
If the feature vectors which are correctly classified for them I do not have to take any action or I do not have to correct or update the weight vectors, but the feature vectors which are misclassified for them with them I have to define a loss function.
or I have to define an error function and the weights are updated in such a way that the loss or the error is minimized.
And for that in gradient descent procedure what you do is you take the gradient of the loss function or you take the gradient of the weight function with respect to the weight vector W. And for that if I want to take the gradient it is necessary that your loss function .
or the error function should be differentiable because the gradient is nothing, but a differential operator.
Whereas, if I put the output nonlinearity which is also known as the activation function of the neurons.
So, if the activation function of the neurons is a threshold function a threshold function is not differentiable because I have an abrupt change at W transpose X equal to 0.
Hence the threshold function is not differentiable though it is a very very simple nonlinearity.
So, instead of using the threshold function a sort of nonlinearity which is used is what is thesigmoidal function.
We have also talked about the sigmoidal function when we have discussed about the nonlinearity.
So, the sigmoidal function is simply given as say I can represent sigmoidal of some function s some argument s is equal to 1 over 1 plus e to the power minus s .
So, here you find that as s is equal to 0 at s is in our case this s is nothing, but W transpose X which is the weighted sum of the feature vector components weighted by the corresponding weight component ok.
So, s is this .
So, at s equal to 0 sigma s is equal to 0.5 which is over here .
And, as s goes on increasing say as s tends to infinity sigma x sigma s tends to plus 1 and as s tends to minus infinity sigma s tends to be 0.
So, that is what is given by this particular curve and at s equal to 0 sigma s is equal to 0.5.
So, you find that as s increases on the positive side, the sigmoidal function asymptotically reaches value equal to 1 and as s decreases on the negative side, the sigmoidal function asymptotically reaches value equal to 0.
The other advantage is this is a differentiable function and also for when s is sufficiently high.
to be positive, I can consider the output to be equal to 1.
And if it is sufficiently low that is on the negative side, I can consider the output of the neural network or output of the sigmoidal function to be 0 indicating whether the class is omega 1 or class is omega 2.
The other advantage that you find that if I take the derivative of sigma.
So, I my sigma s was 1 over 1 plus e to the power minus s. So, if I take the derivative of sigma s with respect to sig s this simply becomes sigma s into 1 minus sigma s which is also a very very simple form.
So, you can take the derivative of this with respect to s and you can verify that actually this is the derivative that you get.
So, these are the other advantages or many of the advantages of using sigmoidal function as a non-linearity or as an activation function of the neural network right.
So, given this so, again I consider a single output, but now this neuron is with a non-linear activation function and the non-linearity I consider is a sigmoidal function.
So, given this now how can such a single layer neural network can be trained.
So, as before I assume that my training samples are given as.
ordered pairs x i y i, x i being the feature vector and y i is the class level of that feature vector.
Here when I feed in this input vector x i to this neural network, I get an output y i hat in the previous case without nonlinearity this y i hat was simply W transpose x i.
But now I impose a nonlinearity by the sigmoidal function.
So, y i hat is now sigma of W transpose X i right.
So, this is my y i hat whereas, the class level for this X i is given as y i.
So, as a result I have an error which is given by y i hat minus y i.
So, this is the error if my output y i hat does not agree with the class level y i that is given.
So, using this error I can define again a loss function or an error function which is nothing but e is equal to half of y i hat minus y i square.
So, you find that now the procedure that I am following is a stochastic optimization procedure.
If I take the sum of this error over all the samples i equal to 1 to n the kind of optimization procedure that I will be using is a batch optimization procedure.
So, if I take single vectors it is a stochastic optimization procedure.
So, let us now continue with the stochastic optimization.
So, my error is given by half of y i hat minus y i square which is nothing but half of sigmoidal function of W transpose X i minus y i square.
So, now if I take the gradient of this error function or this loss function the gradient with respect to W.
you find that the gradient will be given by this.
You find that in the earlier case when our y i hat in absence of non-linearity absence of non-linearity was y i hat minus y i square half of this, okay.
Then the gradient of sorry this is not sorry y i hat this is when my error was e is equal to half of y i hat minus y i square, then gradient of e with respect to w was simply y i hat minus y i times x i this was without non-linearity.
Now, as we have imposed non-linearity which is a sigmoidal function.
And, we have said just before that for sigmoidal function if you take the gradient if you take the derivative with respect to argument it becomes sigmoidal s into 1 minus sigmoidal s. So, here just before because of that if I take the gradient of E gradient of the error function the gradient of error function becomes y i hat into 1 minus y i hat where this y i hat is nothing, but my sigmoidal which is sigma times.
W transpose X i and the other one term simply comes from here.
This is basically gradient of E with respect to W without the sigmoidal function or without the non-linearity.
So, this is the gradient of E or gradient of the error function that we get when the sigmoidal non-linearity is imposed.
And once you do that now my weight updation rule following the gradient descent procedure .
simply becomes W gets W minus eta again the rate of convergence, convergence factor into y i hat into 1 minus y i hat into y i hat minus y i times x i.
This is myweight updation rule in case I have a single output layer node or a single layer neuron with only one node.
and by assuming non-linearity of the neurons, ok. And here again you can compare that this is a rule which is similar to same perceptron algorithm that we considered earlier, hence the perceptron network.
So, the network that we are talking about right now is what is known as a single layer perceptron, because I have only one layer in this neural network and that too I have only one node in the neural network to implement.
are two class problem.
If the number of classes are more than 2, then I have to go for more than 2 neurons, but again that number of layers will be 1.
So, let us see such a neural network now.
So, now I will consider a neural network again a single layer neural network, but the number of neurons in the output layer is more than 1.
That means, we are considering multiple classes multi class problem.
where the number of classes is more than 2.
So, here as we said before that I will have 2 layers, one of the layer is an input layer and this input layer does nothing other than simply passing the input to the output.
So, if I take this i th neuron, i th neuron gets x i which is i th component of the feature vector x and simply passes this to the output.
Ok.
So, the output of this neuron is also X i that is what this input neurons does, it simply passes the input to the output nothing else it is just for an interface.
The actual classification is done by this output layer neuron.
And when I discuss this I also assume that these output layer neurons have non-linearity as activation function and as we have done before just in the previous problem.
The non-linearity we consider in this case is sigmoidal non-linearity or a sigmoidal function right.
So, if I consider an ith neuron, the output of the ith neuron in the input layer is x i which is ith component of my feature vector x ok. And this x i is fed to the inputs of all the neurons in the output layer.
So, in the output layer now if I consider a jth neuron.
So, this i th neuron in the input layer is connected to the j th neuron to the output layer wide a connection weight which is W ij.
You find that when we introducedthe feed forward neural network in its totality we had put an index a superscript which was k to indicating the layers right.
So, superscript k indicates that this is a connection from the i th layer from the ith node in k minus first layer to the jth node in kth layer.
Now, since we are talking about a single layer neuron neural network.
So, I will not use that superscript k because it is simply connection from input layer to the output layer that is known.
So, this superscript k I will not use for this purpose.
So, let us remove this superscript k. So, I have this situation that x i which is the output of the ith node in the input layer is connected to the jth node in the output layer.
through a connection weight x ij.
So, every node in the input layer is connected is feeding input to every node in the output layer of the jth layer.
As a result the weighted sum of all the inputs collected by the jth node in the output layer is given by theta j is equal to w ij times x i .
where i varies from 1 to d assuming d to be the dimensionality of the feature vectors the input feature vector has got d number of components.
So, this is the sum of or weighted sum of the inputs or this is nothing, but if I represent all the weights from the input layer to the jth layer by say W j the weights from all the nodes from the input layer to the jth layer to the jth node in the output layer if those connection weights are represented by a vector W j, this expression theta j is nothing but W j transpose x.
So, I can also put in this vector form or this is a scalar form, but both of them are same.
So, that is the weighted sum of the inputs.
And then we said that every neuron has a sigmoidal activation function.
So, the output O j that I am getting from the jth neuron is given by a sigmoidal function of theta j which is nothing but 1 upon 1 plus e to the power minus theta j .
right.
Again you remember that our input vectors are given by x i y i where y i is the class node.
So, if this input x is said that it belongs to class j that means, we we know that what is the output of the jth neuron which should be the output of the jth neuron when this x is fed to the input of the neural network.
And, that is what we are representing by T j which is the expected output or the true class of the input vector x which is fed to the neutral network.
But, O j is the actual value that you are getting from the j th node of the neutral network when you are feeding the same input vector x.
So, as a result you have an error which is O j minus T j.
So, as before we define the sum of squared error which is nothing but O j minus T j square because now output is a vector right.
For every input x I will have if there are say m number of classes I have j number of m number of nodes in the output layer every output layer node will give me some value.
So, if this input vector x belongs to jth class only output of the jth node in the output layer should be equal to 1.
all other outputs should be 0.
So, that is what is my target vector and using this what is my target vector and what is the actual vector that I get we define what is this sum of square data .
So, again for training this neural network or for learning I use the back propagation learning algorithm for that what you have to find out is the gradient for using gradient descent approach.
So, if I take the gradient of E which is the loss function or the error function with respect to W ij which is the weight component com component connecting the ith node from the input layer to the jth node in the output layer.
So, this gradient del E del W ij I can compute this using chain rule it is del E del oj where oj is a function of theta j ok because oj is nothing, but .
sigma theta j.
So, I compute del o j upon del theta j into del theta j again del theta j is a function of w i j.
So, del theta j upon del y jdel w i j.
And through this chain rule you find that what I get as del e del w i j is nothing, but o j minus t j o j is the output of the j th node the actual output of the j th node.
T j is the target output.
So, del E upon del W i j becomes O j minus T j into O j into 1 minus O j times X i where X i is the ith component of the input vector or X i is the output of the ith neuron in the input layer.
So, this is the gradient that you get.
So, using the weight updation ruleusing the gradient descent procedure.
The weight updation rule for the weight component W ij as before will be simply W ij is equal to W ij minus some constant the convergence rate eta into O j minus T j into O j into 1 minus O j times x i ok, this is the weight updation rule.
So, if you do it for every i and every j you are updating.
every component of the weight vectors which are connecting the outputs of the input layer nodes to the inputs of the output layer nodes .
So, once you do this for all ij, so all the weight components.
Now, you find that we have two sets multiple sets of weights right, because for every output node I have a weight vector So, since there are m number of nodes here instead of a single vector I have a matrix.
So, I can put this matrix as W, where W will have say m number of rows andright d number of columns ok, where every row corresponds to weight vector .
of a particular class right.
So, later on when we talk aboutmultiple multiple multi layer perceptron with multiple nodes at the output very often we will use this matrix convention other than singlevector convention.
So, this is the weight updation rule or the training procedure for a two layer network having multiple nodes in the output layer.
right.
So, given this now we can go for training of thefeed forward neural network having multiple number of hidden layer nodes and also multiple number of output nodes.
So, for that the network that we have considered was having this kind of architecture which also we said before that I have an output layer which I put as kth layer the input layer over here.
So, this is the output layer which is kth layer I am putting it as capital K, this is the input layer and as we said before the function of the input layer is simply pass the input vector to its output.
So, which we are representing as jas 0th layer in between a kth layer represented by lowercase k and as we said before that we will also assume .
that from a node i in K plus first layer to a node j in Kth layer, I have a connection weight which is given by W i j K. So, this is the convention that I will use.
I will also use the convention that in the Kth layer the number of nodes is given by M k. So, M k is the number of nodes in the Kth layer.
So, while doing this in the output layer which is capital K the number of nodes will be given by M capital K which is same as the number of categories or the number of classes we will consider.
Now, unlike in the previous case here I will have two distinct situations.
One is as we said that we can only compute the error at the output because there only at the output I know what is the target.
I cannot compute error at any of these layers.
I cannot compute error here, I cannot compute error here because here I do not know what are the targets.
That is the reason these all these layers are known as hidden layers and this is the layer where the output is visible and the output is known for known classes, this is a output layer or visible layer all rest of the layers are hidden layers.
So, we will see that when we talk about the the back propagation learning of such a multi layer perceptron or a feed forward neural network, then we will have a little difference because now I have a number of hidden layers which I did not have earlier.
So, error that you computed the output that can directly propagated to the connections between the output layer and the hidden layer just before that.
So, here computing the gradient of the error is straightforward.
But, when we try to update the weights of the layers in between hidden layers, then I have to see that how this error actually propagates to this level and that is where we will have a little bit of mathematics.
So, we will talk about this training or backpropagation learning of the feed forward neural network in our next class.
Thank you.
Hello, welcome to the NPTEL online certification course on deep learning .
.
We have started our discussion on the neural network particularly the feed forward neural network and in the previous class we have discussed about or we have started our discussion on learning in a feed forward neural network .
We have discussed about learning in a single layer perceptron.
In today's class we will talk about the back propagation learning in multi layer perceptron .
or multi layer feed forward network.
So, just to recapitulate what we did in our previous lecture is we had taken single layer neural neuronsneural networks with nonlinearity as well as without nonlinearity at the output.
So, when we have considered a single layer neural network withnonlinearity at the output and the nonlinearity that we have considered was a sigmoidal function.
as given in the right hand side in this slide .
So, this is the sigmoidal function that we have considered and for such a kind of network the output of the neuron which is given as y i hat is equal to sigma of W transpose X, where W is the weight vector at the input side of the neuron and X i is the input vector .
So, for training you remember that we have said that we obtain the training vectors as ordered pairs given as x i y i, where x i is the feature vector and y i is the class to which this feature vector belongs.
And when I have a single output in the neural network or only one neuron at the output layer actually we are considering a two class problem.
And, in two class problem this y i can take value of 0 or 1, 0 means it belongs to one class and 1 means it belongs to another class.
So, given this we have seen that the training algorithm or the weight updation algorithm wasobtained as W that is the weight vector is updated as W minus.
some eta times y i hat into 1 minus y i hat into y i hat minus y i times x i, where x i is the input vector and y i hat is the computed output and y i is the desired output, it is the class index of the class to which x i belongs.
So, this is what we have obtained for a single output with nonlinearity where nonlinearity was a sigmoidal function.
Then we had moved on to single layer network with multiple outputs and when we have multiple outputs then obviously, I have to consideran weight weight of the form W ij.
So, what I have considered is from the input layer if I take an ith neuron And, we have considered that this i th neuron in the input layer is connected to the j th neuron of the output layer through a connection weight which is given by W ij .
So, training in this case means that I have to find out the optimal values of W ij for all values of i and j .
So, as you vary the index i that means, I am considering different neurons at the input layer as I vary.
the index j that means, I am considering the connection to all neurons in the output layer.
And as before if I consider the output of the jth neuron which we are putting here as o j, o j is a sigmoidal function of theta j, where theta j is nothing but weighted sum of all the input feature components or in other words this is the dot product.
of W ij for all values of i the vector that I get with the input vector x.
And given this we had defined an error function or a loss function which is half of o j minus t j square where t j is the target output or I can say this t j is nothing but y j .
So, the t j that we have considered it is nothing but y j .
that is the actual class belongingness ofthe feature vector.
So, what we need to do is we need to minimize this error function with respect to W ij or this loss function with respect to W ij.
So, for that again we are using the gradient descent approach.
So, I have to find out what is the gradient of this loss function with respect to the weights.
So, I compute del E del W ij which is equal according to chain rule comes out to be del E del o j into del o j del theta j into del theta j del w i j.
And if you compute this, this comes as o j minus t j into o j into 1 minus o j times x i.
And given this my weight updation rule in this case becomes w i j gets w i j minus theta times o j minus t j into o j into 1 minus o j times x i .
where X i is the ith component of the input vector and W i g is the connection weight from the ith node in the input layer to the jth node in the output layer.
So, this is the updation rule that we get within a single layered perceptron when I have number of outputs or multiple number of outputs.
Now, let us go to a multi layer perceptron.
So, as we justindicated in the previous class that when I consider a multi layer perceptron or multi layer feed forward network, then obviously, I have to have an input layer which receives the input vector and I have to have the output layer which tells you the class or class belongingness of the input vectors.
So, if I have say C number of classes, then I will have C number of neurons at the output layer.
So, accordingly I will have c number of outputs.
And if a feature vector belongs to say class 5, then output of the fifth neuron should be high and the output of all other neurons should be low.
In this case in this particular example we are assuming that there are k number of layers where capital K isthe output layer.
At the input layer we are considering that this is zeroth layer and we said in the previous class that the neurons in the zeroth layer simply passes the input to its output, it does not perform any other function.
Whereas, in all the hidden layers the neuronsperform two tasks every neuron takes the weighted sum of all the inputs that it receives from its previous layer.
And, then computes a non-linear function over it and the non-linearity that we are considering over here is nothing, but a sigmoidal non-linearity.
And, if I consider any layer say kth layer where this k I represent as a lowercase in lowercase.
So, every kth layer passes its output to k plus first layer and it receives inputs from k minus first layer.
So, every node in the K minus first layer passes output to every node in the Kth layer and every node in the Kth layer passes the output to every node in the K plus first layer.
Now coming to the error function or the loss function which we want to optimize while training this neural network,you can easily imagine that we can only compute the error function or the loss function at the output layer.
Because, it is only at the output layer I know what is my target output T j.
The reason being if I say that a feature vector a training vector belongs to jth class I know that T j should be high or ideally T j should be 1 and all other outputs except T j or except the output of the jth neuron in the output layer should be low or ideally they should be 0.
So, this is known .
And, as the expected output or the ground truth at the output layer is known, I can only compute the error function at the output layer.
I cannot compute error function in any of the layers any other layers the reason being I do not know what is the expected output or what is the target output at the outputs of any other layers.
And, that is the reason all other layers except the output layer and the input layer of course, are known as the hidden layers because I do not know what is their outputs.
So, given this now let us see that how we can train this neural network.
So, over here the training unlike in case of a single layered neural network where training was very simple because I had to update the connection weights from the input layer to the output layer.
Now in this case the training for the output layer and training for the hidden layers will be slightly different because I can compute the error at the output layers.
And, once I compute the error at the output layers, I can back propagate that error through the gradient descent approach to the connection weights in between this K minus first layer K capital that means, it is the layer just before the output layer.
So, I can back propagate the error for updation of the weight vectors from K minus first layer to Kth layer.
But, want to update any of the weight vectors in between the hidden layers, you find that I do not have I cannot compute directly what will be the error at the output of any of the hidden layers.
So, following chain rule I have to get the feedback or I have to back propagate the effect of the output error error that you can compute at the output layer to the hidden layers and that we have to use .
for updation of the weight vectors in between the hidden layers following our gradient descent approach.
So, let us see how we can do it .
So, given this you find that updation of the weight vectors at the output layer is almost similar to updation of the weight vectors in a single layer single layer neural network where we had multiple number of outputs .
So, here again I assume that this capital K indicates index of the output layer.
I take a jth neuron in the output layer and the output of the jth neuron is represented by O j k. And from our previous discussion you can recollect that O j k is a sigmoidal function, it is the sigmoidal function of weighted inputs of .
weighted sum of the inputs which are coming to the jth neuron.
So, that weighted sum of the inputs I am representing as W ijk x i k minus 1.
What is this x i k minus 1?
This k minus 1 indicates that this is the output of a neuron from the previous layer that is k minus first layer and this subscript i indicates that it is the output of the ith neuron.
in the k minus first layer.
So, I take this ith neuron in the k minus first layer, this ith neuron is connected to the jth neuron in the kth layer through a connection weight w i j k. So, the weighted sum of all the inputs to the jth neuron in the kth layer is given by w i j k into x i k, I have to take the sum over i is equal to 1 to m k minus 1, where m k minus 1 is the number of neurons in the k plus first layer.
So, this is the weighted sum and once I have this weighted sum then I have to compute the sigmoidal function over it.
So, the sigmoidal function is given by 1 over 1 plus e to the power minus theta j k where this theta j k is the weighted sum and that is the output of the j th node in the output layer which is o j k. So, once I have this output I know because all the vectors that we are feeding to the input of this neural network are the training vectors.
So, I know what is my target output at the jth node, because if the sample belongs to class j ideally T j should be equal to 1.
And this O j k is the output as computed by the neural network at a particular instant of time known as epoch that is the different steps of training.
So, I compute the sum of squared error which is given by O j k minus T j square take the summation.
over all j for j is equal to 1 to m k, m k is nothing but the number of nodes at the output layer.
So, that gives you the sum of squared errors.
We take half of this because as we have seen in our previous class also that we have to take the gradient descent and this is a squared error.
So, when I take the derivative this when I take the derivative 2 comes over here.
So, that 2 and half that gets cancelled.
So, that is the only reason that we are putting a scale factor which is half .
So, for updation of the weight vector W ij what I need to do is I have to take the gradient of the derivative of this error e with respect to weight vector W ij k .
So, as you take the derivative of the error or squared error withW ij k. So, this is what we just said I need to take the derivative del E del W ij k .
And, if I compute this again following the chain rule I get del E del W ijk is equal to del E del ojk.
The reason we are using thischain rule is that E is given as a function of oj that is the output W ijk comes indirectly.
So, we compute this derivative using the chain rule that is del E del ojk.
times del o j k del theta j k. So, if you remember that o j k is nothing, but sigmoidal function of theta j k. So, it is del o j k times del theta j k and theta j k is the weighted input or weighted sum of the inputs at the j th node.
So, that is given by w i j k times theta i k minus 1.
So, the last component in thischain becomes theta j k del w i j k. And if you compute this you find that del E del theta j k we said that one advantage yeah.
So, del E del o j k that simply becomes o j k minus t j because E was half of o j k minus t j square.
So, del E del o j k becomes o j k minus t j then del o j k del theta j k.
O j k is a sigmoidal function of theta j k so as given by this expression .
So, del O j k del theta j k becomes O j k into 1 minus O j k we said that the advantage of using sigmoidal function is the derivative becomes very simple which is of this form and del theta j k upondel theta j k del W i j k simply becomes O i k minus 1 .
1 because if I take the derivative of this with respect to del W i j k it simply simply becomes O i k minus 1.
So, if I put this O j k into O minus 1 minus O j k into O j k minus T j as delta j k that is why I am putting this as delta j k will become clear when we go for updating of the hidden layer weights .
So, if I put this then del E del W i j k simply becomes delta j k into O i k minus 1, where O i k minus 1 is the output of the ith node in the k minus first layer.
And given this the weight updation rule W i j k simply becomes W i j k minus eta times delta j k O i k minus 1, where this eta as we said before.
is nothing, but a constant which controls the rate of convergence orthe learning rate.
So, this is how I can update the weights of the output layer that is the layer between the output layer and the layer just before the output layer.
Now, let us say how we can update the weights of the hidden layer.
As we said that, I can only compute error at the output layers this is where I can compute the error.
But when I am updating the weights at the hidden layer, I cannot compute what is the error at the output of any of the hidden layers, because I do not know what is the target output over here.
So, whatever error I compute here that through back propagation has to be brought to this particular layer and in this layer I have to see what is the effect of this error and using that.
I have to go for weight updation again following the gradient descent approach.
So, let us see how we can do that.
So, for doing this I assume that initially we have updated the weights between the output layer and the layer just before the output layer.
So, which was my K minus first layer.
Now, K minus second layer is one of the hidden layers.
So, let us see that how we can update the weights between k minus first layer and k minus second layer.
Then we can generalize that result that we get the weight updation rule to any of the hidden layers.
So, in order to do this what I do is I take a neuron say p-th neuron in k minus second layer, I take i-th neuron in k minus first layer you follow you see over here that I am just following a chain because in the previous case I am I have assumed that.
from ith neuron in the k minus first layer, I have a connection from the jth neuron in the kth layer and that connection weight was W ijk.
Now, I am going one more layer before k minus first layer.
So, in k minus second layer I have this pth neuron and this pth neuron in the k minus second layer is connected to ith neuron in the k minus first layer and ith neuron in the k minus first layer is connected to jth neuron in the kth layer.
And, I am also assuming that this pth neuron in k minus second layer is connected to ith neuron in the k minus first layer through a connection weight which is given by W pi k minus 1.
But, you remember as we just said that I can compute the error only at the output layer, I cannot compute the error in any of the hidden layers.
So, my error function or the loss function is still given by E is equal to half.
o j k minus t j square j varying from 1 to m k that is output that I am computing the error that I am computing at the output of the output layer.
So, this is still my error function .
So, now in order to update this connection weight W p i k minus 1 what I have to do is I have to take the derivative of this error function E this loss function E .
with respect to W pi k minus 1, it is no more with respect to W ij k. So, again I have to follow the chain rule in order to find out that how this error e which is computed at the output layer varies with the variation of W pi k minus 1.
So, let us apply this chain rule again over here .
I will just skip this slide .
So, this is what I have to do and the derivative that I just said that I have to compute del E del P i k minus 1.
So, this is the derivative that I have to take and following the chain rule you find that I can write this as del E del O i k minus 1 into del O i k minus 1.
1 del W P i k minus 1.
And again del O i k minus 1 W P i k minus 1 using the chain rule can be written as del O i k minus 1 del theta i k minus 1 del theta k i k minus 1 del P i k minus 1.
So, these two are very simple because .
I know that O i k minus 1 is just the sigmoidal function of theta i k minus theta i k minus 1.
And I also know that theta i k minus 1 is nothing but W pi k minus 1 O p k minus 2 and you take the summation from p is equal to 1 to m k minus 2.
You remember that we thiswe are considering k minus second layer.
So, number of nodes in the k minus second layer is m k minus 2.
So, as these two functions are known I can easily compute what is del o i k minus 1 del theta i k minus 1 which is nothing, but this the derivative of the sigmoidal function and del theta i k minus 1 del p i k minus 1 which is simply o p k minus 2 that is this.
So, what I am left with is del e del o i k minus 1.
So, how I can compute this del e del o del o i k minus 1 .
So, over here you find that E is given as as we have already said the error at the output layer and where o j k is the sigmoidal function of theta j k and theta j k is nothing, but weighted sum of o i k minus 1 .
So, given this you find that del E del o i k minus 1 again by chain rule I can write this as del e del o j k into del o j k del theta j k into del theta j k del o i k minus 1 right.
And that simply becomes sum of o j k minus t j into o j k into o j k minus o j k which is the derivative of this .
into del theta j k del o k minus 1 is nothing, but w i j k. So, this is w i j k and del e del o j k is nothing, but o j k minus d j it is o j k minus d j and that has to be summed over all j.
So, it is again varying from j is equal to 1 to m k and this I can write as now you can .
Try to recollect earlier we had written this term O j k minus T j into O j k into 1 minus O j k as delta j k. So, that I put over here.
So, that simply tells me that O del O e del sorry del e del O i k minus 1 simply becomes sum of delta j k W i j k where you take the summation .
over j is equal to 1 to m k that is the total number of nodes of the neurons that you have at the output layer.
So, given this now our weight updation rule of the weights between k minus second layer to k minus first layer simply becomes W k minus 1 W pi k minus 1 that is updated as .
W pi k minus 1 minus eta times del i k minus 1 intosorry this was the weight updation rule at the last, but output layer.
So, that was eta times delta i k minus 1 into O p k minus 2.
So, this was the weight updation rule between the output layer and the layer before output layer.
So, in the hidden layer .
I will have a summation term here I am putting delta i k as summation of this as I said that I can generalize this from output layer to any of the hidden layers.
So, I am taking any of the hidden layers say kth layer.
layer at which I can put this delta i k, k is lower case means it is any of the hidden layers that simply becomes o i k into 1 minus o i k into summation of the contribution error of error term that you are getting from k minus first layer.
So, that gives you delta i k and using this my weight updation rule in the kth layer that simply becomes w i j k getting w i j k minus eta times delta j k .
times O I k minus 1.
So, this O I k minus 1 is the output of the k minus first layer the ith node of the k minus first layer.
And using this I can update the weights of any of the hidden layers.
So, what I have to do is I have to compute this for all values of i j and k. And while you do that you have to start from your output layer and then gradually move to all the hidden layers.
So, this is what is the weight updation rule or the back propagationlearning algorithm in multi layer neural network.
And you find that here the error function that we have considered is the sum of squared error or that is also known as quadratic error.
So, in your in our next class we will try to see that what is the problem that we face with the quadratic errorquadratic error.
and what sort of remedy we can have.
Thank you.
Hello, welcome to the NPTEL online certification course on deep learning.
.
We have started our discussion on the neural network and in our previous class we have talked about the basicimplementation of few of the logic functions and OR and XOR function using the neural networks.
Today we will continue our discussion with the neural network and particularly we will talk about the feed forward neural network also known as multi layer perceptron .
And, then we will see that how these neural networks can be trained to solve certain problems.
So, for that the algorithm that we will talk about is what is known as back propagation learning algorithm.
And, in fact we will talk about back propagation learning in details the reason being that this back propagation learning is the basic of the deep learning or the deep neural networks that we are going to discuss in future.
So, just to recapitulate what we did in the previous class is that we have implemented three of the logic functions AND function OR function and XOR function using the neural network.
And we have seen that in case of an AND function if the input is X 1 and X 2 for unified representation as we have done before that we have appended an additional component which is equal to 1 and this is what helps in giving a bias to the neural network or every nodes in the neural network.
And the weights that we considered was minus 1.5, 1 and 1 and with this we have seen in the previous class that output becomes x 1 and x 2 and that is what our AND function.
In case of an OR function, Of course, there was a non-linearity involved in it, the non-linearity that we have considered was a threshold non-linearity.
That means, if the output or weighted sum of the input that is W transpose X, this was greater than or equal to 0, we have assumed the output to be 1 and if it is less than 0, we have assumed the output to be 0.
And with that we have seen that this network, this simple node implements an AND function.
Similarly, in case of OR function again our input is the binary input x 1 and x 2.
The bias term in this case is minus 0.5, here we have giventhe weights as 1 and 1 and with this again we have seen that the output becomes x 1 or x 2 .
function.
We could implement this AND function and OR function using a single neural network because we have seen before that these functions are actually linear functions.
I can separate the outputs which are 1s from the outputs which are 0s by a straight line which is not possible in case of an XOR gate.
So, XOR is a non-linear function and we have seen in the previous class.
that, because XOR is a non-linear function.
So, it cannot be implemented a single neural network or single neuron.
So, I need multi layer neural network and that is what is in this figure.
So, here again our inputs are 1, X 1 and X 2 those are the binary inputs.
And one of these two gates we have seen before that it implements an OR gate and the other one implements a NAND function.
So, let us assume that the first one implements an OR function and this one implements a NAND function.
So, for implementing OR function just as we have seen over here our weights will be minus 0.5, 1 and 1 and for implementing a NAND function it has to be just complement of AND.
So, weights for implementing NAND function will be plus 1.5 minus 1 and minus 1.
And these two outputs are finally, to be ANDed because our XOR function X 1 XOR X 2 is nothing, but X 1 or X 2 ANDed with X 1 NAND X 2 .
So, here I have to have .
a AND function.
So, for this weights will be minus 1.5, 1 and 1.
So, here at the output what I get is x 1, x or x 2.
So, you find that these are simple implementations of the logic functions usingneural networks.
Now, from particularly this XOR function, it is quite obvious that if the function that we have to implement or the problem is a non-linear problem, the non-linear problem cannot be solved using a single layer network.
I need multiple or multi-layer neural network for implementing a non-linear problem.
So, for a non-linear problem as we also have seen in the previous class that I have layers of neural networks.
So, I have neural networks in multiple layers and from every layer say layer 1 to layer 2, I havecomplete connection that is every node in the layer 1 is connected to every node in the layer 2 and if the final function.
So, here you find that there there are k number of layers.
So, at the output I call the output layer to be kth layer.
So, the final function that is being implemented is.
f k ok and the input layer implements a function f 1.
So, with my input vector at as x the first layer that is f 1 layer it implements function f 1 of x, the second layer implements f 2 of f 1 x, third layer will implement f 3 of f 2 of f 1 x and finally, the k th layer implements f k of the output.
that has been generated by all the previous layers.
So, when it is a non-linear problem we can say that all the layers from f 1 to f k minus 1 they will implement a non-linear mapping because as we said earlier that if I have a non-linear problem then instead of trying to design a non-linear classifier you try to map the input vectors using a non-linear mapping function.
So, that they are mapped into intermediate feature space and in the intermediate feature space this non-linearly mapped input vectors will be linearly separable.
And then finally, at the final layer or at the kth layer I can have a linear classifier to classify all those samples correctly.
So, all these layers from f 1 to f k they actually implement this non-linear mapping.
So, at the output of f k the new feature vectors that I get so, feature vectors which are now linearly separable and the kth layer I can implement a linear classifier which will classify all these vectors h which are now linearly separable .
So, this is just a block diagram representation of a multi layer perceptron or a feed forward neural network .
Now, why it is feed forward?
Because I am inputting the feature vector x at the input layer .
They are being processed at every layer and being forwarded in the forward direction to the next layer and finally, you get the output from the final layer at the output layer.
And nowhere in this path the information is fed back to the previous layer.
So, always the information flows in the forward direction.
So, it is feed forward network, but we will see later that for learning or for training this neural network the error is propagated in the backward direction.
Because, our aim is to minimize the error by adjusting the weights in between the layers.
So, for training this neural network the error is propagated in the backward direction through each of the layers and while it is being propagated at every layer the weights are updated in order to minimize the error.
So, that is why the learning algorithm is known as back propagation learning.
So, our neural network is a feed forward neural network.
algorithm, but the learning algorithm is a back propagation learning algorithm.
Now, let us see in details that this how this neural network or multi layer perceptron that looks like.
So, this is what is a somewhat detailed representation of the speed forward neural network.
So, here I have assumed that there are k number of layers in the network.
At the beginning we have an input layer which is represented as zeroth layer.
The purpose of this layer is simply whatever comes at the input is simply passes to the output.
So, this layer does not have any other function other than simply passing the input to the output and every other layer from 1 to say layer k minus 1 each of these layers participate in non-linearly mapping the input feature vector x to a new feature space H. And kth layer which is represented by capital K it is the final output layer right.
And we also assume that from every layer every intermediate layer K where this K is represented by lower case letter, the nodes are connected or the neurons are connected to the next layer which is K plus 1.
And this connection is .
complete in the sense that every node in the kth layer is connected to every node in the k plus first layer.
And if I take an ith node in the kth layer, it is connected to jth node in the k plus first layer through.
So, let me put it like this.
So, I take a node i or a neuron i in the kth layer and I take a neuron g in the k plus first layer which is the next layer.
So, the output from the node i is connected to the input of node j output from node i in kth layer is connected to the input of k plus first layer to a connection weight which is W i j k plus 1.
So, this is the convention that we will use when we discuss about the back propagation learning.
So, this is the overall .
of our feed forward neural network.
There are k number of layers, kth layer is the final output layer.
Every node or every neuron i in an intermediate layer k represented by lower cost k is connected to the nextconnected to the jth node in the k plus first layer through an weight which is given by .
W ij k plus 1.
And the purpose of training this neural network or learning is that iteratively using the training vectors you try to find out what should be the value of W ij k plus 1 for every i j and k so that the neural network is finally, trained to solve your problem.
So, that is the purpose of back propagation neural network.
network.
And in this network you find that as we are feeding the input X i.
So, for training the training data is fed in the form of X i Y i as a doublet for say i is equal to 1 to n, where n is the number of training samples I have which is given for training this neural network.
So, for every training sample X i, as it is labeled because these are used for the training purpose, I know that to which class this sample X i belongs.
So, X i in this case belongs to class Y i.
So, if I have a binary classification problem we will do thatquickly, this Y i can be either 0 or 1.
If Y i is 0 that means, the sample X i belongs to say class Y i.
1 omega 1 and if y i is 1 that means, the same the sample x i belongs to class omega 2.
So, this tells you that what is the class belongingness of the training sample that I have.
If I have c number of classes, if I have c number of classes then y i will take one of the values 1, 2, c. So, this y i in that case is the index to the class to which x i belongs.
So, if I have a data X i 5, this is the training data given then this means that the training data X i belongs to class 5.
Secondly at the output node if I take a jth node jth neuron at the output layer, I represent the output of the jth node.
in the kth layer as x j k. So, these are the conventions that we will use when we talk about back propagation learning.
And here you find that only at the output given a training vector x i only I know that what should be the corresponding output.
Because if it is x i 2 this is the training pair that is given I know that when this x i is fed to the input vector.
output of the second node that should be high, the outputs of all other nodes should be equal to 0 because my training pair says that this vector x i belongs to class 2.
Similarly, if a training vector x is given which belongs to class 9, then when I fit this x to the input only the output of the ninth node from the output layer should be high and all other output should be low.
So, this I can decide only at the output layer.
I really do not know that what should be outputs of any of the hidden layers that is not visible.
So, that is the reason that all the nodes or all the layers except the output layer they are known as hidden layers because I can only observe I can only decide at the output I cannot decide what should be the outputs of any node in the intermediate layers or hidden layers.
And this is a layer as we said that this is known as input layer.
So, the purpose of every neuron in the input layer is simply to pass whatever is coming to the input to its output and it is subsequently fed to the neurons of the next layer.
So, this is what the architecture of the neural network looks like and the conventions that will follow.
Now, let us see that how can we train the neural network or what is back propagation learning.
So, to talk about back propagation learning.
First, I will consider a very simpleneural network consisting of a single layer which is the output layer.
Of course, you remember that we said before that using a single layer I can solve only linear problems, I cannot solve any non-linear problem.
And as I said that this learning algorithms will do in details because this forms the basis of all subsequent deep learning or deep networks, deep neural networks that we will talk about.
So, it is very important that you understand the back propagation learning very very clearly right.
So, again I take a single network a single neuron where the weight vector is w if I feed an input vector x i.
So, as we said before that for training I get the input vectors as pairs x i y i where this y i is that the output of the neural network should be y i and only when I get the output as y i which matches with my true values, then at least this x i is correctly classified by this neuron.
But what we get is I get a value x i hat which is an approximation of x i.
So, if x i hat, so this x i hat is nothing but W transpose x i where W is my vector and x i is the input vector.
And for the time being I am assuming that I have a neural network with single layer and single output node and I am not assuming there is any nonlinearity in this neuron, ok.
So, my output simply becomes W transpose x i and I am assuming that this W transpose x i is an approximation to y i which is y i hat.
So, if y i and y i hat they are same.
that means, my input vector is correctly classified.
So, I do not have to take any action to modify the weight vector w. Now, suppose y i hat and y i they are not same they are different ok.
So, my error will be y hat minus y i this is the error and what I compute is the sum of squared error.
So, this is sum of y i minus x i.
where this i will vary from 0 to n. So, this square error is computed over all the training vectors that I have as i varying from 0 to capital N and I have capital num capital N number of vectors for training purpose.
And I scaled it this by half the reason of scaling it by half is as we have seen earlier also for any learning algorithm or any training algorithm we go for gradient descent approach.
That means, I have to take differentiation of the error function or the loss function that we have generated.
And because it is squared error or squared loss, so having a factor half will simplify our expressions.
So, that is the reason this half is put.
If I put it in an elaborated form, so this error function or the loss function e is nothing, but half of W transpose X i, this W transpose X i is nothing, but our Y hat.
So, W transpose X i minus Y i squared.
take the summation over all i varying from i equal to 1 to n that means, you are summing the errors of all the training vectors that you have.
Next as we said that we will employ the gradient descent approach as we have done before for training the network or for updating the weight vector w. So, I take the gradient of E with respect to weight vector w and here you find that this gradient is nothing, but y hat minus y i into x i where x i is the input training vector right.
And now you find that why we have put this half because otherwise there would have been a scaling function 2 over herescaling factor 2 over here.
So, in order to avoid that you put half.
So, this is the gradient of the error or the loss right.
So, I have to update w.
or the weight vector in such a way that this loss is minimized or the error is minimized.
And for that as before my weight updation rule follows the gradient descent procedure.
So, my weight updation rule will be simply W gets if this is the previous value of the weight vector, the updated weight vector will be W minus some eta times the gradient.
What is this eta?
We also said before that this is nothing but a rate of convergence factor.
So, this eta indicates if the value of eta is very high then the rate of convergence will be fast, if the value of eta is low then the rate of convergence will be low.
So, I will have a slower convergence of course, both has their individual merits and demerits we also discussed about those before.
Now, if you look at this weight updation rule you find that we get something more.
I said my output Y should be either 0 or plus 1.
So, if it is 0 it belongs to one class, if it is plus 1 it belongs to another class.
And if you remember that earlier the linear discriminators that we have talked about that is a linear plane which separates two different classes which are linearly separable.
We have said for one of them if W transpose X greater than 0, then it belongs to one class if it is less than 0 it belongs to one another class.
Now, here let us come to a situation that if I find that W transpose X is greater than 0 for samples belonging to some class omega 1 and this is represented by Y i is equal to plus 1 .
So, as long as my W transpose X i is greater than 0 then it is correctly classified right.
Now, here you find that if my Y i is plus 1, but I get W Y i hat to be negative then and instead of taking the sum let us consider a single feature vector X i.
So, which is nothing, optimization, we have also talked about that before.
If you sum all of them that means, you are considering all the training vectors together it becomes a batch optimization technique.
So, we have talked about batch optimization, mini mini batch optimization and stochastic optimization.
So, if I consider only X i that becomes a stochastic optimization procedure and then our weight updation rule will be simply W gets W minus eta times Y i hat.
hat minus y i times x i.
So, here if my y i is actually plus 1 that means, W transpose x should be greater than 0 for correct classification of x i.
But suppose W transpose x which is nothing but y i hat happens to be negative.
In that case this y i hat minus y i this whole term will be negative.
And, in fact what we are doing is we are making W updating W as W plus some factor say eta times let us put chi times x i .
So, here you find that there is some similarity with the perceptron algorithm that we talked about towards the beginning of our course that if an x i is misclassified.
we add a fraction of x i to the weight vector for weight vector updation right.
In the same manner if I assume that y i is 0 for which this y i hat should be negative because it belongs to the other class.
But if I get y i to be positive y i hat to be positive say over here.
my y i is 0 that means, x i belongs to another class for which y i hat which is computed by this neuron should be negative, but if I get this as positive ok. Again as before the updation that we are doing is W as W minus sum eta chi times x i .
So, this is the other vector for which W transpose x i should have been negative .
But, it has been misclassified because y i hat has been positive has been computed as positive by the neuron by the neural network.
And in this case what we are doing is we are subtracting a fraction of y i from w or in the other words we are adding a fraction of negated y i to w again the same thing that we have done in case of our perceptron algorithm.
So, this is what you have in case of single layer perceptron .
that following gradient descent procedure the way we update the weights or the weight vector is by adding or subtracting a fraction of the misclassified samples to the weight vectors.
So, I will stop this lecture here we will continue with this next.
Thank you.
Hello, welcome to the NPTEL online certification course on deep learning.
In our previous lecture, we started discussions on the topic in neural network and we have talked aboutimplementation of two of the logic functions the AND logic and OR OR logic using the neural network.
And we have seen that AND logic and OR logic both of them being linear functions, they can be implemented.
very easily using a single neuron.
These single neurons are known as or a single layer neuron, they are known as single layer perceptron.
We will see later that why they are single layer perceptron.
Today, we will talk about other implementations of neural network like XOR logic, we will also talk about feed forward neural network or a multi layer perceptron.
And, we will also talk about how the neural networks can be trained using learning mechanism known as back propagation learning.
So, let me just recapitulate what we did in our previous class.
We have implemented two logic functions AND logic and OR logic.
And, we said that both this AND logic and OR logic can be implemented using single neuron or single layered neural network.
So, for implementation of a AND logic, we what we have done is we have taken our input vector to be 1, x 1 and x 2 and the weight vectors were taken as minus 1.5, 1 and 1.
In the neuron I can consider that it has two functional parts, in the first part it computes W transpose X.
So, in this case it will compute W 0 which is nothing but minus 1.5 minus 1.5 times 1.
1 into x 1 plus 1 into x 2.
So, effectively this function which is computed is x 1 plus x 2 minus 1.5.
Now, this computed value is passed on to the second compartment of the neuron which computes the nonlinearity and in this case the nonlinearity is a threshold nonlinearity.
So, the nonlinear function that we have considered is a threshold nonlinearity.
So, for x transpose W transpose x greater than 0.
the output is 1 for w transpose x less than or equal to 0 it is equal to 0.
So, as a result at the output the function that I get is an AND function .
So, this is what we get in case of an AND function when the input vectors are minus 1.5 1 and 1.
Then I want to compute an OR function .
I simply my input vectors remains the same that still is 1 x 1 and x 2 whereas, weight vectors are changed to minus 0.5 1 and 1 as before the first compartment computes W transpose x and the second compartment gives you the nonlinearity which imposes nonline threshold on nonlinearity in W transpose x.
So, as a result at the output.
what we get an OR function OR of x 1 and x 2.
And here with these weight vector this weight vector the classifier that you are doing of the separating plane that you are designing is nothing, but x 1 plus x 2 minus 0.5 that is equal to 0 that is the separating plane between the two classes omega 1 and omega 2.
And we have also said that we could implement AND function and OR function using a simple single layer neuron or a single neuron because the classes in this case are linearly separable.
Now, what happens if the classes are not linearly separable?
So, we have discussed earlier that in non-linearly separable cases or linearly non-separable cases either I can have a non-linear classifier.
that is the boundary which is its which itself is non-linear or the other approach can be that you can map the feature vectors using non-linear functions to an intermediate feature domain where because of this non-linear mapping the in the intermediate feature domain the feature vectors will be linearly separable.
And as in the intermediate feature domain the feature vectors are linearly separable.
So, I can now have linear classifiers which classifies the features in the intermediate feature domain.
So, I have two levels of operation in the first level of operation the feature vectors will be non-linearly mapped to an intermediate feature domain, where they will be linearly separable and in the second step you these feature vectors in the intermediate feature space where they are linearly separable.
In the second step I can design a linear separator or a linear classifier which classifies these vectors in the feature space.
So, I have a non-linear mapping of the feature vectors in the original space and finally, a linear classifier.
So, let us again see a very simple example, what happens that in case of and instead of and function or or function if I want to implement an XOR function.
So, all of you know that in the in case of XOR function when the inputs are 0 0 that is both X 1 and X 2 both of them are 0s the output is 0.
If both of them are ones that is x 1 is equal to 1 and x 2 is equal to 1, then also the output is 0 only when the x 1 and x 2 are different that is in case of 0 1 or 1 0 the output will be 1.
So, that is what I have in case of xOR function.
And in the same manner as we have done before if I want to plot this in the feature space if I plot this feature space and plot all these feature vectors and x 1, x 2 that is 0 0 0 1 1 0 and 1 1 in this feature space then this is the situation that I have.
You find that when x 1, x 2 both of them 0 then the output is 0, if both of them are 1 then the output is 0, if they are 0 1 or 1 0 then the output is 1.
And now find that I have a difficult situation in the earlier cases.
when we talked about OR function OR AND function, the classes were linearly separable on one side of the straight line of a straight line I had the value 0 on the other side the values were 1s.
But here you find that the values of 0s and 1s cannot be separated by a single straight line rather I need two different straight lines.
And in between the straight lines the values are 1 and outside the straight lines the values are 0s.
So, this is a clear case where the problem is not a linear problem, but it is a non-linear problem.
So, can I implement or can I implement this XOR function using neural networks or neurons let us see.
So, you know from your digital circuits course that an XOR function X 1 or X 2 X 1 XOR X 2 .
2 can be broken into a multiple stepped function.
So, what I can do is I can perform OR operation of X 1 and X 2 and I can also perform NAND operation of X 1 and X 2.
So, here what I have this first I compute X 1 or X 2 and the second one is NAND operation of X 1 X and X 2.
Obviously, this is NAND operation because this is nothing, but x 1 and x 2 complement of that which is nothing but x 1 complement or x 2 complement .
So, I perform AND operation of x 1 x 2, I perform I perform OR operation of x 1 x 2, I perform NAND operation of x 1 x 2 and these two outputs the AND output OR output and AND output I AND them AND them together.
and that gives me x 1 x or x 2.
So, effectively what I am doing is as shown in this particular table I am converting the input vectors x 1 x 2 to an intermediate vector h given the given by components h 1 h 2.
So, here you find that if I consider that output of the OR operation is h 1.
and output of NAND operation is H 2 as we have shown here, then when the input is 0 0 H 1 will be 0 and H 2 will be 1.
When the input is 0 1 H 1 will be 1 H 2 will also be 1.
When the input is 1 0 H 1 will be 1 H 2 will also be 1 and when the input is 0 1 H 1 1, then H 1 will be 1 and H 2 will be 0.
So, as we have done before if I plot H 1 and H 2 that is the intermediate feature space.
So, say this is H 1 and this is H 2, you find that when H 1 and H 2 they are having values 0 and 1, H 1 is equal to 0, H 2 is equal to 1. .
which is the mapped x as x 1 equal to 0 and x 2 is equal to 0.
So, I have h 1 0 and h 2 1 this is where I want my xor output to be equal to 0.
Similarly, when the values are 1 1 both h 1 and h 2 they are 1 1 which is equivalent to x 1 is 0 and x 2 1 or x 2 x 1 is 1 x 2 is 0.
In both these cases I want the xor output to be 1.
sorry I just made some mistake.
So, over here when the h 1 h 2 both of them are 0, x 1 x 2 both of them are 0 that is h 1 is 0 and h 2 is 1.
So, this was correct right.
So, here I want the output to be 0.
In the other case when x 1 is 0, x 2 is 1 or x 1 is 1, x 2 is 0.
which are mapped to both h 1 and h 2 become becoming 1 1, I want the output to be 1.
So, here I want the output to be 1 which is the case when both x 1 and x 2 are either 0 1 or 1 0.
And in the other case when x 1 x 2 are 1 1, I get h 1 to be 1 and h 2 to be 0 that is somewhere over here and here my xor output should also be 0.
So, you find that now I have this is the place where I.
have XOR output 0, this is the place where I also have XOR output to be 0 and this is the place where I want to have XOR output to be 1 and now you find that del linear is a variable.
So, the first non-linear mapping or the first step in the first step when I am computing X 1 or X 2 and X 1 NAND X 2.
These two operations are transferring transforming my input vector to an intermediate vector given by h 1 h 2.
And in this intermediate space in this intermediate feature space the classes the problem is linearly separable.
So, now I can have a linear classifier to classify the input vectors x 1 and x 2, where the linear classifier instead of operating on x 1 x 2.
the in-linear classifier operates on H 1 and H 2.
So, I can represent this again in matrix form.
So, what I have is in the first level I have a set of matrices given by set of weights given by the matrix W 1.
So, these weights are so, if you look at the first one it is minus 0.5 1 1 which are the weight vectors we have used for an OR operation.
The second one the second weight vector is given by 1.5 minus 1 minus 1.
And if you remember from our previous discussion that minus 1.5 1 1 was the weight vector corresponding to AND operation.
So, if I negate it I make it plus 1.5 and this has minus 1 and this has minus 1 this is the weight vector which is used for NAND operation.
So, using this weight vectors I compute W 1.
transpose X, where W 1 is actually the weight matrix consisting of minus 0.511 and 1.5 minus 1 minus 1.
So, this is my intermediate the matrix product that I get.
Now, if I apply the nonlinearity which is a threshold nonlinearity, I get 1 0 0 0 1 1 1 1 and 1 1 1 0 that is my intermediate set of vectors.
which is given by matrix H .
So, this is my H 1 this first row corresponds to vector H 1 the second row corresponds to vectors H 2 .
So, my final output if I put another weight vector now you find that this weight vector corresponds to an AND operation that is minus 1.5 1 and 1 this was my intermediate matrix or set of intermediate feature vectors H 1 and H 2 .
which is given by H. So, if I compute H transpose W 2 now, the output of this matrix multiplication becomes minus 0.5, then 0.5, 0.5 and minus 0.5.
Again I pass this through this threshold nonlinearity and I get the output as 0, 1, 1, 0.
So, you find that, when my h 1 h 2 are 0 1 or 1 0 the output becomes 0 and these are equivalent to the input vectors becoming being 0 0 or 1 1.
And when my h vector that is the intermediate features are 1 1 that is this case the output is also 1 and this is the case when both x 1 and x 2 .
they are either 0 1 or 1 0.
So, I have implemented this XOR function in two steps.
So, I can consider this to be implemented using two layer neural networks.
In the first layer I will have two neurons one giving an OR operation and other one giving an NAND operation.
And you find that because AND is a linear problem NAND is also a linear problem because NAND is nothing but complement of AND.
And, that is what we have done over here that plus 0.1 plus 1.5 minus 1 minus 1 this weight vector actually gives a NAND operation ok.
So, I can implement this XOR operation which is linearly non-separable, but for implementation of this as obvious from this operation that a single layer neural network or a single neuron is not sufficient rather I need 3 neurons.
And, those three neurons are to be arranged in two layers.
One neuron in the first layer will compute OR operation, the other neuron in the first layer will compute NAND operation and the then the third neuron which is in the second layer will combine the outputs of these neurons in the first layer using an AND operation to give you the final output.
So, effectively the kind of neural network that I have is this.
So, here I put my input to be again 1 x 1 x 2.
Suppose this first neuron computes OR operation.
So, we know that for OR operation my weight vectors should be minus 0.5 1 and 1.
And suppose this second neuron computes NAND operation for which I have to have weight vector.
as plus 1.5 minus 1 and minus 1.
So, here what I get is x 1 or x 2 and here what I get is x 1 and x 2.
So, I use this as h 1 and this as h 2 and this second third neuron in the second layer.
This computes AND operation.
So, here I have to have the weight vectors as minus 1.5, 1, 1.
So, this performs an AND operation and finally, at the output what I get is XOR of X1 X2.
So, I get X1 or X2 at the output .
So, through this discussion it is quite clear that, if I have linearly separable problems, single neuron for a two class problem is sufficient.
If I have multiple classes, then I have to have multiple neurons.
So, in case of multiple classes, the way this has to be done is I can have a number of neurons, but in a single there here I have the weight vectors say.
something like this.
So, here I have x over here we have the weight vectors a set of weight vector for this out this neuron, a set of weight vectors for this neuron, a set of weight vectors in this neuron.
Each of these neurons perform suppose the weight vector for this is a W 1 weight vector for this neuron connecting to this neuron is a W 2.
weight vector connecting to this neuron is a W n, if I have got n number of classes or n number of categories, this neuron will compute W 1 transpose X and then a function of this, this one will compute W 2 transpose X and then a function of this, this one will compute W n transpose X .
and a nonlinearity on this.
So, I will get a number of outputs o 1, o 2 and o 3 which actually gives you the score functions or score values for different classes or different categories corresponding to input vector x.
And here you find that all these are linear combinations, linear combinations of different components of the input vector.
So, if my problem is a linear problem.
I can solve it using single layer neuron or single layer neural network.
But if it is not a linear problem, if it is a non-linear problem that as has been demonstrated with a very simple non-linear function of XOR function, I need multiple levels of neurons or multi layer neural network.
So, the kind of neural network that I have to have over here is as given by this.
You find that every layer of the neural network computes a non-linear function of the input feature vector of the feature vector inputted to that particularlayer of neurons and the output of that layer of neurons is an intermediate feature vector maybe of same dimension or maybe of different dimension depending upon how many neurons I have in that particular layer.
We will see that.
as we proceed through our discussions.
But effectively what I have is I have a multi layer neural network and in every layer the neurons compute a non-linear function.
So, when I have a cascade of this non-linear functions as given by the multiple layer neural network as has been shown in this particular diagram.
So, here you find that suppose each of these are neurons a layer of neurons.
And, in every layer it computes a non-linear function.
So, in ith layer it computes a non-linear function f i on the feature vectors which are computed by the previous layer given by the function f i f i minus 1 and each of this are non-linear function.
So, I can cascade all of them together to give you an overall cascaded function as given by this.
So, if my input vector is x the first layer computes f 1 of x, this f 1 of x is inputted to the next layer.
So, which computes f 2 on this f 1 of x that means, this layer computes f 2 of f 1 of x and this way it continues when it comes over here say I have a.
set of mapped feature vectors over here which is given by say h k minus 1 which is a mapped feature vector mapped from this vector x through a series of non-linear mapping where each of these layers of neurons gives some sort of non-linear mapping.
And then I have a final layer of neuron which is the kth layer which when I come to this h k minus 1 these feature vectors are non-linearly mapped from x.
And, as these are non-linearly mapped, so this in in this feature space as given by this vector h k minus 1, these feature vectors are the problem that is posed is a linearly separable problem.
And, once I have such a linear problem, the final layer or the kth layer can solve this using a linear discriminator.
So, effectively I can put all of them in a cascade of operations something like this.
So, at the first layer you compute f 1 of x which gives you say set of feature vectors h 1.
In the second layer you compute f 2 of f 1 of x which gives you a set of feature vectors h 2.
So, you find that each of them are non-linear mappings.
Over here through this cascading operations I get a set of feature vectors say.
h k minus 1.
And when I have this h k minus 1 in thisfeature space as these are non-linear mapping of X, I can have a set of non-linear mapping such that h k minus 1 will lead to a linear problem or a linearly separable problem.
And once I have that the final one that is f k can be a linear machine or it might be a linear classifier.
So, I have a set of operations which maps an input vector non-linearly into a space where they are linearly separable.
And once they are linearly separable then in the final step I can have a linear classifier.
That is what a multi layer feed forward neural network does.
And why is it feed forward?
Because you find that the information is always flowing from input to output.
right, the information does not flow in the reverse way.
Of course, there are other variants of the neural network known as recurrent neural network, where the information can also be fed back we will come to that later.
And here you find that in the number of such cascading stages, here the number of cascading stages which is equal to k, this tells you what is the depth of the network.
So, in this particular case the depth is k. And as the value of k increases, that is the number of layers increases, the depth also increases.
the network giving you the non-linear transformation, it is related to that.
So, I will stop here today and I will continue with the discussion in our next class.
Thank you.
Hello, welcome to the NPTEL online certification course on deep learning.
So, you have seen thatin previous lecture we have talked aboutthe optimization problem particularly optimization in machine learning and we have discussed how optimization in machine learning is different from the general optimization tasks.
We have also talked about the linear and logistic regression.
So, in case of linear regression we have seen that a variable y a dependent variable y ispredicted using a linear function of the components of the feature vector y or a feature vector x or we have written y in the form of y hat the predicted value of y as y hat as w transpose x where x is the feature vector and w is the weight vector.
And in case of linear regression We have said that based on this predicted value of y you take certain decision.
And, we have also seen we have also discussed that this linear regression at W transpose X gives you an idea of what is the distance of feature vector X from the separating plane that is a plane having equation W transpose X equal to 0.
And, more the distance of the feature vector X is from the separating plane.
more confident our decision is.
That means, we know that the feature vector x is well within the region given to the corresponding class.
In case of logistic regression we have seen that this distance measured can be interpreted as a probabilistic measure.
That means, more our confidence is that a feature vector x belonging to some class say y, the probability of that class y given x and the parameters w the parameter vector w should also be high.
So, as the distance goes on increasing the probability asymptotically reaches to 1 or in the other case as the distance goes on reducing in the negative side the probability of the other class goes on increasing.
So, that some of the probabilities belonging to the two classes always becomes equal to 1 because the data has to belong to either of the classes either class omega 1 or class omega 2.
So, both this linear regression and the logistic regression we have discussed with respect to the problems which are two class problems or binary classification problems.
Then we have generalized this logistic regression which is a probabilistic measure to a class of classifier set of classifiers which are known as soft max classifier.
So, the soft max classifier is a classifier which deals with multi class problem.
So, earlier we had seen that in case of multiclass problem we could have a linear machine or we could have a multiclass support vector machine.
Where the linear machine or this multiclass support vector machine gives you class score or it outputs a k dimensional vector where k is the number of classes and the linear machine or the support vector machine tell or multiclass support vector machine gives you a score for every class.
And then for whichever class the score was maximum.
2, we could classify the vector to that corresponding class.
So, as in case of logistic regression for a two class problem or a binary problem that the distance measure is converted into a probabilistic measure.
In case of softmax classifier the class score can also be converted to a probabilistic measure.
For that what we have done is given the class score plus course a class y i to be S y i.
And, class score for every class J to be S j, we have converted this class score into a probabilistic measure that P of y i given X w is equal to e to the power S y i upon sum of e to the power S j, where the summation is taken over all j that comes into a denominator and that is what becomes a normalized probabilistic measure and that is what you have done in case of softmax classifier.
In today's lecture, we are going to discuss about the nonlinearity, how considering nonlinearity is important for machine learning techniques and then we will extend our discussion to neural networks or deep neural networks.
And here it is very very important because when we talk about deep learning the entire deep learning algorithm entire set of deep learning algorithms are based on .
neural network architecture only.
So, in some cases the deep learning is also known as deep neural network algorithms or deep neural network architectures.
So, first let us talk about nonlinearity.
So, again I am repeating the samefigure that earlier we had seen that if the classes are linearly separable, I can pass a straight line or a plane or a hyper plane in multi dimension between the vectors given for two different classes or in other words that this linear function or the hyper plane can separate support can separate the feature vectors belonging to classes omega 1 and omega 2 and I can pass I can have such a hyper plane without any error.
But, you consider a case say something like this if my feature vectors are of this form.
So, here all the feature vectors marked as plus.
So, these are the vectors belonging to belongs to one class let us call it class omega 1 and all the vectors which are marked as minus they are the feature vectors belonging to belonging to class omega 2 .
So, I have the distribution of the feature vectors .
to classes omega 1 and classes omega 2 as shown over here and you can well imagine.
So, this is the case in a 2 dimension,in 3 dimension I can have a spherical distribution of feature vectors belonging to one class and outside the sphere I can have feature vectors belonging to another class and I can have many such complicated distribution of the feature vectors.
So, given this you can well imagine.
that here it is not possible to have a linearseparator between the classes omega 1 and omega 2 ok.
Whichever straight line I form whichever I take I will always have some specification.
So, this is the problem which is linearly non separable problem.
I can I cannot separate the classes using a linear function or this problem cannot be solved using linear function.
So, what we can do in such cases?
Let me just simplify the problem instead of taking vectors in two dimension, let us consider that I will take vectors in one dimension or say scalar features.
So, I take a set of features let me draw it properly.
So, as I said that I have the features on in one dimension.
So, I have a set of samples which are like this that belongs to one class and I can have another set of samples which I am marking in pink which belong to a separate class .
So, given this you find that all the samples which are marked in pink .
are sandwiched between the samples which are marked in blue.
So, given such a case I cannot draw a straight line or I cannot separate the samples belonging to these two classes using a single point on say line x.
So, this is the feature direction.
I cannot have a single point on this featureline x which separates the samples belonging to class omega 1 and the samples belonging to class omega 2.
I can of course, separate them if I take a curve of this form ok or in other words I have got two points on this feature line using which I can separate these two classes.
So, this becomes a non-linear problem it is not a linear problem anymore.
So, how to solve I can still solve this by having some non-linear mapping of the feature vectors right.
So, what I will do is I will have a non-linear mapping of this feature vectors in such a way that these feature vectors will be remapped in this direction ok.
So, as a result the feature vectors all the feature vectors belonging to class say omega Let me draw redraw this figure .
So, all the feature vectors which are marked in pink they will be mapped over here and all the feature vectors which were marked in blue they will be mapped over here .
So, you find that .
which separates the feature vectors which are pink from the separate feature vectors which are blue.
So, one way we can tackle the problem of non-linearity that is if the feature vectors are mixed.
So, that problems can still be solved using a linear classifier, but for that instead of trying to use a non-linear classifier you apply some non-linear mapping on the feature vectors and once the feature vectors are non-linearly mapped.
possibly on a higher dimensional space in that higher dimensional space the feature vectors can be classified using a linear classifier.
So, what we will do in that two dimensional example that we have just shown.
Let us go to this two dimensional example again.
So, the example was like this ok.
So, what I can do here is you find that this is something a circle at the center and the feature vectors belonging to class omega 1 are within this circle and all the feature vectors belonging to class omega 2 they are outside the circle.
So, I can have a non-linear mapping of this feature vectors say non-linear mapping using a non-linear function say phi.
How this non-linear function will work?
You find that I have feature vectors in two-dimensional space which is given by x 1 and x 2.
If I introduce a third dimension say I want to have a value of z where z will be equal to x 1 square plus x 2 square right.
So, if I do that then you find that for any feature vector which which is outside the circle.
So, over here the value of x 1 square plus x 2 square will be more than the value of x 1 square power.
plus x 2 square for any feature vector which is within the circle.
Because for any point on the circle x square plus x 1 square plus x 2 square is nothing but square of the radius of the circle.
And because all these feature vectors which are negative are outside the circle for them the square of the distance will obviously, higher than the square of the radius of the circle.
So, for all these feature vectors which are outside the circle x 1 square plus x 2 square or the value of z will be more than the value of x square plus y square for any feature vector which is within the circle.
So, what I will do is I will now represent all these feature vectors.
So, in the original form the feature vectors are two dimensional having components x 1 and x 2.
Now, I will add a third component z which is x 1 square plus x 2 square ok.
So, now, my feature vectors will be x 1 x 2 and x 1 square plus x 2 square.
So, this becomes the feature vector .
So, you find that I have applied a non-linear function which non-linearly maps these feature vectors from a two dimensional space to a three dimensional space.
And when you consider in the third dimension that is in the dimension of z.
For all these feature vectors which are within the circle having marked positive for them the z value is less than the z value of the feature vectors which are marked negative.
That means, when I consider in the z dimension all the feature vectors which are negative is somewhere over here and all the feature vectors which are positive is somewhere over here.
So, once I have this then I can always pass a plane in between which is passing through the feature vectors belonging to one class and the feature vectors belonging to another So, to by this non-linear mapping on the feature vectors, I am converting on linearly non-separable set of feature vectors to a linearly separable set of feature vectors.
And we can say that this is being done by a non-linear mapping phi and this phi is nothing, but a collection of three functions.
phi 1, phi 2 and phi 3 where phi 1 and phi 2 phi 1 works on x 1 x 2.
So, I will write it in this way let me clear this.
So, x is my feature Richter which is having two components x 1 and x 2.
So, phi of x I can write it as phi 1 x phi 2 x phi 3 x phi x and phi 3 x.
What is phi 1 x?
Phi 1 x gives you x 1, phi 2 x gives you x 2 and phi 3 x gives you x 1 square plus x 2 square and that is what is the non-linear mapping that we are going to have.
So, you find that now I have converted this feature vectors from x 1, x 2 plane from 2 dimension to 3 dimension where now the dimensions are given by phi 1, phi 2 and phi 3.
So, I can simply write this as the space now I can represent as I am having 3 dimensions, I can put it like.
this is phi 1, this is phi 2 and this is phi 3 .
And once I have this you find that the hyper plane in this phi 1, phi 2, phi 3 space can now be written as say W 1 phi 1 plus W 2 phi 2 plus W 3 phi 3 that equal to 0 .
right.
And now if you expand this phi 1, phi 2 and phi 3 what I have phi 1 is nothing but x 1.
So, this simply gives you w 1 x 1 phi 2 is nothing but w x 2.
So, that gives you w 2 x 2 plus phi 3 is x 1 square plus x 2 square.
So, I have w 3 x 1 square plus x 2 square .
which is equal to 0 that is the equation of the plane that I have.
Now, I find that I can have a dual interpretation of this equation in the sense if I consider this equation where my variables are x 1 and x 2 the equation is a non-linear equation right.
Whereas, while training I am given the training vectors that means, for all those training vectors x 1 x 2 are fixed.
So, I can also consider this equation to be a equation in W 1, W 2 and W 3 where W 1, W 2 and W 3 are the variables, but because the feature vectors given for training are fixed so I can also consider X 1 and X 2 to be constants.
So, if I consider X 1 and X 2 to be constants then this equation is a linear equation.
So, if I consider this equation to be a equation in X, X 1, X 2 it is the non-linear equation.
which gives you a non-linear mapping of the feature vectors.
So, the feature vector x 1 x 2 is non-linearly mapped into phi 1 phi 2 phi 3 domain.
And the classifier or the separating plane is a plane where w 1 w 2 w 3 are variables and x 1 and x 2 are fixed ok.
So, by this non-linear mapping you are mapping the feature vectors into another space and possibly a higher dimensional space and in that higher dimensional space.
the feature vectors are linearly separable.
So, you will come to so, this is a very simple example where using a simple mapping I can convert a non-linearly a linearly non-separable set of feature vectors into a linearly separable set of feature vectors.
However, the type of non-linearity may not be so simple, there can be complicated non-linearities and those are the non-linearities which are to be solved by the neural networks that we will see later.
this mapped space is of higher dimension than the original space.
And theoretically it can be proven that if I map feature vectors to infinite dimensional space, then whatever be the complexity of the nonlinearity every nonlinear problem can be solved as a linear problem when you are increasing the dimension to 0.
And for such nonlinear mapping, we can I obviously, I have to have some non-linear functions.
So, now let us see that what are the different kinds of non-linear functions that can help us in such a non-linear mapping.
So, one of the non-linear function which we will use is what is known as threshold function, it is a very very simple function.
So, the threshold function says that if y is a function of x, then output y the dependent variable y will have a value of 1 if x is greater than or equal to 0.
and it will have a value of 0 if is x is less than 0.
So, here this non-linear function is depicted in this form.
So, as x is greater than 0 value of y is 1.
So, here it is 1, if x is less than 0 the value of y is 0.
So, here it is 0.
So, this is the simplest kind of non-linearity which is a threshold function that we can have.
And we will see the use of this sort of non-linearity when we start our discussion on neural network.
The next type of nonlinearity which we have already discussed is logistic regression.
You find that this mapping that we have done from W transpose X to sigma W transpose X is nothing but a nonlinear function, ok.
So, this sigma W transpose X which is 1 over 1 plus e to the power minus W transpose X this is a nonlinear function.
So, this function also helps us to transform a non-linear problem to a linear problem.
The other kind of non-linearity which is widely used in case ofneural network or modern deep neural network is what is known as ReLU or rectified linear unit.
So, this rectified linear unit simply puts as if y is a function of x, then y will get a value of maximum of 0 or x.
So, naturally you find that if x is greater than 0, y gets the value of x and if x is 0 or less y will have a value of 0.
So, for all positive x y and x are same for 0 and less if value of x is 0 or negative then y will assume a value of 0.
So, I can depict this function in this form.
So, here you find that as x is greater than 0, y is equal to x.
So, if you take the gradient of thisslope of this line, this slope is nothing but 45 degree.
So, as y is greater than 0 value as x is greater than 0, y becomes equal to x.
If x is less than 0 or 0, then value of y is 0.
So, this is a linear rectified unit rectified linear unit.
which is in short it is written as ReLU.
And obviously, you find that it is also a non-linear mapping this also gives a non-linear mapping because if y is equal to x or y is some constant a times x that is a linear function, but the moment I put y in this form it is a non-linear function.
So, in case of neural networks we will see later that in simpler cases we can use threshold functions.
In most of the popular neural networks the nonlinearity that is used as a sigmoidal function, but in modern neural networks in deep neural networks instead of sigmoidal function people prefer nonlinearity.
The reason isthreshold function is a simple sort of nonlinearity for simple networks we can use that, but the problem with the threshold function is it is non-differentiable I cannot differentiate it threshold function.
And, we have seen in training earlier that in most of the cases training uses a gradient descent approach where gradient is nothing, but a differentiation right differentiation in multiple directions.
So, because threshold function is an cannot be differentiated it is a non-differentiable function.
So, there it leads to problem in case of training of the neural network because gradient descent cannot be applied on that.
So, that problem is slightly overcome if we use sigmoidal function is a non-linearity.
Of course, sigmoidal function is not the only one.
The other kind of non-linearity which people also have tried is what is known as tan hyperbolic non-linearity, where you have variation of the output from minus 1 to plus 1.
In case of sigmoidal function it is 0 to plus 1.
So, tan hyperbolic kind of non-linearity has also been used, but there again the problem is when the value of W transpose X is very high or the value of the argument is very high the gradient is very very slow right because your curve is almost parallel.
So, gradient almost vanishes and because of that your training or the learning algorithms becomes very slow which is solved by this ReLU because in case of ReLU as long as X is greater than 0 the gradient of the function is unity right.
So, here the gradient does not vanish.
So, that is an advantage of ReLU and when you have modern deep neural networks where you have large number of nodes, large number of layers, hidden layers,then ReLU becomes more advantageous over sigmoidal function.
So, today we have discussed about nonlinearity and we will continue with this discussions in our next lectures.
Thank you.
Hello, welcome back to the NPTEL online certification course on deep learning.
So, in our previous class we have started discussion onautoencoder versus principal component analysis.
So, what we had discussed in the previous class is how you get the principal components from the covariance matrix of the input data.
So, let me just briefly tell youwhat we have discussed in the previous class.
So, that our platform for discussion on comparison between autoencoder and principal components.
becomes complete.
So, what we presented in the previous class is that given a set of your input vectors x, where x we have considered to be a vector set of vectors x 1, x 2 up to say x n having n number of population, where each of this vector x is of dimension d.
that means, there are d number of components in the feature vector.
Then first you find out the covariance matrix from this set of input vectors.
So, the covariance matrix is given by C x is equal to expectation value of x minus mu x into x minus mu x transpose where this mu x.
is the mean of the input vector.
So, mean mu x is equal to sum of x over all x where x is the set of these vectors.
Once you form this covariance matrix C x, then for from this covariance matrix C x is compute the eigenvectors and the eigenvalues.
So, I have set of eigenvalues lambda i.
where i varies from 1 to d as d is the dimensionality of the feature vectors and for each lambda i I have the corresponding eigenvector E i .
And once I had this E i the set of eigenvectors then we had formed a transformation matrix A where A was formed as with the eigenvectors E s as rows.
So, the first row was E 1, the second row was E 2 .
and the last row was E d and these eigenvectors were arranged in rows in such a way that here lambda i or the lambda 1 is greater than lambda 2 and so on it is greater than lambda t .
So, an eigenvector corresponding to the maximum eigenvalue is put as first element or the first row in my transformation matrix and the eigenvector corresponding to the minimum eigenvalue is put as the last row in the transformation matrix.
So, all the eigenvectors are actually arranged in rows in order of the descending order of the corresponding eigenvalues.
So, once I define thistransformation matrix, then I have a transformation which is given by Y is equal to A into X minus mu X and this transformation is what is known as K L transformation.
And from the nature of this transformation you find that x is an input vector and a is the transformation matrix where every row in this transformation matrix are the eigenvectors.
So, every component of y is nothing, but projection of the vector x minus mu x onto the ith eigenvector which is the ith row in this transformation matrix.
So, ith component of my transform vector y is nothing, but the projection of the vector x minus mu x onto the ith eigenvalue .
And, these components of the vector y the transform vector y is are nothing, but my principal components.
And, you find that this principal components are nothing, but the transformation of the input vector x shifted by mu x onto the eigenvector.
So, this transformation basically gives you a mapping from the input space to an eigen space.
And, as we said the eigenvectors been orthogonal, eigen space is also orthogonal.
So, effectively what this transform and the way you obtain the data reduction is that in the transformation matrix A, where A is of dimension d by d right, it has d number of rows, d number of columns.
So, if I reduce the number of columns from the bottom that is I remove columns corresponding to minimum eigenvalues that ensures that in the reconstruction I will have minimum amount of error.
So, that is a different analysis which is beyond the scope of this lecture.
So, I assume that I truncate this transformation matrix A by reducing some number of rows from the bottom.
So, what I do is I make a transformation matrix A with say p number of rows and of course, d number of columns I am not reducing the number of columns where this p is much less than d. So, using that when I go for this transformation y is equal to A into x minus mu x.
This vector a that you get that will have p number of components and if I do not go for any truncation this vector a will have d number of components.
So, as I reduce the number of rows my in my transformation matrix I go for reduction of the dimensionality ok.
But this reduction is done in such a way by removing the eigenvectors in such a way that when I try to reconstruct my original x from y by an inverse transformation.
So, from here x will be A inverse y plus mu x.
So, as in transformation matrix A, I am not retaining all the components or all the rows.
So, obviously, the reconstructed x that I will have will not be actual x, but it will be an approximation of x which is exact ok.
So, this is what I get after reducing the dimensionality.
So, as PCA gives you reduction in dimensionality and we have also seen that autoencoder gives you reduction in dimensionality.
So, whether we can have some relation we can establish some relation between PCA and autoencoder.
We will come to that a bit later, but before that let us try to see what this PCA is actually giving you.
So, to further illustrate the principal components I have taken this binary image.
So, here you find that it is a binary image where all these.
elements which are blue these are 1 that means, I can say that I have pixels present in this location in these locations I do not have any pixel.
So, I can form a vector with only the pixel locations where the pixels are present.
So, accordingly my population of vectors will be given by this 3 4 ah.
So, this is 3 4 right sorry this is 4 3 .
0 1 2 3 4 0 1 2 3.
So, this is 3 4 3.
Similarly, this is the vector which is 3 4 and so on.
So, all the vector positions all the positions where I have a pixel present, I take those positions as my vector population.
And from this set of vectors, I compute the mean of the vectors which is mu x and in this particular case you can compute that this mean vector will be 4.5 4.5.
So, once I have this computation now as we said before that for principal components or for KL transformation I need to have the covariance matrix.
And we defined covariance matrix as expectation value of X minus mu X into X minus mu X transpose.
So, here I have a very small example.
So, from hereif I try to compute that covariance matrix.
So, the covariance matrix for I take this as my first.
vector x 1.
So, x 1 minus mu x where mu x was 4.4 5 4.5, if you subtract 4.5 4.5 from 3 4 what you get is your x minus mu x becomes minus 1.5 minus 0.5.
So, x minus mu x into x minus mu x transpose if you do this multiplication it simply becomes 2.25 0.75.
0.75 sorry yeah 2.25, 0.75, 0.75 and 0.25.
So, that is for the first vector in my vector population that I get.
Similarly, when I compute x 2 minus mu x into x 2 minus mu x transpose in the same manner that will give you 0.25, 0.75, 0.75 and 2.25 you can compute this and verify .
So, this way you compute x minus mu x into x minus mu x transpose for all the vectors that we have in set of vectors x .
And, the expectation value is nothing but average of these matrices that you are getting for each of the vectors I am getting x minus mu x into x minus mu x transpose.
So, in this particular case I have 8 such vectors.
So, I will have 8 such matrices and the average of all those matrices gives you the covariance matrix.
So, covariance matrix in this case is nothing but if you compute that will come out to be the covariance matrix C x equal to 0.75, 0.375, 0.375 and 0.75.
And, from here you compute the eigenvalues as I said that it will be the determinant 0.75 minus lambda 0.375, 0.375 and 0.75 minus lambda you set this determinant equal to 0.
So, you get a quadratic equation in lambdas you solve that and you get two values of lambda, lambda 1 comes out to be 1.125 and lambda 2 comes out to be 0.375 .
And, for these values of lambda you compute the corresponding eigenvectors as we said that the eigenvectors will be nothing, but C X e is equal to lambda e where C X is your covariance matrix e isthe eigenvector and lambda is the eigenvalue.
So, you solve those equations you get your eigenvectors .
So, it comes out to be that in this particular case your e 1 for eigenvalue lambda 1.
comes out to be 1 upon root 2 1 1.
Similarly, E 2 for eigenvalue lambda 2 comes out to be 1 upon root 3 1 minus 1.
Now, what comes next is the interesting one that is we want to see that what are these eigenvectors.
So, I simply superimpose these eigenvectors in the same image space here.
So, here you find that if you look at this you find that the direction of E 1 if you look at this direction, this is the direction in which your spread of data is maximum right and that is what we said that lambda 1 that is the eigenvalue indicates that what is the variation of data in the direction of the corresponding eigenvector.
Similarly, lambda 2 this tells you that the that the variation of data in this direction is minimum right.
So, I get lambda 1 and lambda 2 and here again you find that thissorry E 1 and E 2 and here again you find that E 1 and E 2 they are actually orthogonal and they are centered at the centroid of the pixels.
So, that also says that a KL transformation is a transformation which gives you translation and rotation operations because E 1 E 2 are nothing, but rotated x 1 x 2.
And, this coordinate system u 1 e 2 is translated to the mean of the vectors that we have.
So, this also gives you the rotation and translation transformation right.
Now, coming to the concept ofyour principal components.
So, what are principal components over here?
So, I have data point this, if I take the projection of this data point.
.
See if I take the projection of this onto E 1, so this is the projection on E 1 and this is what is the principal component.
Similarly, if I project this onto E 2, this is also the principal component.
So, this is the first principal component and this is the second principal component.
If I want to represent this by a single dimension or by scalars, I will not consider the projection onto E 2 rather I will consider only projection onto E 1 because E 1 corresponds to the eigenvalue which is maximum.
So, this is my first principal component.
So, to think of this in other way the principal components or the KL transformation is actually a linear transformation.
So, what you are doing is given your input vector input data you are linearly transforming it.
to your principal components right.
So, principal components gives you a linear transformation or linear mapping from the input data space to the output data space.
So, if I want to retain only one component you find that that component is nothing but projections onto this eigenvector E 1.
So, you are projecting onto a line.
If I want to retain two components I want to convert the input data into two principal components by this principal component analysis I will take projections onto E 1, I will also take projections onto E 2.
That means, every vector will now be mapped to a point in the plane E 1, E 2.
So, this transformation is a linear transformation.
So, given this now can we try to establish that what will be the relation between the principal component analysis and autoencoders.
So, bothbefore that just to illustrate what is the power of this principal component analysis.
So, the principal components.
You find that this is just an illustration that I havethis input image.
So, this is the original input image and this is the image which has been reconstructed using only one principal component and you find that the reconstruction is amazing.
So, that simply says that principal components retains the structure of the data.
Instead of onethis original image is actually 256 by 256 number of pixels.
So, you find that reduction is from 256 to by 256 to 1.
So, the amount of compression that has been achieved.
Instead of 1, if I use 5 principal components, then this is the reconstruction that you get.
Instead of 5, if I use 25 principal components, this is the reconstruction that you get.
So, here you can imagine what is the amount of compression that we are getting or what is the compression amount of compression that we are getting over here 256 by 256 is to 256 is to 25.
So, that shows you what is the power of the principal components.
So, this principal components using principal components I can go for dimensionality reduction.
And as we have seen that using auto encoders also we can go for dimensionality reduction.
So, definitely I can establish some relation between these two.
So, what we have seen is in case of principal components this is a linear transformation from n dimensional space or say d dimensional space you are transforming it to say two dimensional space or three dimensional space depending upon the number of principal components that you want to use.
And this mapping is a linear mapping.
As against auto encoders beingneural networks which can implement non-linear functions.
So, the kind of mapping mapping that we can use in case of auto encoders is a non-linear mapping.
That means, in other words we can say that the principal components or the auto encoders can be thought of as generalization of principal components.
So, whatever principal components analysis can do auto encoders can also do the same thing, but auto encoder can do something more because here I can have non-linear mapping not simply linear mapping.
whereas, in case of principal components we have only linear mapping ok.
So, this is what I just said that given a set of data which are just red dots in this figure and I want to convert this set of data using principal component analysis into principal components.
So, if I use just two principal components P c 1 and P c 2.
I am transforming this set of data into onto a state line as given by this pink line right.
So, this issorry I am transforming this set of data onto a plane as defined by P c 1 and P c 2 and this being a linear transformation or linear mapping what I can do is given this set of data this can be approximated on a straight line or the straight line is defined in space P c 1 P c 2.
Whereas, autoencoders are capable of going for imparting non-linear transformation.
So, using autoencoders I can even go toestablish or to extract the non-linear structures which are present in the data.
So, it is possible the same data or autoencoder will learn the inner structure which is not just linear, but it is non-linear as shown by the blue curve.
So, all this set of data points which are red points can now be represented by points on this blue curve, whereas principal components will represent this set of data by points on the pink line.
So, we will present some experiment fromthis particular source which is given at the bottom.
So, these experiments are done on MNIST data set, MNIST data set which is a public domain data set.
which has total this dataset is actually dataset of handwritten digits digits from 0 to 9.
It has total 60,000 training images and total 10,000 test images all handwritten character handwritten digits.
Every image is of size of dimension 28 by 28 that means, it has got 784 number of pixels.
So, when you think in terms of autoencoder at the input side we have 784 plus 1 to take care of the bias 785 number of neurons.
dimension dimensionality reduction from 784 to 2.
So, using autoencoder as well as PCA and then we will also talk about the reconstruction fromthe reduced dimension where for reconstruction the dimension was reduced to 30 from 784.
For dimension dimensionality reduction demonstration it was reduced to 2 because the plane on which we will be projecting this data are two dimensional So, if it is reduced to two dimensional data then projection is I can visualize it.
Then the other things thatoptimizer which is Adam optimizer we will come to that later we have not yet talked about the different types of optimizers.
The loss function that was considered is mean square error that we said and for training 100 iterations was used.
So, this is what is the experiment setup.
Now, what is this is an example of the MNIST data set I was as I said that this is data set of handwritten digits digits from 0 to 9.
ok. And again this is taken from the source asgiven at the bottom.
Now for this data set, the data reduction was done using principal component analysis to two dimensional data you remember our input was 784 right.
So, using principal component analysis it was reduced to two dimensional vectors and also by using auto encoders it wasreduced to .
2 dimensional vectors.
But in this particular caseno non-linearity was used the activation function of the neurons was linear function.
And as is shown in these two data setthese two outputs you find that the output as given by PCA is almost identical to output as given by two layer autoencoderwhich has just one hidden layer right hidden layer and an output they are almost identical.
So, and this is what we said that principal component analysis is our data dimensionality reduction technique, autoencoder can also be thought of as dimensionality reduction technique.
So, somewhere there must be a relation between these two and this is what ok.
So, you find that in some case PCA and autoencoders they give you identical results.
Of course, we will havedifference of results as we said that .
auto encoders are more general than principal component analysis because auto encoders can impose can implement non-linear functions ok.
So, let us see what other results that we can obtain.
So, this is a comparison between deep versus shallow autoencoder.
So, on the left hand sidewhat is shown as the output of a two layer autoencoder.
and on the right hand side what is shown as a deep autoencoder.
And here you find that a deep autoencoder having multiple number of layers as the architecture of the deep autoencoder is shown on the right .
So, deep autoencoder with multiple autoencoding layers or decoding layers can capture the structure of the data better and that is where the beauty of the auto or the beauty of the deep learning .
comes.
Coming to another example, so if we have so in the previous case the output was from deep autoencoder without any non-linear activation function.
Now, you find thatif we use non-linear activation function in a deep autoencoder, then as shown in this diagram on the right the deep autoencoder with non-linear activation function can even capture the structure of the data better.
So, here you find that the different digits this is 0 yeah this is actually 0 set of 0s here it is set of 1s and so on.
So, it can capture the inner structure of the data even better than a shallow autoencoder ok.
So, that is what is the beauty of autoencoders with non-linearity it can.
understand or it can learn the inner structure of the data much better.
This is another example where it was a data reduction from articles from Reuter corpus those scanned images were reduced to two dimensional data.
So, here again you find that autoencoder as compared to principal components that has captured the inner structure much better than what the principal components can do.
This is just another reconstruction example where we said that for reconstruction the data reduction was done up to dimension of 30.
So, from 784 it was reduced to dimension 30.
So, here the top row gives you what was the original image.
In the middle row you have the reconstruction using autoencoder where the autoencoder architecture is given over here and the activation function was relu.
that is rectified linear unit and the bottom row is the reconstruction using principal component analysis.
So, the principal components again having 30 principal components.
So, here again you find that though the data reduction was equal the data were reduced to the same extent both using autoencoder as well as principal components, but the reconstruction using the encoded data using autoencoder.
is much better than reconstructionfrom the principal components though the reduced dimensionality was same.
So, that tells you what is the power of auto encoder over principal components.
Earlier we have seen the power of principal components.
Now, it shows thatthe auto encoders are even more powerful than principal components.
Similarly, this is another example of reconstruction where we have a set of face images.
So, the top row is youroriginal image, the second row is again output that is the reconstruction from 30 dimensional autoencoder outputs and the bottom row isreconstruction from 30 dimensional principal components.
So, here again you find that the middle row in the middle row the data structure of the input data or the input images.
has been reconstructed much better than in the bottom row.
So, it also shows that auto encoders with nonlinearity or deep on auto encoders with deep nonlinearity are much more powerful than principal components ok.
So, let me stop thistodays lecture here.
We will next talk about .
the training algorithms of autoencoders.
Of course, the training will be back propagation training as we said earlier and the errors or the loss function for back propagation training that we will be using will be the sum of squared error loss.
So, we will come back later.
Thank you.
Hello, welcome to the NPTEL online certification course on deep learning.
In our previous lecture, we were discussing about the back propagation learning in feed forward neural network or multi layer neural network.
And you remember that the loss function or the error function to be minimized that was considered in our previous lecture was sum of squared error or which is also known as quadratic error.
So, the error function which was considered .
was given by E is equal to half of O j k minus T j square, where the summation is taken over j is equal to 1 to m k that is all the nodes in the output layer.
Now, we will see that what are the problems with this quadratic loss function or squared error function and we will try to find out or we will try to investigate that what alternative loss function that can be used .
to avoid the problem which is given by this quadratic loss function .
So, for updation of the weight rules you find that the updation rule that we have used is W i j k gets W i j k minus eta times delta j k times O i k minus 1 in case of quadratic loss function, where this delta j k was derived from the derivative of the sigmoidal output .
So, we got delta j k as o j k, o j k is obviously, the output of the j th node in the k th layer and we are considering the layer to be the output layer here.
So, it is o j k into 1 minus o j k into o j k minus t j .
So, you willappreciate that when this error updation rule or weight updation rule is necessary .
If, I feed a training vector and I get the output if I find that the vector is correctly classified obviously, I need not go for weight updation.
So, I have to weight update the weights only when I find that I have fed an input vector which may belong to say class i, but my classifier is misclassifying that to class j.
So, only in such cases when the decision given by the classifier is wrong.
I need to update the weight vector.
As long as the decision is correct the weight vectors need not be updated.
And for updation of the weight vectors when the moment you get an error is obtained by gradient descent rule as is given over here.
So, as is givenin this particular case.
And the one that is involved in the weight updation rule that is derived from the gradient is delta j k .
So, you find that this O j k into 1 minus O j k is nothing, but derivative of the sigmoidal function which is W transpose X .
Now, let us consider a case that I have an weight vector X which actually belongs to class say 1 .
So, this was the training pair which was given .
x that weight vector x it actually belongs to class 1.
So, when it is classified by this classifier the output of the node if I consider I have a single neuron at the output say it is a two class problem.
So, output of the neuron should be 1 or close to 1, if the output of the neuron is not 1 or say it is very close to 0 that means, I have an error.
right.
And because I have an error I have to go for updation of the weight vectors by propagating this error in the backward direction and that is where this gradient of the output that comes into picture.
So, here you find that actually my y should be equal to 1, but I am getting an y which is equal to 0 or near to 0 and that comes over here.
my output is O j k times 1 minus O j k. So, this product O j k minus 1 into 1 minus O j k becomes very very low.
Similarly, in the other case if attaining vector is given as belonging to 0, but the classifier classifies that to class 1 that means, my output of the neuron should actually be 0, but the classifier has given a very high output.
That means, it is a misclassification.
Again in this case I have to go for updation of the weights following the same weight updation rule and here you find that O j k as decided by the classifier being very high 1 minus O j k will be very low and that is becauseif you look over here when your output is very low in my sigmoidal function I am somewhere over here .
When the output is very high in the sigmoidal function I am somewhere over here.
And in both these regions your derivative of the sigmoidal function that is sigma dash w transpose x that is very very low.
And in the extreme case it may even vanish.
So, the gradient vanishes and if the gradient vanishes or the gradient is very very low you find that this gradient is directly influencing the rate of training.
k because your rate of training is controlled by not only theconvergence rate eta it is also controlled by delta j k. So, if O j k minus O j k into 1 minus O j k whether O j k is 0 or O j k should be 1 whatever the case may be if any of the terms is very low then your rate of learning becomes very very low.
So, that is the effect of or the bad effect of this quadratic loss function that we have.
So, is there any remedy of this?
So, let us try to see that whether any other loss function can avoid this tendency of slow training or slow learning.
So, here comes another loss function which is called cross entropy loss.
So, how do you define this cross entropy loss?
Again I am taking a two class problem.
So, if of which are vector x.
So, again I am assuming that my input training vectors are given by given as ordered pairs x y, where x is the input vector and y is the ground truth that is the actual class to which this vector x belongs this is given for training purpose right.
So, if y is actually equal to 1 that means, I get a training vector from class.
1 for which output should be equal to 1 whereas, output of your neural network is O.
So, whatever is the output this output actually gives you the likelihood that Y is 1.
In the same way if Y is equal to 0 that means, the training vector belongs to class belongs to another class then 1 minus O where O is the output of the neuron that gives you the likelihood that Y is 0.
So, I can combine these two to get a likelihood that needs to be maximized which is given by o to the power y into 1 minus o into 1 minus o to the power 1 minus y.
So, this is the likelihood that needs to be maximized for training this neural network.
And from here of course, you find that an exponent is involved in this expression and I do not like exponents.
So, how do we how do we avoid the exponents.
So, obviously, you take instead of the likelihood you take the log likelihood.
So, the log likelihood simply becomes y log o where o is the output of the neuron ok. Again here I am assuming that the output is a sigmoidal function of the weighted sum of the inputs as given over here.
So, I get this log like log likelihood which is given by y log o into 1 minus y log o minus y .
log 1 minus o.
So, here you find that if y is equal to 1 in that case this term that is if the training sample x is taken from class omega 1 then I should get y equal to 1 right in that case 1 minus y is 0.
So, what I have to maximize is y log o.
In the other case if y is equal to 0 that means, the training sample has been taken from other class.
So, this term y log o becomes 0.
So, what I have to maximize is 1 minus y log 1 minus o.
So, this is what is the likelihood that I need to maximize.
So, once I have this log likelihood from here I can find out thecost function, I can define a cost function which is minus y log o.
into 1 minus y log 1 minus o take the sum over this sum of this over all the input feature vectors and then you take the average.
So, I am defining a cost function c to be minus of 1 by n summation of y log o plus 1 minus y log 1 minus o taking the summation of this over all input vectors and taking the average which is given by 1 by n and this is what is known as cross entropy loss.
So, you find that the in the previous case when we talked about we said that we want to maximize this log likelihood which is y log o plus 1 minus y log 1 minus o which is equivalent to minimization of the function c which is our cross entropy loss.
So, again as before .
So, for minimization of this cross entropy loss we have to follow the gradient descent approach.
So, I need to take the derivative of this cross entropy loss C with respect to the weight vector of the gradient of C with respect to weight vector w and I can do it partially.
So, you take the derivative of thiscross entropy loss C with respect to w i that is the ith component of the weight vector w. So, what I compute is del C del w i .
And, from here you find that when I take the derivative of this cos entropy loss c the derivative of c with respect to w i what I get is y by sigma theta as we have said earlier this this theta is nothing, but w transpose x .
So, I get 1 upon sigma theta minus 1 minus i 1 minus y upon 1 minus sigma theta .
into del sigma theta del w i because this is a logarithmic function.
So, it is actually 1 upon O and O is nothing but sigma theta.
So, del C del w i simply becomes 1 upon n sum of y upon sigma theta minus 1 minus y upon 1 minus sigma theta into del sigma theta del w i take the summation over of this.
over all input vectors x and that simply becomes minus 1 upon n then sum of y upon sigma theta minus 1 upon y upon 1 minus sigma theta into again here I apply the chain rule.
So, it is del sigma theta del theta into del theta del w i simply using the chain rule.
So, if I go for simplification further .
The same expression is written as minus 1 upon n then sum of 1 upon sigma theta minus 1 minus y upon 1 minus sigma theta into del sigma theta del theta into del theta del w i which simply becomes if I go for simplification of this it simply becomes minus.
So, here you find that del sigma theta del theta.
This del sigma theta del theta as it is a sigmoidal function is nothing, but sigma theta into 1 minus sigma theta and del theta del w i as you remember that del theta was w transpose x.
So, del theta del w i simply becomes x i because it is nothing, but w i times x i take the summation over all i.
So, it simply becomes x i.
So, this term del sigma theta del theta into del theta del w i is simply sigma theta into 1 minus sigma theta into x i.
And when I simplify this term that is y upon sigma theta minus 1 minus y upon 1 minus sigma theta the say expression simplified expression simply becomes y minus sigma theta upon sigma theta into 1 minus sigma theta.
So, from here you find that this sigma theta into and thissigma theta into 1 minus sigma theta and this sigma theta into 1 minus sigma theta gets cancelled.
So, my expression simply becomes 1 by n into x i times sigma theta minus y take the summation of this and then you take the average.
So, my gradient del C del w i simply becomes 1 over n then x i o minus y where o i is nothing but sigma theta take the summation over all input vector x that is what is my del C del w i.
And once I have this del C del w i I can just write the weight updation rule which is nothing but w i gets w i minus eta times 1 by n sum of x i into O minus 1, where the summation has to be taken over all the feature vectors input feature vectors.
Now, what is the advantage that we get?
You find that in case of cross entropy ah in case of the squared error loss or the quadratic loss.
In the updation term we had a term sigma theta into 1 minus sigma theta which is nothing, but derivative of sigma theta with respect to theta.
And this sigma theta into 1 minus sigma theta these terms are responsible for slow learning.
because, if sigma theta is very high very high means it is very near to 1, 1 minus sigma theta almost vanishes.
On the other hand if I am on the other side that is sigma theta is almost equal to 0, then also the derivative term almost vanishes.
And because of the vanishing of the derivative the learning becomes very very slow.
Whereas in this case in my updation term I simply have O minus y i times x i, where O is the output and y i is the ground truth.
So, here you find that if my ground truth is actually 0 that means, if y is equal to 0 whereas, if I get O to be very high say 0.99 something like this, then O minus y is high unlike in the previous case.
Similarly, if my y is 1, then I get O minus 0 and O is a 0.001 it is very low, then again O minus Y the absolute value is very high.
That means, my rate of learning is now proportional to the difference of the output that you get and what is my ground truth.
So, if the error is more the rate of learning is more which is in contrast to what we have obtained in case of quadratic error that if error is very very high.
your rate of learning becomes low which is not desirable.
So, here if your error is high the rate of learning is also high.
So, that is the advantage of this cross entropy loss over thequadrature loss.
So, this is what we have got in case of a binary classifier or a two class problem and this cross entropy that we have defined.
.
As given by this term this is what is known as binary cross entropy that is y log o into 1 minus y log 1 minus o this is what is known as binary cross entropy right.
So, what we have done is in case of a two class problem or binary classifier, how do we extend this to a multi class problem?
So, you remember .
that in a multiclass problem, I have at the output layer say c number of nodes where c is the number of categories or the number of classes.
And when I want to use this concept of entropy loss or cross entropy loss, then the outputs are to be defined or are to be obtained in a probabilistic sense in the manner that O j should give me that, what is the likelihood that y j is equal to 1.
And for that we had defined something called softmax classifier, where softmax classifier gives you the normalized probability at the output that 2 what is the probability that an input vector x belongs to class omega j.
So, that is what is given by softmax classifier.
So, when I want to use this cross entropy loss.
for training the feedback feed forward neural network at the output layer I assume that the output is a soft max output.
That means, the outputs aregives you the probability of belongingness of a of an input vector to a particular class.
So, given this the same thing that we have defined before that we discussed before that if my y j is equal to 1 then o j k that is the output of the j th node at the output layer that is k th layer that gives you what is the likelihood.
of Y j belonging being 1.
Similarly, 1 minus O j k tells you that what is the likelihood that Y j k Y j is 0.
So, accordingly in the same definition of the binary cross entropy for every individual output node I can use.
So, for the j th node the corresponding cross entropy is defined as Y j .
log o j k plus 1 minus y j log 1 minus o j k take the negative of this.
So, that becomes the cross entropy or binary cross entropy corresponding to the j th layer j th node in the output layer.
And to get the overall cross entropy what I have to do is I have to take the sum of all these entropies all these cross entropies over all the nodes in the output layer.
So, that is the reason that I take the summation over of this over all j, where j varies from 1 to m k, where m k is the nodes number of nodes in the output layer.
And this I have to compute over all the feature vectors x.
So, I have this outer summation you take the summation over all input feature vectors x.
And again here as before if I take the derivative of C with respect to w i j k. Now, you find that I have multiple number of layers.
I have multiple number of nodes in every layer.
So, my index becomes i j that is from ith node in k minus first layer to jth node in the kth layer.
So, I compute del C del w i j k and as before you can verify that the output will be 1 upon n summation of o i k minus 1 where o i k minus 1 is the output.
of the ith node in the k minus first layer.
So, it is sum of O i k minus 1 into O j k minus y j and you take the summation over all j and take the average.
So, accordingly your weight updation rule using this cross entropy loss function becomes W i j k getting W i j k minus eta times again eta is your rate of convergence .
1 over n summation of O i k minus 1 into O j k minus y j.
And again as before you can find that more the difference between O j k and y j, O j k is your actual output that you are getting from the j th neuron in the k th layer and y j k is the expected output or this is the ground truth.
So, if o j k minus y j is more that indicates that error is more and your updation component the value by which you want to update the weight vector is now directly proportional to o j k minus y j.
That means, if the error is more your rate of learning is more, if the error is less the rate of learning is less.
So, this is the advantage of using the cross entropy laws.
for training the neural network over the quadratic error which is used for training the neural network.
But you have to remember that if I want to use this cross entropy loss, then output of the neural network must be a softmax output because it has to be a probabilistic measure.
We will stop here today, we will continue with other neural networks in future classes.
Thank you.
Hello, welcome to the NPTEL online certification course on .
deep learning.
In previous few classes we have discussed about the back propagation learning, we have also talked about the different types of loss functions which the back propagation learning algorithm tries to minimize while training a neural network.
Now, as we said earlier that back propagation learning is the heart of the deep learning process.
So, understanding the back propagation is very very important.
So, though we have talked about the back propagation learning algorithms before, I want to explore further on back propagation learning with examples.
So, that the process of back propagation learning is very very clear to you.
The reason being in all subsequent lectures whenever learning is involved, I will simply refer to as back propagation learning without going into details of the algorithms.
So, it is very very important that you fully understand what is back propagation learning and what is the mechanism for back propagation learning or even when you write an algorithm, when you write a program involving the learning of a neural network, how you can write simple programs even exploring the parallelism of the machines.
And that is also important because when we talk about deep learning in today's scenario.
You will find that the concepts that we discussed in deep learning those concepts are not very new the concepts even existed long back the neural network is few decades old.
But why this branch of machine learning was not so popular earlier or not being used in a broader sense is that we did not have the computational power.
Now, we have the computational powers in the form of high performance computers, in the form of GPU or graphics processing units which are massively parallel.
So, making use of those parallel processing capability now implementing the machine learning algorithms working onhuge amount of data has become feasible.
So, over there exploiting parallelism is also very very important in order to make your machine learning algorithm successful.
So, over there the deep learning or understanding of the deep learning and how the parallelism in deep learning algorithms can be exploited knowing that is very very important for application of deep learning algorithms or development of any deep learning solution for any kind of problem.
So, what we have done as I said in our previous classes is that we have talked about back propagation learning inmultilayer perceptron.
We also talked about the loss functions like sum of squared error loss function and also we talked about the cross entropy loss function.
And over there we have seen that the problem or the drawback of squared error loss function which sometimes leads to very slow learning rate particularly when your actual output that you are getting from the network is far away from the target output.
output.
There the derivative almost becomes 0, the derivative almost vanishes.
So, as a result the learning rate becomes very very low, which in case of cross entropy loss function that problem is avoided because there your learning rate as we have seen before is proportional to the difference between the target output and the actual output that you get.
So, as a result if I use cross entropy loss as loss function to be minimized, the learning rate is much higher than the learning rate that you get in case ofusing sigmoidal function andthe squared error loss.
So, today we will further explore on back propagation learning with some examples, examples at the network level and also examples at the node level because within every node.
partial derivatives.
So, what we have is say I have a feed forward network let us assume that I have a feed forward network having 5 layers.
So, I have first layer, what the input is actually input vector x.
The first layer transforms this x with a mapping function say f 1 and this gives me an output of say theta 1 which goes to next layer of this feed forward network that computes a function f 2 on the input which is fed to it.
So, it computes f 2 on theta 2 and this gives me an output say it computes f 2 on theta 1 and that gives me an output say theta 2 which in turn is fed to the next layer which has a mapping function f 3 and this gives an output of theta 3 and so on it theta 3 enters f 4 giving you an output of theta 4 and finally, this theta enters the final layer which computes the mapping f 5 and so, you get your final output say O right.
So, now, if I want to find out the derivative of O with respect to X.
So, remember what this what does this derivative actually tell you?
That derivative tells you that considering O as a function of X, if I make a little per perturbation if I perturb x slightly what is its effect on the final output O and that is what is given by del O del x.
Now, in this case your final output is f 5 ok.
So, I can say that my final output O which is equal to function f 5 that works on f 4 that works on f 3 .
which works on f 2 and finally, f 2 works on f of x .
So, this is my final output function o.
So, if I want to compute del o del x, I cannot really directly compute del o del x because output o is quite far away from x .
So, from input x I cannot directly see what is O.
So, to come to O or the output I have to pass through the functions f 1, f 2, f 3 and f 4.
So, to compute this f x what I have to do is O is directly visible to theta 4 right.
So, what I have to compute is I have to compute del O del theta 4.
Then theta 4 which is the output of f 4, 2 that is directly visible to theta 3.
So, I have to compute into del theta 3 del and theta 3 is directly visible to theta 2sorry theta 4 is directly visible to theta 3.
So, I have to compute del theta 4 del theta 3, where theta 3 is visible to theta 2.
So, I have to compute del theta 3, del theta 2, then del theta 2, del theta 1 and then finally, del theta 1 and del x.
So, you find that this chain of partial derivatives when multiplied together that actually gives you del o del x and del o del x as we said is nothing but the sensitivity of the output.
on the input x that is if I x if I change a x or perturb x slightly what is its effect on the final output.
That is what is given by this partial derivative and as we have seen here that when I have a sequence of functions from input to output I cannot directly compute this partial derivative rather I have to make use of the chain rule of differentiation to find out the partial derivative of output with respect to input.
So, in the back propagation learning algorithms, we have made use of these properties of the partial derivatives.
So, let us see that what we have done.
So, now, we will explain this back back propagation learning algorithm with an example and the example that I am going to take is on the network layer that is considering the neural network as a whole.
So we have explained this algorithm before.
We have had all the derivations, but I want to explain further and physically see practically see how does it work with the help of an example.
So, for this example I consider a two layer neural network.
So, MLP is a multi layer neural network out of that I am only considering two layer with one output layer, I have two nodes in the output layer, one hidden layer and one input layer.
And we had said earlier that the input layer nodes actually gives you the identity functions or identity mapping.
That means, whatever is the input to a node in the input layer that node simply outputs that input and fits it to the nodes in the next layer.
The hidden layer nodes as we seen earlier actually imposes the non-linearities.
So, as we increase the number of hidden layers, we can capture more and more complex form of non-linearity.
Then finally, at the output layer node which are actually classifying layer.
So, we can classifying nodes, it implements actually linear classifiers.
So, if we have the cases which are not linearlyseparable, the hidden layer nodes actually maps those non-linearly separable inputs non-linearly to a space where they are linearly separable.
And once they are linearly separable in a latent space, then I can have a linear classifier to classify them.
So, that is what the hidden layer does.
and we have seen that earlier.
So, we compute orwe consider that we have 2 sets of weights which later we will see that we will represent these as weight matrices.
One set of weights connecting the input layer to the hidden layer and another set of weights connecting the hidden layer to the output layer.
So, those weights connecting the input layer to the hidden layer are given by W 0 1 1 because the convention that we said we are following is.
as I have I am considering a two layer network.
So, the output layer is termed as layer 2, the hidden layer which is before the output layer is termed as layer 1 and the input layer is termed as layer 0.
And we have considered earlier that a node say ith node from K plus say an ith node from k minus first layer is connected to the jth node of the kth layer through a connection weight which is given by W ijk.
So, following the same convention at the input layer from the input layer to the hidden layer the connection weights or the weight values are W 0 1 1 similarly W 0 2 1 these are the weights which connect the 0th node at the input layer.
2 1 1 and W 2 2 1, they connect the second node of the hidden layer to the first and second nodesecond node of the input layer to the first and second node of the hidden layer respectively ok. And the same the same convention is also followed for the connection weights between the hidden layer to the output layer.
And in this casein this particular figure as I am considering are two category case.
So, my outputs are actually x 1 2 and x 2 2.
So, these are the outputs which will be available from this neural network.
So, that example that I am going to consider here isthis.
I consider a set ofconnection sets between the input layer to the hidden layer and hidden layer to the output layer.
at random initially.
So, the connection weights that we given as W 0 1 1 is 0.5, W 1 1 1 is 1.5, W 2 1 1 is 0.8, then W 0 2 1 is 0.8 again, W 1 2 1 is 0.2 and W 2 2 1 is 0.8.
is minus 1.6.
So, these are the connection weights from the input layer nodes to the hidden layer nodes.
So, that is why we use the superscript which is 1.
Similarly, I take another set of weights W 0 1 2 which is 0.9, W 1 1 2 which is minus 1.7, W 2 1 2 which is 1.6, W 0 2 2 which is 1.2, W 1 2 2 which is 2.1.
2 and W 2 2 2 which is minus 0.2.
These are the connection weights from the hidden layer nodes to theoutput layer nodes ok. And I also consider an input vector which is a two dimensional vector as given as 0.7 and 1.2.
So, these are the components of the input vector.
And, I assume that this input vector belongs to category 1.
So, as I assume this belongs to category 1 that means, the output from the output layer nodes the output of the first node will be equal to 1 because my input is category 1.
And, the output of the second node in the output layer should be equal to 0 because my input belongs to category 1.
So, output of the first node should be equal to 1.
So, as a result the target that I have is the target vector which should be 1 0.
So, the back propagation learning should try to adjust the weights in such a way that the output that I get should be equal to 1 0 or should be close to 1 0 when the learning is complete or when the input vector is correctly classified by this neural network.
So, let us see how this is done .
So, first let us see what happens in the forward pass or in the feed forward pass.
In the feed forward pass whatever you are feeding to the input as we have seen before it is processed at different layers in different hidden layers finally, goes to the output layer and output layer gives you the final output.
So, I am representing the connection weights from the input layer to the hidden layer.
layer by an weight matrix W 1 over here.
So, this weight matrix W 1 represents the connection weights from the input layer to the hidden layer.
So, these are the connection weights which is represented by weight matrix W 1.
And, simply similarly another matrix W 2 which is given this is represented by W 2 which is representing the connection weights from the hidden layer to the output layer.
So, given these two now you see that how this computation is done in the feed forward pass.
We said that we have the input vector which is a two dimensional vector having values 0.7 and 1.2.
So, this is 0.7 and 1.2 and as we said earlier that we append an additional component in this input vector.
which is equal to 1.
The whole purpose of doing this that I can have an unified representation that is the bias term which is W 0 can be considered as part of weight vector only right.
So, that it is W 0 plus W 1 times x 1 plus W 2 times x 2.
So, to have this unified representation we include an additional component in in the input vector x which is equal to 1.
So, given this the computation at the input layer can be done like this that the output of as we said earlier that every node in the in the neural network computes two parts.
The one part computes weighted sum of the inputs which is given by w i x i summation over all i.
And, the second part computes a non-linear function f on sum of w i x i .
So, this non-linear function f that we are considering a sigmoidal function over here.
And, we are representing this weighted sum of the inputs as a different parameter or with a different variable which is theta j or which is theta j indicates from which node it comes out .
So, given this now you find that weighted sum of these two nodes in the hidden layers node 1 and node 2 will be simply given by the matrix multiplication.
I have this weight matrix W 1 multiply that or post multiply that with your input vector or augmented input vector and that gives you the weighted sum that you get.
at every node in the hidden layer.
So, node 1 at node 1 in the hidden layer your weighted sum theta 1 1 is coming as 2.51.
If you simply do this multiplication the first row with this column vector you get 2.51.
Similarly, the weighted sum that you get from the second node in the hidden layer is minus 9.8.
If you multiply this row second row of the weight matrix W 1 with this input vector I get minus 9.8.
So, these are these are actually the intermediate values.
And final output value is after application of nonlinearity and as I said that the nonlinearity that I am considering in this case is a sigmoidal non nonlinearity.
So, the output from the first node x 1 1 will be sigmoidal function of theta 1 1.
At sigmoidal function is nothing, but this that is 1 upon 1 plus e to the power minus theta j 1.
You remember that the superscript 1 is being used that I am doing this computation at the hidden layer which is layer 1 in our case.
So, with this the output of the first node after applying nonlinearity which is x 1 1.
So, this is nothing, but x 1 1 plus So, this x 1 1 will become 0.92 and x 2 1 that is the output of the second load in the hidden layer will be 0.27.
And these are the values which are intermediate vector obtained after applying a non-linear function in the hidden layer is fed to the final output layer.
So, this vector along with So, this vector comes out over here appended with 1 as before to take care ofthe bias term as part of the weight vector.
So, this is the vector which is actually fed to the input of the output layer.
And the connection weights or the weight matrix between the hidden layer to the output layer is given by W 2 which is nothing but this.
And in the same manner The weighted sum of the inputs as given by the nodes in the output layer which is W i j 2 into x i 1, x i 1 means it is the output of the ith node in the first layer which is hidden layer and W i j 2 is the connection weight from the ith node in the hidden layer which is layer 1 to the jth node in the output layer which is layer 2.
So, if I compute this .
1 2 and this is theta 2 2.
Final output from this output layer is sigmoidal functions of these two quantities which comes out to be 0.44 and 0.95.
So, now, you find how what is the concept of error?
We said that this vector x which is 0.7 1.2, we have taken this vector x from class 1 or category 1.
And, because this vector x is taken from category 1, my target vector is actually 1 0 because it is taken from category 1.
So, the output of the first node that should be 1 and output of the second node that should be 0 ideally.
Or in other words output of the first node should be near to 1, very close to 1 and output of the second node should be very close to 0 if this x is correctly classified by this neural network.
But, in contrast with this arbitrary or randomly chosen weight vectors, the actual output which is given by this neural network is 0.44 and 0.95.
So, obviously, the output of the second node is much higher than the output of the first node.
So, clearly it is a misclassification.
I should actually try to get 1 0, but rather I am getting 0.44 and 0.95.
So, the difference between these two vectors is what is my error and the back propagation learning algorithm tries to minimize this error by adjusting the connection weights by propagating the error terms in the backward direction.
So, now, let us see that how that can be done or how the network does it.
So, these are things which we said before by that is the first component of the target was 1 and the first component of the output that I was getting is 0.44.
So, clearly there is a difference and that is what is the first component of the error vector.
Similarly, for j equal to 2 that is output of the second neuron over here, we actually got 0.95 whereas, ideally it should be 0.
So, 0.95 minus 0 that is the second component of the error vector.
If, squared these two error components add them together that gives you sum of squared error.
I put a multiplication factor half because as I am taking sum of squared error I have a squared term when I take the derivative this squared term becomes 2 and 2 and half gets cancelled.
So, that is the reason I have half over here and that is what is my loss function, the loss function that I want to minimize by using .
learning algorithm or back propagation algorithm.
Again over here you find that this x j 2 I want to minimize this with respect to different weight vectors.
So, I have to go for the chain rule x j 2 is defined in terms of theta j 2 which is an intermediate variable and theta j 2 is defined.
with respect to weight vectors and the output from the first layer nodes of the hidden layer nodes that you get.
My aim is that I should adjust the weight components W ij 2 that is the weights which are connecting the nodes in the first layer to the output layer by using back propagation algorithm.
So, for doing that what I have to do is I have to get the derivative of this error with respect to ij 2 and then I follow.
the gradient descent procedure.
So, I have to take the gradient of the error with respect to the weight vectors.
And again here here what we have done before this is not new we have discussed this before that I have to compute del a del e del w i j 2 and following chain rule this becomes del e del x j 2 because e is directly visible to x j then del x j 2 del theta j 2 because x j 2 is a function of theta j 2.
2 and then del theta j 2 del w i j 2 because theta j 2 is a function of w i j 2.
And if you do this each of these terms del E del x j 2 is x j minus T j del x j 2 del theta j 2 is x j 2 into 1 minus x j 2 and del theta j 2 del w i j 2 is nothing, but .
x i 1.
And if you remember what we did before is we have defined this term this part that is x j 2 into 1 minus x j 2 into x j 2 minus T j as delta j 2 because this is a term which will be passed backward.
W ij 2 minus eta times del E del W ij 2 or this is nothing, but del E eta times delta j 2 x i 1 where this eta is an hyper parameter which controls the rate of learning or the rate of convergence.
So, in this lecture let us stop here today, we will continue with this in our next lecture.
Thank you.
Hello, welcome to the NPTEL online certification course on deep learning.
In previous few lectures, we are discussing about the autoencoders and we have seen that autoencoder is an algorithm which tries to learn a compressed domain representation of the input data or it tries to learn the structure present in the input data.
And in order to do that, what you do is given an input data.
you pass it through a neural network which is obviously, a multi layer neural network where you try to reconstruct whatever input you are feeding the same signal the same data you try to reconstruct at the output.
But while this information the data passes from the input to the output layer it passes through one or more hidden layers.
So, in the basic configuration we have seen that an autoencoder consists of one hidden layer, where the number of nodes in the hidden layer is much less than the number of nodes in the input layer that is a configuration which is known as under complete autoencoder.
So, as it passes through a hidden layer which is also known as bottleneck layer, the information which while it passes through the hidden layer having a lesser number of nodes than the input layer, the network learns a compressed domain representation.
What will happen if the hidden layer contains the same number of nodes as the input layer or the number of nodes which is even larger than the input layer.
Later on of course, we will see other configurations of the autoencoder where such thing is also possible, but in such cases the autoencoder will learn a compressed domain representation by introducing some constraint which we will term as sparsity constraint.
We will come to that later, but so far what we have discussed is what is known as under complete autoencoder.
So, there if we assume that your number of nodes in the hidden layer is same as the number of nodes in the input layer and obviously, the same as number of nodes in the output layer and or even more than that.
In that case it is possible that autoencoder will simply try to memorize the input data and the function that it will learn is a simply identity function, whereby wherever is there at the input the same will be reproduced at the output.
But, by putting a hidden layer having limited number of nodes known as bottleneck layer as we have just said, the network is forced to learn a compressed domain representation or the network is forced to learn the salient features which are present in the input data and that is the purpose of an autoencoder.
Reconstruction of the input signal is not the purpose of autoencoder.
But, the purpose of autoencoder is that it learns a compressed domain representation or it learns the salient features in the input data and using this salient features the decoder side should be able to reconstruct your original data.
So, we are actually interested when it comes to the application of autoencoder we will come to that a bit later.
We are actually interested in the output of the hidden layer or the bottleneck layer which is a compressed domain representation.
So, we have also seen that as we are talking about the compressed domain representation, this is also nothing, but what is known as dimensionality reduction.
So, if your number of nodes in the hidden layer is much less than the number of nodes in the input layer, then in the compressed domain representation the dimensionality of the latent variable which is mapped from the input data, the dimensionality of the latent space data.
is much less than the dimensionality of the input data, but still the decoder should be able to reconstruct the input from that reduced data.
So, this is what is dimensionality reduction.
And we have also discussed in our previous lecture that when we talk about dimensionality reduction, traditionally a very popular method for dimensionality reduction is what is known as principal component analysis.
That is the input data is projected onto the eigenvectors into the eigen space.
And if the input data is projected into the eigenvectors obviously, if you have a set of data which is of dimension d the number of eigenvectors will be d number of eigenvectors, but for every eigenvector there be a corresponding eigenvalue.
So, when you form the transformation matrix, you form the transformation matrix.
using the eigenvectors as rows in the transformation matrix.
And when you form this transformation matrix as we have seen in your previous lecture that the first row in the transformation matrix will be the eigenvector corresponding to the maximum eigenvalue and the last row in the transformation matrix will be the eigenvector corresponding to minimum eigenvalue.
And in between all the rows are formed by eigenvectors arranged in descending order of the corresponding eigenvalues.
So, now for transformation purpose if we retain only few number of rows in the transformation matrix from the top.
So, we retain only one row.
In that case your d dimensional data is transformed into a one dimensional data which is nothing but projection onto the eigenvector having my maximum eigenvalue.
If we retain only two eigenvectors then the d dimensional input data will be transformed into two dimensional data.
principal components in eigen space.
And we have also seen through reconstruction with examples of reconstruction that the power of such principal components.
So, we have seen with examples that even with oneprinciple component it is possible to reconstruct most of the information present in the input data.
And there we have compared the principal component analysis PCA with autoencoders.
And, we have seen that in case of autoencoder if we do not impose any nonlinearity in the neural in the neurons in the neural network, then your principal components and the autoencoder outputs they almost coincide.
But, we have also discussed that autoencoders are much more powerful than principal components because of the presence because the neural neurons in the neural network are capable of imposing or implementing nonlinear functions.
So, as principal component analysis is a linear transformation from the input space to the eigen space, auto encoders can give you a non-linear transformation.
So, as because auto encoders can give us non-linear transformation, we can even represent non-linear manifolds using the auto encoders which is not possible using principal components.
So, we have seen that if we do not impose the nonlinearity the nonlinearity in the neurons or if the activation function of the neurons are linear then autoencoder and principal component analysis they become almost identical.
However, because of the presence of nonlinearity autoencoder gives us much better representation in lower dimensional space than in case of principal components.
So, that is what we have discussed in our previous lectures.
So, today what we are going to discuss about is how do you train a deep autoencoder.
So, we are talking about deep autoencoder training.
Then subsequently we will talk about other versions of autoencoders like sparse autoencoder, denoising autoencoder, contactive autoencoder and so on.
And after few lectures we will also talk about convolution autoencoder, but I will come to that topic after we discuss about convolution and convolution neural network.
So, let us start with how do we learn a deep autoencoder.
So, as we told before that in case of deep autoencoder and instead of having only one hidden layer, we have stacks of hidden layers placed one after another.
So, that is what is nothing but stacking of different autoencoder layers and the depth of the network depends upon how many such autoencoder layers you have in your autoencoder neural network.
So, a typical figure of a deep autoencoder is something like this.
So, here you find that we have our this input layer.
So, this is the layer which is input layer.
So, you are feeding your input data x to this input layer.
So, obviously, the number of nodes in the input layer.
we have also told that before is same as the number of elements in vector x plus 1, why that additional one?
Because we also wants to incorporate the bias term in the same layer.
So, if the dimensionality of the input vector x is d, number of nodes in the input layer will be d plus 1.
So, this is what we have discussed before.
And then you find that we have a number of autoencoders.
So, I can call this one as say the first autoencoder, autoencoder 1, this is the autoencoder 2, this is autoencoder 3 and so on.
And here in this configuration I get the coded output or the reduced dimensional representation which is also known as latent space representation at the output of autoencoder 3.
Of course, each of these autoencoders will give a reduced dimensional representation for the same input, but at different levels.
So, output of autoencoder 1 what you get here is also a coded version of input vector x, output of autoencoder 2 is also a coded version of input vector x, here this is also a coded version of the input vector x right.
So, this is where we get maximal dimensionality reduction.
And once I have this coded output, the coded output is decoded by this decoding layers, then I have a number of decoding layers to get my reconstruction.
or the reconstructedsignal x hat.
So, in case of autoencoder what we said is we try to reduce the error between x and x hat or the loss function in this case is L x x hat which we have also said that the loss function that can be defined is sum of squared error that is what is the error in the construction of x hat when you compare that with input x.
And, training of the autoencoder or deep autoencoder will try to reduce this error the sum of squared error to a minimum value.
So, you find that in case of autoencoder we have also said before that I have an encoding part.
So, this is a portion which is encoding and this is a part of the autoencoder which is decoding.
So, I have an encoding portion and I have a decoding portion.
And, these two taken together forms an autoencoder and the depth or the number of layers that you have auto that you have within this one autoencoder that tells you that what is the power of the autoencoder ok.
So, given this that as we said that for training such autoencoder I want to minimize the loss or minimize the error between the input vector x and the reconstructed vector x hat .
So, what I want ideally is the reconstructed x hat should be identical with my input vector x.
And that has to be done using the back propagation learning algorithm that we have talked about earlier.
Now, you find that the number of weight vectors or number of weight matrices or the number of elements weight elements that you have to determine by training of the autoencoder is tremendous.
So, I have weightmatrix W 1, I have weightmatrix W 2, I have weightmatrix W 3 on the encoder side on the decoder side I have weightmatrix W 3 hash, W 2 hash and W 1 hash.
And the number of weight matrices will go on increasing with the depth of the auto encoder that is the deeper the network is the number of such weight matrices will go on increasing.
So, if I try to train this entire autoencoder end to end that is given the full autoencoder architecture, I have the state of training vectors given to the input, I have x hat xhat as the output and by reducing the error between x and x hat, if I try to train this autoencoder then I have to deal with so many weight matrices simultaneously and that leads to a problem.
One is obviously, the memory problem that I have to save so many weight elements into into the memory.
The other problems are that if the weights that you have is close to the solution weights, the convergence of the learning algorithm is quite easy.
But if the weight elements are too large, then finding a global minimum becomes a difficulty.
And on the other hand, if the weight elements because initially all of them are chosen at random.
So, if the weight elements becomes too low in that case the convergence of the learning algorithm becomes very slow.
So, to avoid this problem the kind of approach for training a deep autoencoder can be that you go for a stage called pre-training which will be finally, refined with end to end training mechanism, but before going for that to reduce your complexity you go for a pre-training.
and this p training is done layer by layer that means, while pre training you deal with lesser number of weight matrices.
So, let us see that how we how it is actually done.
So, we are talking about the layer by layer p training mechanism.
So, while doing this you I take only one layer of autoencoder at a time.
So, as before I have this input layer where the input vector x is applied.
And, I take initially only one autoencoder say autoencoder layer 1 and then I put a decoder which decodes the coded output from autoencoder 1.
So, because I have only one layer this decoder output should besame as reconstructed X.
So, I call it X hat.
And, while training this first autoencoder or while trying to determine what will be the value of W 1, I you go for back propagation and this back propagation will try to minimize the error between x and x hash x and x hat and while doing so it learns the weight vectors w 1.
And once this weight vector w 1 is learned then you discard the first level of decoder that we have used and discarding this now I put a second autoencoder.
Now, before that you find that say output of the autoencoder which is an which encodes X is say a latent space vector Z 1.
So, the next level of autoencoder will try to encode this Z 1.
So, I put the second autoencoder and let me call this second autoencoder as A e 1 A e 2 sorry.
So, this A e 2 will try to encode Z 1.
And, while doing so it will try to learn the weight W 2 or weight matrix W 2.
So, to train a E 2 now what I do is I put another decoder of course, these decoders I am putting they are all intermediate finally, these decoders will not be there.
So, I put another decoder and this decoder tries to decode the output of autoencoder 2.
That means, it will try to reconstruct Z 1 which is input to input to autoencoder 2.
And, I call this Z 1 hat and for this learning the back propagation learning it will consider only the auto encoder 1, auto encoder 2 and the decoder that we have used.
And, while training this it will try to minimize the error between Z 1 and Z 1 hash and Z 1 hat.
And once that training is complete that learning is complete.
what we have learnt is the weight matrix W 2.
And once weight matrix W 2 is learnt you remove this second decoder that we have placed and there the autoencoder 2 let me assume the output of the autoencoder 2 is Z 2.
And after I got this I put the third autoencoder say A e 3 this is third autoencoder A e 3 .
So, now we have to train this third on at autoencoder that means, we have to find out that what are the weight what is the weight matrix W 3.
So, again for training this I put another decoder over here and this decoder will decode the output of autoencoder 3.
So, you find that the input to autoencoder 3 was Z 2 which was the output of autoencoder 2.
So, this last decoder that we are talking about this decoder will try to decode this Z 2 ordered by autoencoder 3 to give you Z 2 hat.
And for doing this the layers which are involved is only these layers and it will try to minimize the error or the loss function between Z 2 and Z 2 hat or L Z 2 Z 2 hat.
And once that is trained so, you find that we have the pre trained values of W 1, W 2 and W 3.
And if I assume that this is the last autoencoder layer that we had in our dipautoencoder, then the next part comes is the decoder part.
So, to set the decoder, so what I have to do is I have to again remove this autoencoder, I have to have a decoder part.
So, the decoder that you put is .
this by wrapping the encoder side and by taking the corresponding weights in the decoder side you take the transposes of the encoder weights.
So, this forms the total autoencoderor the encoder decoder pair which forms the total autoencoder.
So, here you find that W 1, W 2 and W 3 they were pre trained by using layer by layer mechanism.
layer by layer learning mechanism.
Now, I have this decoder also forming a full autoencoder and now you go for fine tuning or finer training of the weight vectors.
For that on the decoder side you initially assume that the weights are W 3 prime, W 2 prime and W 1 prime.
Now, you try to train this entire autoencoder chain using end to end training mechanism.
that is you feed your training vector x to the input, output of the autoencoder becomes x hat which is the reconstructed value of x and by back propagation learning you try to minimize the loss function between x and x hat which is L x x hat.
And here you find that because the encoder side was pre trained.
So, the values of the weight matrices W 1, W 2, W 3 are close to the actual values.
And while you go for this end to end training, now we have set of values which are very close to the actual the convergence of this training algorithm is much better than convergence of the algorithm if we try to train the entire network at a time.
So, this is what is layer by layer pre training.
So, what we have to do is after the pre training is complete, we have to introduce the decoder part and then for fine tuning of the pre trained weights I have to go for one or more iterations for end to end training giving the input as x and the output encoder output of the autoencoder x hat try to reduce the error between x and x hat using the gradient descent or back propagation learning mechanism.
So, that will ensure the convergenceto be faster and likely to attainthe global minimum of the loss function.
So, this is what isthe pre training the layer by layer pre training of autoencoder.
Now, suppose my autoencoder is trained.
So, what the autoencoder is giving me giving an input x the encoder side actually gives me say a function f of x that is my encoding function and say decoding let me call it a function g. So, this gives me decoding of f of x.
So, what is my x hat?
X hat is nothing, but g of f x.
So, the training mechanism try to minimize the error between f x and g of f x and that is how you try to find that x and x hat will be identical when the autoencoder is properly trained ok.
So, given this as we said before that we are not really interested in the reconstruction part I have input x.
do I do with x at that is not my aim, but my aim is actually the output of the auto encoder that is the latent space representation of the reduced space representation that I get from x.
And as we said that what I get at the output of the auto encoder say let me call this as output vector h which contains the stallion features present in x.
after discarding the redundancies or non salient features which are present in X.
So, H will contain the salient features and that is in the reduced dimension.
So, using H how I can go for further applications.
So, one of the applications that can be there are various applications.
So, one of the applications can be say classification.
So, what I have is I have input vector vector X ah.
So, this input vector X.
The output of the autoencoder gives me h and here this h can be fed to a classifier for classification.
It is not necessary that classifier this classifier has to be a neural network, it can be any classifier, it can be a support vector machine, it can be a base classifier or whatever which we have discussed earlier.
But let us assume that we have a neural network, we have talked about multi layer perceptron before.
So, I can feed a multi layer perceptron or MLP over here.
So, the input to MLP is H and output of the MLP is the class identification of H or eventually the class identification of the input vector X.
So, now, we find that what the autoencoder does is it gives a coded output H of X it does not give you what is.
the class belongingness of H or what is the class belongingness of X.
So, if for the training vectors the class belongingness of X is also known that is the training vectors are given in the form of X Y, where X isthe input vector and Y is the class.
Then again I can go for and end to end training fine with finer refining of the weight matrices W 1, W 2, W 3 and now this training this back propagation training will consider the output of this MLP or will consider the loss function with the output of the MLP that also we have seen before.
It can be cross entropy, it can be again sum of squared error.
So, various suchoutput error function.
So, the loss function can be defined which is defined at the output of the MLP depending upon whether the class say Y hash which is decided by this MLP taking this H that matches with Y or not.
If it matches with Y then there is no error or no training required, if it does not match then obviously, we have to train this inter neural network with that professional learning again.
And during that time also this W 1 to W 3 this encoder encoder weight matrices can be defined further.
This is one of the application.
The other application can be even I can go for say pixel classification or segmentation operation of an input image.
So, for that what I can do is suppose I have an image and input, you take various patches of this input image convert that into a vector and feed that to the input.
As we have seen before that output of the autoencoder will contain the salient features of each of these image blocks that is what the autoencoder is doing.
And then at the output again I put a classifier, what this classifier will do?
It will put this input matrix the input image or patch of the image into one of the classes.
And if that class and using that again I can form an error function or a loss function and the autoencoder can again be trained using this loss function.
And, finally, once the autoencoder is trained again now you feed an input image again feed all the different blocks of the input image to this autoencoder, the autoencoder will give a salient or the latent space representation of each of the blocks and the blocks can be classified by the trainedMLP.
So, thereby I can classify each and every block in the image.
which is nothing but a segmentation operation.
I can also consider this to be a pixel by pixel classification orthe semantic segmentation of the input image.
And for that what I can do is given and patch in the image whatever representation I am getting at the output I can consider that this is the salient features of the central pixel of this patch.
So, this is my center pixel.
So, this classifier output or the pixels in the of the input image are now classified into different class and all the pixels belonging to the same class that forms a particular segment.
So, today what we have discussed is that how you can train an autoencoder, we have talked about layer by layer training of the autoencoder and once the autoencoder is trained what can be possible applications of such autoencoder.
So, we will stop here today.
Thank you.
Hello, welcome to the NPTEL online certification course on deep learning.
In our previous lecture, we have talked about the various non-linearity functions.
In today's lecture what we are going to talk about is the neural network.
And when we talk about neural network, we will initially see that how differentlogic functions the simple functions like AND function OR function or XOR function can be implemented using the neural network.
Then, we will talk about the feed forward neural network or multi layer perceptron and we will also talk about the learning or the training mechanism of the feed forward neural network which is known as back propagation learning.
So, before we go to the neural network, let us quickly recapitulate that what are the different types of non-linearities or non-linear functions that we have discussed in our previous lecture.
So, we have talked about the very simple type of nonlinearity which is the threshold nonlinearity that is if y is a function of x, then y will be equal to 1 if x is greater than or equal to 0 and y will be equal to 0 if x is less than 0.
So, this is a simple threshold function or a nonlinear function where the threshold value is equal to 0.
I can also have a threshold where the threshold value can be non-zero maybe say I take the threshold value to be equal to 5.
So, in that case value of y will be 1 if x is greater than or equal to 5 and it will be 0 if x is less than5.
So, this is the simplest kind of non-linearity that I can have which which is a threshold function.
The other kind of non-linearity that we can have is what is known as sigmoidal function which is used in logistic regression.
So, the sigmoidal function is actually given by 1 by 1 upon e to the power minus s which is a sigmoidal function of the argument s. Now, in this case since we are talking about the classifications or machine learning techniques where we will be frequently talking about the dot product of two different vectors w and x where w is the weight vector and x is the sample vector.
Then, our argument S becomes W transpose X.
So, the sigmoidal nonlinearity or logistic regression will be given by sigma W transpose X is equal to 1 upon 1 plus e to the power minus W transpose X.
So, as you find in the right hand side, the sigmoidal function has been shown graphically and you see that at W transpose X equal to 0, the value of the sigmoidal function is half.
And, as W transpose X goes on increasing the sigmoidal function asymptotically reaches a value equal to 1 of course, it will never reach the value equal to 1, but asymptotically you can say that it reaches the value of 1.
And, as W transpose X becomes negative as it increases on the negative side or in other terms W transpose X goes on reducing on the negative side, then the sigmoidal function asymptotically reaches a value equal to 0.
So, this logistic regression actually gives an output a limit on the output where the output is limited between 0 and 1 and between 0 and 1 we have a smooth transition where at the center that is at the value of W transpose X equal to 0 the sigmoidal function passes through 0.5.
So, this is another type of nonlinearity which we will see that this can this is widely used in implementation of neutral network.
The other kind of nonlinearity is what is known as rectified linear unit or ReLU which is given by y is equal to maximum of 0 or x.
So, if x is greater than 0, then value of y is equal to x, if x is 0 or less than 0, then value of y will be equal to 0.
And the representation the graphical representation of this ReLU function is also shown on the right hand side in this figure.
So, ReLU is also a nonlinearity which is widely used in modern neural networks particularly when we talk about deep neural networks or deep learning.
So, we will come across all these different types of nonlinearity as weproceed in our discussion.
So, let us come to the neural network now.
The heart of the neural network is neuron So, when we talk about neural network, the concept of neural network is actually inspired from the way our we believe our brain works.
Of course, till now nobody has been able to say with certainty how the brain actually functions, but this is what till now what we believe how our brain actually functions.
So, in our brain we have a network of neurons and if you look at every neuron as is shown in thisfigure on .
The right hand side the neuron consists of a cell body, the cell body.
So, this is the center of this which is the cell body or the nucleus and the cell body collects information, it receives information through a number of sensors coming to the cell body which through a connectors which are known as dendrites.
Cell body processes this information and the information is outputted.
through a connection which is name known as action and action finally, it branches out and connects to other neurons through synaptic connections.
And it is believed that the information as it passes to actions and then finally, branches out and then it is passed on to other neurons in the network through a synaptic connections in this process there is a multiplicative interaction .
What is that multiplicative interaction?
If the signal outputted by the cell body is the x, then when it reaches the other neurons through this multiplicative interaction coming through the synapticconnections, the value which reaches the other neurons is W times x.
So, this is the kind of multiplicative interaction which is given in the neural network .
or in the network in the brain.
So, when you talk about neural network we will also see that the neural networks are derived from this particular concept.
So, what we have in neurons?
In neurons we have cells which receives signals through dendrites and it pass it passes these signals after processing.
to the other neurons in the network through synaptic connections.
So, the processing is done in an unit in the cell which is known as soma and action is the connecting path which transmits the signal from one neuron to another neuron.
So, this is what is the concept of a neuron in human brains.
So, when you talk about a neuron in our neural network you find that here also every neuron consists of a functional unit which is the cell body given by this unit.
This collects information x or the vector x through a number of inputs which are equivalent to dendrites.
And when these inputs are coming to the cell body they pass to a weighting function given by W or weight values given by the weight vector W .
And, the output of the neuron is of the form some function of W transpose X, where W is the weight vectors, X is the input vector and output Y of the neuron will be a function of W transpose X.
And, when you talk about neural network this function f in most of the cases is a non-linear function like the non-linear functions that we have discussed before.
So, we will come to the use of those non-linear functions in neural network.
inour discussions.
So, given this model of the neuron a neural network is nothing, but an interconnection of all those neurons.
So, here you find that in this figure what we have shown is we have a number of neurons which collects information x that is ourinformation vector or sample vector and in every level it is passed through or multiplied by an weight vector w .
So, I will have a set of weight vectors over here.
This processed information from every neuron is passed to the other neurons through say dendrites or synapses and while it passes through these dendrites or synapses while passing they are also multiplied by another set of weights or weight vectors w and it continues.
And finally, when you get the output the output of every neuron or every unit in this neural network is given by this function w transpose x.
and usually a non-linear function f of this W transpose X that is what is the output of every neuron right.
So, this is howis the architecture of a neural network looks like.
So, given this let us now see that how these neural networks can be used to implement various functions.
So, the first function which is a very simple function that we discuss is an AND function.
And the AND operation that we are going to consider which is a logical operation on two units or two inputs, the inputs are x 1 and x 2 and obviously, these inputs are binary inputs.
So, I have the input vector which is given by x 1 x 2 and my output function which is an AND function is given by y as shown in the table on the left hand side.
So, as you all know that if the input vector is 0 0, then given the function to be AND function the output is obviously 0.
If the input is 0 1, the output is also 0, if the input is 1 0, output is 0.
Only when input is 1 1 that is both the inputs both the variables on the input which are binary variables are 1, then only the output of the AND logic will be equal to 1.
So, if I consider this x 1 and x 2 which are inputs tothis AND gate to be the features or x 1 x 2 given together become a binary feature vector, then I have a feature space or a two-dimensional feature space.
So, if I plot these outputs in this two-dimensional feature space as given on the right hand side of this figure over here you see .
that, when x 1 is 1 and x 2 is 0, the output is 0 which is shown here.
If x 1 is 0 and x 2 is 1, then also output is 0.
If x 1 is 0, x 2 is 0, output is 0, only when both x 1 and x 2 they are 1, the output is 1.
So, this is how the functional values will be distributed in the feature space given by the features x 1 and x 2.
Now, I can consider this to be a classification problem that is when I am considering the input to be a binary feature vector, then I can consider the output to belong to one of the two classes or the input vectors belonging to one of the two classes.
In one class which is class 1 and the other class which is class 0.
So, all the feature vectors 0 0 0 1 and 0 they will belong to one class when the output should be equal to 0.
only when the feature vector is 1 1, it should belong to another class and output will be equal to 1.
And those distributions of the feature vectors are as shown in this plot on the right hand side.
Now, consider and now find that considering this to be a binary classification problem, I have to find out a classifying boundary or a classifier which classifies these two classes.
And, as you see that this is a linear problem as I can separate these two classes by using linear boundaries.
And, over here though there are multiple boundaries possible that is I can have this as a linear line which separates these two classes, this can also be a linear separator which is which separates these two classes .
But, one of the option is that as shown over here and you find that equation of the straight line in this two dimensional space is given by x 1 plus x 2 minus 1.5 equal to 0.
And as I said that this is one of the many possible linear boundaries that I can have between these two classes .
So, considering this now you find that I can consider this to be a feature vector where my feature vector is .
1 x 1 x 2 and I have an weight vector which is given by minus 1.511.
So, equation of this straight line in that case becomes W transpose X or X transpose W whichever way I put it because the value of W transpose X and X transpose W is same.
So, the equation of the straight line is given by W transpose X equal to 0 or X transpose W equal to 0.
And the feature vectors 0 0 0 1 and 1 0 that will fall on one side of the straight line and the feature vector 1 1 will fall on the other side of the straight line.
And incidentally if you analyze you find that this particular equation the classifier that I get this is nothing, but a two class support vector machine or a binary support vector machine.
because, it maximizes the margin.
And as I said that though there are many possible straight lines that I can draw for them the margin will be less than the margin which is given by this.
So, this is also a support vector machine.
Now, given this now let us see that how I can implement this using a neural network.
So, as I said before all the feature vectors taken together.
I can put that in the form of a matrix and we are also putting this in unified form that is I am adding in each of the feature vectors one additional element which will be equal to 1.
So, my feature vectors are 1 0 0.
So, 1 is added which is an additional element as we have shown over here.
So, this is one of the feature vector which is 1 0 0, 1 0 1 is another feature vector.
1 1 0 is another feature vector and 1 1 1 is the fourth feature vector.
And we also said, so all these feature vectors are represented are put together in the form of a matrix ok. And out of this we know that this first four feature vectors they belong to say class omega 1 for which output will be equal to 0 and for this it belongs to class omega 2 for which output will be equal to 1.
And, I also have this weight vector w which is minus 1.511.
So, given this representation representing all the feature vectors in the form of a matrix and the weight vector now how my classifier will work?
Let us see this one.
So, I can put it in the form of x transpose w where yeah I I just .
put it this way.
Here instead of writing this as x, I will write this as x transpose because whenever we talk about a vector, we usually talk the vector as a column vector.
So, this 1 0 0 which is rho over here, it is actually 1 0 0 that is a column vector.
So, instead of writing this matrix as x, let us put this as x transpose.
So, that every rho in this matrix is actually transpose of our feature vectors.
So, with this understanding you find that the way the classifier will actually work is if I compute W transpose Xsorry X transpose W where X is this matrix X transpose is this matrix and W is a weight vector which is this then the output of this multiplication this matrix multiplication is minus 1.5 then minus 0.5.
minus 0.5 and 0.5.
Now, if I pass it through a non-linearity, so you remember we said that this non-linearity is a non-linear functions are widely used in neural networks.
So, this vector that I get if I pass it through a non-linearity which is a threshold function my output becomes 0 0 0 1.
So, this threshold function is when the input is less than 0 the output should be equal to 0.
should be 0, if the input is greater than 0 then output will be 1.
So, in all these cases here it is minus 1.5 which is less than 0 obviously.
So, I will have an output 0, here here it is minus 0.5 again I will have an output 0, here it is minus 0.5 again I will have an output 0, here it is plus 0.5 which is greater than 0.
So, here I get an output equal to 1.
So, you find that this matrix multiplication followed by this threshold operations actually perform an AND operation.
which is a logical operation.
So, given this now how a neural network I can design a neuron to perform this particular task.
So, in case of neuron as it inputs the feature vectors and I say that the feature vectors are in inputted through the dendrites, one of the input I will put it as 1 because the feature vector x 1 x 2 we are converting that to 1 x 1 and x 2 .
we are adding an additional component and making that equal to 1 which is you know unified representation.
So, I have 1 over here, I have x 1 over here and I have x 2 over here which are my input vectors.
Then the weight vector I put it as minus 1.5 here it is 1 here it is 1.
So, this function of the neuron I can put it in two forms.
2parts, the first part computes W transpose X and what is this W transpose X?
W transpose X is nothing, but X 1 plus X 2 minus 1.5.
So, it becomes X 1 plus X 2 minus 1.5 and then the second part of the neuron that gives you the threshold function.
This threshold And, at the output what I get is function y is equal to f of W transpose X.
So, I can put this either in the form of W transpose X or I can also write it as W i x i, i varying from 0 to 2 and function of this.
And, in this particular case this function with these weight vectors will be an AND function.
So, this is one of the ways you find that I can implement an AND logic which I can pose as a classifier problem as a binary classifier problem with binary inputs.
So, we have two dimensional binary inputs x 1 and x 2 and that classifier can easily be implemented by a single neuron.
So, I do not need multiple number of neurons or a neural network for that purpose.
So, using a simple single neuron neuron having a threshold non-linearity.
can implement an AND logic.
In the same manner let us consider that whether I can have some other logical functions to be implemented in the same manner.
So, I consider here the other logical function which is an OR function.
Again in case of OR function I again consider the inputs to betwo-dimensional binary vectors having components x 1 and x 2.
So, my function will be if both one x 1 x 1 and x 2 then output should be 0.
So, I can 0.
In all other cases that is if the inputs areonly when x 1 and x 2 both of them are 0, then output should be 0 that is it belongs to one class.
And in all other cases that when x 1 x 2 is 0 1 or x 1 x 2 is 0 1 0 or x 1 x 2 is 1 1, then output should be equal to 1 indicating that these feature vectors belong to the other class.
Again, as before if I plot these feature vectors in the two dimensional feature space as given over here, you find that when x 1, x 2 both of them are 0 the output is 0.
In all other cases the output is 1.
Here again it becomes a linearly separable problem.
Again you can see that I can have multiple number of straight lines or infinite number of straight lines which separates these two different classes.
One of all these straight lines one of the straight lines is given by x 1 plus x 2 minus 0.5 which is equal to 0.
And here you can easily verify that both if both x 1 and x 2s are 0s then output becomes minus 0.5.
However, if any of or both x 1 and x 2 they are equal to 1 then output becomes output becomes plus 1.5.
clearly indicating that when it is 0 0 the output is negative in all other cases the output is positive ok.
So, when it when both x 1 and x 2 are 1 1 the output is 1.5, if one of them is 1 and the other one is 0 the output is 0.5.
However, in all these three cases the output is positive .
So, given this so, this becomes a simple linear classifier and when I have this simple linear classifier .
you find that a single straight line in this feature space can separate these two different classes.
How can I put it in the form of a neuron?
Can I how can I implement it in the form of a neuron?
So, here as we have shown before in the same manner I can do it as X transpose W where the matrix X is the matrix which is formed from all those two dimensional feature vectors .
W is the weight vector which indicates what is the separating planebetween the two different classes.
If I perform W transpose X, then this is the output vector that I get.
Again you pass through this threshold nonlinearity, so output becomes 0 1 1 1.
So, when my input vector is 0 0, output is 0.
When the input vector is 0 1, output is 1 1 0, again output is 1.
1 1 the output is 1.
So, this simple operation implements the OR logic.
And how do I implement it in neural network?
Again a very simple I put the input vector to be 1 x 1 x 2 and the weight vectors will be minus 0.5 1 1.
This neuron computes W transpose X .
x and I have this threshold nonlinearity which performs f of w transpose x, where this f is nothing but the threshold nonlinearity and at the output what I get is an OR function.
So, again you find that using a single neuron I can implement a OR function.
So, it has been possible to implement these logical functions using a single neuron.
because, the problem that have considered they are linearly separable problems, both and and or functions they are linearly separable.
But, if the problems becomes non separable what will be our situation and how we can solve those problems in neurons or neural networks that we will explain in our next lecture.
Thank you.
Hello, welcome to the online Certification course on Deep Learning.
So, we are discussing about the back propagation learning algorithms with some examples.
In the last class we have taken an example at the network level that is given a feed forward neural network, how to compute the error at the output node and then how do you propagate that error in the backward direction from output towards input layer.
layer.
And as you propagate the error from output layer to the input layer, how the weights in every layer areupdated.
The whole purpose of this back propagation algorithm and weight updation is that we want that output error that is the difference between the actual output that you get from the network and your target output that should be minimized.
So, in the previous class we have discussed this with respect to .
a network in the network level.
Today's lecture we will see that the same back propagation or the gradient propagation is applicable within a node itself because every element in a neural network is nothing, but a neuron or a node and within the node as the complexity of the node increases the node may also consist of a number of layers.
It is not necessarily that every node will compute in a single layer fashion.
So, I can have multiple layers within a node and today we will see that how this gradient can flow within a node from output of the node to the input of the node.
So, before we go into that let us just try to recapitulate what we have done in the previous class.
So, in the previous class we had taken a simple two layer neural network.
So, as is shown over here.
So, this neural network has .
an output layer having only two nodes and we also had an input layer which is also having two nodes that is node 1 and node 2.
In addition we had another node which gives you the bias term and we had an input layer.
So, this was the hidden layer and we had an input layer again consisting of two nodes because we had two dimensional feature vectors x 1 x 2, but the number of nodes in the in the input layer was also 3 because we wanted to augment these input vectors by an additional term by an additional component which is equal to 1 which takes care of the bias in every node.
And there we have seen that the error that you compute at the output.
So, we had an error energy which was given by half of say your actual output x g .
And, because this was a second layer, so we put it as x j 2 minus T j take the square of this and sum of this for j is equal to 1 to 2 .
So, that is the sum of squared error that you compute at the output.
And, you take the gradient of this with respect to the weights weights between the hidden layer and the output layer.
And, accordingly by gradient descent algorithm you go on updating the weights of thishidden layer to the output layer.
Then what we had seen is that whatever gradient that you get over here you remember one thing that we discussed about because this network is a multi layer network.
So, if my input is x I have a function say f 1 which works on x along with an weight matrix or set of weight values which are say w 1.
And this output this particular function is inputted to the next layer which computes say f 2 along with the corresponding weights w 2.
This again in turn inputted to the next layer.
which computes say f 3 along with the corresponding weights w 3 and this continues this goes on.
And this is the final output that we get assuming that it is a three layer network.
So, the output comes from the third layer which is f 3.
So, when I compute the error the error e is computed with respect to f 3 and our target output that is the target that we want to have.
So, here you find that this e or the error is actually available to f 3 or f 3 which is a function of let us call this argument as theta 3.
So, actually error is visible to theta 3, but error is not visible to which was output of the input layer which is a theta 1.
Similarly, this one I call as theta 2.
So, error is actually visible to theta 3, it is not visible to theta 1.
So, if I want to compute the gradient of error with respect to w 1, I cannot compute it directly.
So, what I have to do is, I have to compute the error with respect to theta 3gradient of error with respect to theta 3, then multiply that with the gradient of theta 3 with respect to theta 2, multiply that with the gradient of theta 2 with respect to theta 1, and multiply that with the gradient of theta 1 with respect to w. So, what I have to compute is if I want to compute del E.
say del W 1 let us put del W 1 ij because the W 1 over here is a matrix.
So, this will be del E del theta 3 into del E del theta 2 into sorry del theta 3 del theta 2 into del theta 2 .
1 into del theta 1 del W i j 1 .
So, here you find that this product that is del theta 3 del e del theta 3 into del theta 3 del theta 2 into del theta 2 del del theta 1, these are the gradients which are being back propagated from the output layer to this input layer.
And del theta 1.
del W ij 1 that is what is the local gradient.
So, to find out the gradient of the error with respect to W ij 1, I have two components.
One component is the propagated term which is this and the other component is the local gradient.
So, what we are doing is we are multiplying the propagated gradient with the local gradient to get the actual gradient.
ok.
So, this is the same concept which has been applied over here that whatever was gradient at the output the same gradient when it comes over here it is multiplied with the local gradient to find out how the error is propagated to the input side.
And you remember one more thing that every node in the hidden layer is feeding input to every node in the next layer and that input is fed through a corresponding weight value.
So, when we are propagating error or the gradient in the backward direction, the gradient which is propagated from here through this and the gradient which is propagated through this that will be weighted by the corresponding weight values and added over here.
And this summation is or the aggregate of the gradient which is collected at this hidden layer node, this is the total gradient which is available here through back propagation.
And this total gradient now has to be multiplied with the local gradient to find out what is the gradient of the output on this input weight.
And accordingly I have to adjust theseweight components and that is what we have discussed in our previous class.
So, now let us see that how the same concept x y into z.
So, this is a function that this node computes.
So, if I break or if I expand the particular node here in a hierarchical manner on or in a staged manner, how this node actually computes this function.
So, that every function is broken into a smaller primitive set of functions.
So, what I do is you find that there are a few hierarchical levels.
So, in the first level you compute x into y through a multiplier block.
So, this is a multiplier block which computes x into y, then I have an addition block which computes.
So, this intermediate variable x into y that is assigned to an intermediate variable say v, then I have a an adder block, f, which computes w plus x y or w plus v because v is the intermediate variable which is equal to x into y.
So, this w plus x y is assigned to another intermediate variable u and then finally, I compute f which is nothing but u into z or which is nothing but u into w plus x into y.
So, this is my total node structure and as we have discussed before.
that every back propagation learning algorithm is preceded by a forward pass.
The forward pass in which the functional value is computed or coming to a network layer given an input vector and whatever be the set of weights at different layers available at that point of time using that you go for a feed forward pass and in the feed forward pass you compute what is the output vector that you get.
And, this if this output vector matches with the target then I do not have any error.
So, that means, I do not have to go for back propagation, I do not have to I do not have anything to be adjusted because whatever output we get that is my desired output or the target output.
But in case of an error when the output does not match with the target then I have an error and the gradient of the error has to be back propagated for weight adjustment.
So, here I have to have a forward pass that means, whatever be the value of x y z and w using those values I have to compute what will be the value of f. And then if necessary that means, if the f that you get is not the desired value then I have an error and the gradient of that error has to be propagated in the backward direction for updation wherever it is required.
So, now let us see how this forward pass actually works.
So, to see how the forward pass works let us assign values to w x y and z.
So, we assume w has a value 5, x has a value 3, y has a value 2 and z has a value w z has a value 4.
So, accordingly you will have v which is equal to x into y.
that that gets a value 6, then u which is w plus v or in other words it is w plus x into y if you compute this it will be 11.
And then finally, f which is u into z or it is x w plus x y into z that will assume a value 44.
So, in the forward pass I get the output to be to have a value 44 right.
Now, let us see how the backward pass will work on this.
So, to work on backward pass see what I want to find out is what is the sensitivity of output f sensitivity of f on W, I want to compute what is what is the sensitivity of f on X what is the sensitivity of f on y, what is the sensitivity of f on z or in other words if I need any correction on f suppose my desired value of f was something like 33 whereas, I am getting a value of f which is 44 that means, value of f has to be reduced.
So, in order to reduce the value of f.
valuey.
Whether I should increase value of W or I should also reduce value of W?, whether I should increase value of X or I should reduce value of X?
Whether I should increase value of Y or I should reduce value of Y?
Whether I should increase value of Z or I should reduce value of Z?
that is what is the sensitivity of f on W or on X or on y or on z and this is what I need.
I do not need how f varies with u or how f varies with v that is not my concern, but my concern is how f varies with w x y and z .
But I cannot directly compute del f del w or del f del x the reason being f is not directly visible to w, f is not directly visible to x, f is not visible directly visible to y .
But, yes I can compute del f del z because f is directly visible to z.
So, now, let us see that how this gradient in this case in the backward direction can be computed again using the chain rule.
So, for doing this I assume say what the gradient that you get at the output is say 1, then assuming this how it is propagated.
f del f del w. And as we said because f is not directly visible to w, so I cannot really compute del f w del f del w directly.
So, for doing this I have to make use of the chain rule.
So, I have to make use of what is del f del u in this case that has to be multiplied by what is del u del.
w. So, this is what I was saying that del f del u is the back propagated gradient up to this point and del u del w is the local gradient.
So, you multiply the back propagated gradient with the local gradient to get what is del f del w. Now similarly over here I have to compute del f del v, but I cannot compute it directly because f is not .
to V. So, what I will compute is I will compute del f del u and then multiply with del u del v .
So, that is what gives me the sensitivity of V sensitivity of f with respect to V or the gradient of f with respect to V. Again over here when I compute del f del x, first I have to compute what is f del v and then I have to compute what is del v del x .
Similarly, over here I have to compute to in order to find out del f del y I have to compute what is del f del v and then multiply that with del v del y right.
So, here you find that .
your del f del u is equal to z and what is del u del w you come over here del u del w is equal to 1 right.
So, this will be simply the value of del f del u which is nothing but z.
Come over here what is del f del v?
Del f del v is del f del u into del u del v and what is del f del u?
del u is equal to z and what is del u del v right del u del v here you find that del u del v is again equal to 1.
So, this value will simply be equal to z again.
Coming over here del f del x equal to del f del v into del v del x and del v del x is nothing but y.
So, del f del x will be y into z.
So, this will be equal to y into z.
Similarly here as del f del v is equal to z and del v del y del v del y is equal to x.
So, this will be x into z .
So, you find that finally, following this back propagation and the chain rule I find I can find out what is del f del x, I can find out what is del f del y, I can find out what is del f del z, I can also find out what is del f del w .
So, let us see that how we can implement this in modern programming languages.
I will just put this because this is the kind of structureor the constructs which are given by modern languages meant for deep learning or machine learning implementations.
So, first what you do is you set the input values.
So, you find .
values we had set earlier was W is equal to 5 equal to x equal to 3, y is equal to 2 and z equal to 4.
Then what we have done is we have computed the forward pass.
In the forward pass V is equal to x into y, then u is equal to W plus V and then f is equal to u plus z.
So, this is what we are computing in the forward pass and then you have backward pass.
So, in the backward pass as we as we have .
seen earlier that you see what we have done in the backward pass.
Firstly, we have to compute del f del u, I have to compute del f del z and then following the chain rule finally I have to get what is del f del w, what is del f del x, del f del y and so on.
So, in the same manner here you find out what is del f del u which is nothing but z and del f del z which becomes equal to .
f del v into del v del x, where del v del x is equal to y right.
So, you simply come over here you can look over here that del v del x is equal to y.
So, that gives me del f del x or d f d x is equal to y times d f d v and in the same manner d f d y becomes x times d f d v. So, after this .
Now, we are ready with the sensitivity computation of sensitivity of the output with respect to the inputs to the node ok. And the kind of graph that we have computed this is what is known as computational graph.
So, all these which are shown in red over here they tell you that what is the sensitivity of f with respect to z.
what is the sensitivity of f with respect to w, the sensitivity of f with respect to x and sensitivity of output with respect to y.
So, how does it help?
Why we are doing all this?
One is we know that given a complicated function how it can be broken into simpler functions to give you a complete node function.
And then how we can write the back propagation algorithm as well as the feed forward algorithms using the modern .
deep learning languages programming deep learning tools that we have right.
So, this is simply same computation with the examples that we have shown earlier.
So, here you find that your del f del w actually tells you that how f varies with w. So, if I increment w by say small amount say for example, 0.001.
So, we had fed input w to be 5, now we make w to be 5.005.
And, as a result you find that f has increased in initially value of f was 44, now f has increased to 44.004 that says that by whichever amount we have incremented w f has been incremented by 4 times of that.
That means, if I want to reduce w by varying reduce f by varying w then w has to be incremented and the same thing becomes true.
with all x y and z.
And this graph is what is known as a computational graph and now you find that what is the use of the computational graph or how the computational graph helps you in writing such back propagational problems right.
We have few more examples in the same manner, but I tell you that simple some simples that circuits elementary circuits.
using which you form or you design the complicated nodes, one is add, one is multiply and otherwise max .
So, in the back propagation the multiply operation as we have seen before that if the output is say product x into y, then in the back propagation if the gradient up to this output is g, what you pass to the side of x is actually g into y .
and what you pass to the side of y is actually g into x.
Similarly, if it is summation node, then whatever is the gradient at the output side, it is simply copied to both the inputs.
Similarly, if it is max, then whatever is the gradient at the output, the gradient is simply passed to the node of the variable which has maximum value.
So, if x was maximum of the two x and y, gradient g was passed towards x.
If y was maximum, then gradient g would have to be maximum.
passed to y.
And using this we can havewe can represent complicatednode functions using simpler partial differentiation or gradient operations.
And that helps us to writethe functions in a much simpler way and that helps ultimately helps us to code the complicated functions.
I will So, this is the same thing I will just skip thatthis is again an example using that multiply add and max operation and we hadyou can easily verify that how the forward passinformation is propagated from input to output to get the final output and then in the backward pass we can compute the gradients following the same gradient descent procedure.
And, I want to emphasize one more point over here that in every case the gradient computation at a node is highly local in nature.
As long as your gradient till the previous nodeprevious layer is computed and that is fed to the current layer, in the current layer you can make use of the gradient which has been propagated to current layer and all the computations in the current layer are highly parallel in nature.
The computation of gradient of one node in the current layer is independent of the gradient computation in the other node.
So, as a result this entire function can be implemented in a parallel machine.
If I have a parallel machine then all the gradient computations or the back propagation learning can be implemented using parallel operations.
So, I will stop here today and as we said now onwards I will not consider the gradient learning anymore or the gradient propagation or back propagation learning anymore.
For any application subsequently I will simply use the term that the machine is turned is learnt.
or trained using back propagation algorithm ok.
Thank you.
Hello, welcome to the NPTEL online certification course on deep learning.
You remember in the previous class we were discussing about the back propagation learning with respect to some examples.
So, what I wanted to do is I wanted to explain how the back propagation learning algorithm works with the help of example to have a clear understanding of the back propagation learning and the point till which we completed was this that when you go for back propagation of the error at the output layer that means, now I want to update the weight vectors connecting the nodes from the hidden layer to the output layer.
The example that we are considering is a two layer neural network or two layer feed forward neural network.
So, when you go for updation of the connection weights as the error can be computed only at the output.
I cannot compute an error at the output of the any of the hidden layer nodes because I do not know what are the targets or what are the target vectors at the outputs of the hidden layer nodes.
But given the class belongingness of the training vector I have what is or I know what should be my target output.
So, any deviation of the output that you get from the network from the target output that is my error.
So, the back propagation learning algorithm tries to adjust the weight vectors, so that the error is reduced or the error is minimized and for that you start from the output layer node.
So, in the previous class what we have discussed is that how do you adjust the weights at the output layer nodes using .
the error information or by back propagating the derivative of the error information with respect to the weightvectors or the weight components from the hidden layer node to the output layer node.
So, for that we had defined these various terms as is shown in this particular diagram .
Now, you also remember that in the forward pass we have computed the outputs of every node in the hidden layer .
layer, we have also computed the outputs of every node in the output layer.
In fact, that is from there only I know what is my error.
So, the outputs of the nodes in the hidden layer are as given here that x 1 1 is 0.92.
So, this is x 1 1.
1 1 which is the output of the hidden layer node.
And similarly x 2 1 which is output of the second node in the hidden layer is 0.27.
And I said that I cannot compute the error in the hidden layer because though I know from the forward pass that what are my outputs, but I do not know what are the targets right.
So, I cannot compute error in the hidden layer.
And in the same manner I have computed output of the output layer nodes.
So, this is my x 1 2 that is the output of the first node in the output layer which is 0.44 and 0.95 is x 2 2 which is the output of the second node in the output layer.
And we also said that the vector 0.7, 0.1, 0.2 that we considered this vector is taken from category 1.
So, as a result my target is nothing but 1, 0.
This is my target vector, but this is the actual vector that I got.
So, the difference between the target vector and the actual vector that gives me an error.
And by using the backward learning mechanism we have to reduce or minimize this error.
That means, this output should be as close as possible to the target output and that is what we are doing using the backward learning algorithm.
So, now let us see.
So, as we have seen before that.
how this backward learning algorithms.
So, I need to compute these terms del J 2 that is for J equal to 1 and 2.
So, I have to compute del 1 2 x 1 1.
So, for various combinations of J and I, I have to compute this derivative of E with respect to the connection weights.
And for this.
j I compute delta 1 2.
So, delta 1 2 is nothing, but x 1 2 into 1 minus x 1 2 into x 1 2 minus t 1.
And as we have just seen that what is my x 1 2, x 1 2 is 0.44 as is obvious from here, x 1 2.
is 0.44 and x 2 2 is 0.95.
So, by using this I go for this computation that delta 1 2 is equal to x 1 2 into 1 minus x 1 2 into x 1 2 minus t 1, x 1 2 is 0.44 and t 1 is 1 because the input vector that I am considering I am considering that to belong to plus 1.
So, t 1 equal to 1.
So, as a result when you compute this delta 1 2 it simply becomes minus 1.38 just this computation will give you.
And in the same manner when I compute delta 2 2 which is x 2 2 into 1 minus x 2 2 into x 2 minus t 2.
So, you remember that my x 2 2 was 0.95 and t 2 is equal to 0 because the input vector belongs to class.
1.
So, my T 1 is 1 in the target vector and T 2 is 0 in the target vector.
So, accordingly as X 2 2 is equal to 0.95, when I compute delta 2 2 delta 2 2 comes out to be 0.045.
So, simply by gradient descent algorithm the updated delta 1 1 2 is nothing, but delta 1 1 2 .
minusthe updated W 1 1 2 is nothing, but W 1 1 2 minus some eta times del E del W 1 1 2.
And as I have computed delta 1 2 as minus 0.138.
So, del E del W 1 1 2 it is simply delta 1 2 into x 1 1.
1 that is the output of the first node in the hidden layer that is x 1 1.
So, del E del W 1 1 2 which is delta 1 2 into x 1 1 which is nothing but minus 0.126.
So, what is x 1 1?
Again x 1 1 we have computed in the forward pass as you see from here that x 1 1 is nothing but .
this value 0.92 and x 2 1 is 0.27.
So, using these values I am computing these quantities.
So, this del E del W 1 1 2 is delta 1 2 delta 1 2 is minus 0.138 multiplied by x 1 1 that gives you minus 0.126.
And in the same manner e del W 1 2 2 which is delta 2 2 times X 1 1 which becomes 0.04.
So, as a result the updated weights that is W 1 1 2 which is the connection weight from the first node in the hidden layer to the first node in the output layer that simply becomes W 1 1 2 plus eta which is a hyper parameter indicating the learning rate.
eta times 0.126.
You find that del E del W 1 1 2 was negative.
My learning algorithm the or updation rule is actually W 1 1 2 minus eta times del E del W 1 1 2 as W 1 1 2 is negative.
So, this del E del W 1 1 2 absolute value of that is being added to W 1 1 2.
Similarly, W 1 2 2 which is the connection weight from the first node in the hidden layer to the second load of the output layer that becomes W 1 2 2 minus eta times 0.04.
Now, you note an interesting point over here that is after updation W 1 1 which is the connection weight from first node in the hidden layer to the first node in the output layer that is being increased.
Whereas, the connection weight from the first node in the hidden layer to the second node in the output layer is being decreased.
And you remember that my target vector was 1 0, whereas the output that actually I have obtained after this forward pass is 0.44 and 0.95.
So, that clearly says that I should update the weights in such a way that the first component 1 2 is the connection weight from the same hidden layer node which is the first node in the hidden layer to the second node in the output layer.
So, if I increase W 1 1 then effectively I am adding more values to the output of W 1 to the output of the first node in the output layer.
And if I reduce W 1 2 as this is waiting at this as this is multi being multiplied with the output of the first node in the hidden layer.
So, the effective contribution.
as the weight is reduced effective contribution to the output of the second node ofthe output layer that is being reduced.
So, effectively what we are trying to do is we are trying to increase by doing this we are trying to increase this value this is we are trying to increase and this value we are trying to reduce that means, we are trying to move more our target vectors.
So, this is how it is done in the output layer.
Similarly, in the same manner I can compute what is W 2 1 or the updation of W 2 1 2 that means, the connection weight from the second node second node of the hidden layer to the first node in the output layer this is what is .
W 2 1 2 and also W 2 2 2 which is the connection weight from the second node in the hidden layer to the second node in the output layer.
So, in the same manner by computing this delta 1 2 I can and delta 2 2 and making use of the outputs of the previous layer nodes or hidden layer nodes I can compute what is del E.
W 2 1 2, I can also compute what is del E del W 2 2 2.
And here again you find that del E del W 2 1 2 comes out to be negative which is minus 0.037.
And also del E del W 2 2 2 that is the connection weight from the second node in the hidden layer to the second load to the output layer that is also positive.
So, the connection weight to the first node in the output layer is negative, connection weight to the second node of the output layer is positive.
That means, when you go for weight updation W 212 will be incremented by a factor by a value which is eta times 0.037.
This is the amount which will be added to W 212.
And similarly, when you update W 222, you eta times 0.012 will be subtracted from W 2 2 2.
That means, the value of W 2 2 2 will be reduced whereas, value of W 2 1 2 will be increased.
So, this is a vector which will be increased whereas, this is a vector which will be reduced.
So, again as we have explained before that this increment of W 2 1.
And, reduction of W 2 2 tries to make the output of the first node higher and the output of the second node lower and that is what we should do to reach our target vector which is 1 0.
In the same manner the bias term bias is nothing but W 0 1 2 which is the bias of the first node in the second layer.
or in the output layer and W 0 2 2 this is the bias of the second node in the output layer.
So, in the same manner you find that del E del W 0 1 2 that is also negative that is minus 1.38 and del E del W 0 2 2 that is positive which is 0.045.
So, all these values will come based on .
the computations that we have done in the forward pass.
So, this also says the bias term that is W 0 1 once it is updated, it will be incremented by an amount eta times 1.38.
Whereas, W 0 2 2 will be decremented by amount eta times 0.45.
So, this again tries to increase the output of the first node of the output layer and tries to reduce the output of the second node of the output layer because W 0 1 is being incremented by eta times 1.38 whereas, W 0 2 is reduced by 0.045 ok.
So, this is how the nodes in the or the weights in the output layer will be updated.
And while doing so we have propagated the effect of error computed at the output in the backward direction.
Now, this error has to be propagated further inside to update the weights from the input layer to the hidden layer.
And when you do that you find that coming to a particular node you are propagating the weight from the first node you are also propagating the weight from the second node.
So, these twopropagating the error from the first node and propagating error from the second node.
So, these two error terms are to be added together to consider what is the total back propagated error term at every node in the hidden layers.
So, for that again we have seen before that how this is done we had defined an additional term if you remember that when we discussed about the back propagation algorithm that for every kth layer node we have defined an back propagated error term which is given by delta i k.
i k which was o i k sorry in this particular case our we are considering o i to be x i.
So, please read this as o i has to be x i o i k has to be x i k. So, please read this as x i k. So, my delta i k will be x i k into 1 minus x i k into sum of delta j k plus 1 W ij k plus 1.
So, you remember that this W ij k plus 1 is the connection weight from the ith node in the kth layer to the jth node in the k plus 1 layer.
And this delta j k plus 1 is the error term which is propagated up to the jth node of the k plus 1 layer.
So, this is being weighted by the corresponding weight and you are adding them together.
So, to consider to find out what is the propagated error at the ith node.
And for doing this all the nodes to which the ith node has fed the input that means, all the j s from all of them now you are accumulating the error by weighting them by the corresponding connection weight and adding them together.
And then multiplying that with a local derivative, local derivative of the output of the ith node.
which is x i k into 1 minus x i k right.
So, once you have this you define it with respect to k. Now, this k in our case because we are considering a two layer network the value of k is equal to 1.
So, what we have to find out is what is delta i 1 and as per this definition this delta i 1 is nothing, but x i 1 into 1 minus x i 1.
So, this x i 1 is the output of the ith node in the hidden layer.
So, this x i 1 into 1 minus x i 1 into sum of all the back propagated errors from all the nodes to which the ith node has fed the input.
So, that is what gives us delta i 1.
So, given this and all the pre computed values all the computed values that we have computed during the forward pass, I can find out what is delta 1 1.
1, because I know what is x 1 1 from the forward pass.
I know what is delta 1 1 2, I know what is delta 1 1 1 2 2, because that is what we had initialized or in any intermediate stage in any step of our iteration these are the updated weights those are also known.
I also know what is delta 1 2 that is the back propagated error from the next node, the nodes in the next layers.
Similarly, I also know what is delta 2 2.
1 that is the what is the back propagated error up to again unknown in the next layer.
So, if you compute this you will find that delta 1 1 1 delta 1 1 that comes out to be 0.24.
And in the same manner you will find that delta 2 1 that comes out to be minus 0.02 right.
So, these are the back propagated errors.
to the first node in the hidden layer and to the second node in the hidden layer.
So, this is the back propagated error to the first node in the hidden layer and this is the back propagated error to the second node in the hidden layer.
So, using this now I can go for updation of the weight vectors from the input layer nodes to the hidden layer nodes.
So, how do I do it?
by using the same expressions your del E del W 1 1 1.
What is del W 1 1 1?
It is the connection weight from the first node, the connection weight from the first node in the input layer to the first node in the hidden layer.
So, that is W 1 1 1.
So, this is the one.
So, I can compute del E del W 1 1 1 as delta 1 1.
times x 1 0, what is x 1 0?
x 1 0 is supposed to be the output of the first node in the 0th layer.
And in our case in this case the 0th layer is the input layer and what we said earlier is that every node in the input layer gives you an identity function.
That means, whatever is the input it simply passes that to the output.
So, this x 1 0 is nothing but the first component of the input vector which is x 1 because that is the input vector the first component of the input vector.
So, this first component of the input vector if you remember we considered that to be 0.7.
So, my input vector was 0.7 and 1.2 that was the input vector.
So, using this I can compute what is del E del W 1 1 1, I can compute what is del E del W 2 1 1 which comes out to be minus 0.014.
I can compute what is del E del W 2 1 1 which comes out to be 0.288, I can compute del E del W 2 2 1 which comes out to be minus 0.24, I can compute what is del E del W 2 011, now del 011 del E del W 011, W 011 if you remember it is representing the bias of the first node in the input layer.
So, which comes out to be again 0.24, I can also compute del E del W 021, W 021 is the bias to the second node.
So, I can also compute del E del W 021 which comes out to be minus 0.02 .
And, once once I compute all these my updation rule weight updation rule is as before del ij 1 getsW ij 1 simply gets W ij 1 minus eta times del E del W ij 1 right.
So, you find that by forwarding the backward direction by back propagation of the error terms from the output layer.
layer, I can update the weight vectors even in between the hidden layers and this can proceed further.
So, instead of two layer network if I take a three layer network again the error will be propagated from the second hidden layer to the first hidden layer and it can be used in a similar similar manner for multi layer networks.
So, here you find that what we have done we have just illustrated with the help of examples that in the network layer.
at the network level how the back propagation algorithm is can be implemented to update the weight vectors.
And, you find that when you use this back propagation algorithm the values that you are using at every layer are actually computed in the forward layeractually computed in the forward pass.
So, it will be wise if you save all the values that you have computed in the forward pass for reducing the computation.
during the back propagation learning work.
And each of these works that you have done that independent every node is independent of the other nodes.
So, it is highly parallelizable.
So, I will stop here todaywe will continue with these discussions even at the node level in our next lectures.
Thank you.
Hello, welcome to the NPTEL online certification course on deep learning.
You remember that in the previous class we had started our discussion on discriminant function and the boundary between different classes.
So, in the previous class we talked about the discriminant function under multivariate normal distribution.
And in today's class we are going to continue with our previous discussion and we will talk about we will see that how the decision boundary.
under different classes under various conditions of the covariance matrix we can have.
So, we can have linear boundaries, we can also have non-linear boundaries or quadratic boundaries and this will also illustrate with the help of some examples.
So, let us justtry to start with what we had done in our previous class.
So, in the previous class we have said that the multivariate normal distribution is given by P of x as 1 over 2 pi to the power d by 2 then sigma determinant to the power half and then exponential minus half x minus mu transpose sigma inverse into x minus mu .
So, this was the normal distribution under case, where this covariance matrix sigma is nothing, but x minus mu into x minus mu transpose and the expectation value of this that is what is the covariance matrix and mu is the mean of all the samples all the vectors that we have.
So, now if I want to have represent this as class conditional probability density function multivariate normal density function.
Then, what I have to do is I have to get p of x given omega i which will be 1 upon 2 pi to the power d by 2 and as we said in our previous class that d is the dimensionality of the vector.
And then sigma now becomes sigma i because it is the covariance matrix of all the samples taken from class omega i.
And, the other part remains exponential minus half x minus now mu becomes mu i because this is mean of the vectors taken from class omega i sigma becomes sigma i.
So, this is sigma i inverse this of course, transpose into x minus mu i.
So, this is the class conditional normaldistribution.
So, using this we have in our previous class found out the discriminant function for the ith class or the discriminant function was obtained as g i x that is the discriminant function for the ith class which was w i transpose x plus w i naught where what is this w i?
We had seen that this w i is nothing, but 1 upon sigma square intomu i and w i naught was minus 1 upon 2 sigma square mu i transpose.
mu i plus log of p of omega i.
And this is the expression we have obtained under the assumption that sigma i that is the covariance matrix for all the classes is same and which is of the form sigma square i which indicates that the covariance matrix is a diagonal matrix where all the off diagonal limits are 0.
and all the diagonal elements of the covariance matrix is same as sigma square.
What does it physically mean?
It physically means that the different components of the feature vector that we have those components are statistically independent and every component has the same variance that is the variance of x 1 the first component is same as the variance of x 2 which is the second component like this and every component.
the variance is same which is nothing, but sigma square.
This indicates that the way the training vectors the vectors are distributed in the feature space is like a circle in case of two dimension, it is a sphere in three dimension and it is hypersphere in multi dimension.
Where of course, at the center the density is maximum and as you move away from the center the density goes on reducing.
So, this is the physical significance of this short of distribution where sigma i is of the form sigma square i where this i is nothing but our unity matrix .
So, under this situation we have also stated in the previous class that if I have feature vectors descriptors coming from two different classes the i th class and j th class.
I can find out g i x that is the discriminant function for the i th class.
I can also find out g j x which is the discriminant function corresponding to the j th class.
And given this if I want to find out what is the boundary the decision boundary between these two classes that is we say that if the feature vectors falls on one side of the boundary it belongs to one class and it and if it belongs to falls on the other side of the boundary.
it belongs to some other class.
So, I can find out an expression of the boundary which separates these two classes.
And the expression of the boundary will be simply given by g x which is nothing but g i x minus g j x and because on the boundary g i x and g j x both of them will be equal.
So, this has to be equal to .
So, our expression will be g x is equal to g i x minus g j x which will be equal to 0 and this just from the expression of g i x that we have obtained you can find that g i x minus g j x this will be simply given as 1 upon sigma square into mu i minus g i mu j transpose x minus 1 upon sigma square mu i transpose mu i minus mu j transpose mu j plus log of p omega i upon p omega j that will be equal to 0.
This can simply be put in the form mu i minus mu j transpose X minus this there has to be a half right half of mu i minus mu j transpose into mu i plus mu j plus sigma square log of p omega i upon p omega j that has to be equal to 0 and after simplification you can find that this expressions can simply be written as W transpose X minus X naught that has to be equal to 0, where this W will be simply mu i minus mu j and X naught will be given by half of mu i plus mu j .
j minus sigma square upon mu i minus mu j mod square log of p omega i upon p omega j into mu i minus mu j.
So, you see what is the significance of this particular expression?
It says the equation of the boundary between the two classes i th class and j th class is given by W transpose X minus X naught equal to 0, which simply indicates that this boundary is a linear boundary.
In case of three dimension it is a plane, in case of multi dimension it is a hyper plane.
For the vector W is given by mu minus mu i minus mu j, which is nothing but a vector drawn from mu i to mu j, where mu i is the mean of the vectors taken from class omega i and mu j is the mean of the vectors taken from class omega j.
And the expression for x naught and it says that because W transpose x minus x naught equal to 0.
So, the decision surface is obviously perpendicular orthogonal to the vector W and as W is the vector drawn from omega i to omega j.
So, the decision surface is orthogonal to the line joining mu i and mu j and it passes to the point x naught and here if you look at the expression for x naught and particularly under the case if I consider that mu p of omega i is equal to p of omega j if I use if I consider this condition that is both the classes omega i and omega j.
they are equally probable the afterey probability is same under this case log of p omega i upon p omega j will be equal to 0.
So, this term will be equal to 0 and in which case your x naught simply becomes half of mu i plus mu j.
That means, x naught is a point which is at the middle of the vector the line joining mu i and mu j.
So, under such circumstances where the a priori probabilities of the two classes are same your decision surface becomes an orthogonal bisector of the line joining mu i and mu j.
And obviously, the kind of decision that you take in this case that given an unknown vector whether the vector should belong to class omega i or the vectors should belong to class omega j that decision will be taken by simply taking the distance.
of that unknown vector from the mean vectors mu i and mu j.
So, whichever mean is nearest to this unknown vector x the unknown vector will be classified to that corresponding class.
And obviously, if p of omega i and p of omega j they are different that is a priori probabilities are different that will give a bias in the decision.
So, if a priori probability for omega i is greater than a priori probability for omega j.
Then, your decision surface though it will be orthogonal to the line joining mu i and mu j, but it will be shifted towards mu j indicating that more space is allocated to class omega i.
Similarly, if mu j p of omega j is greater than p of omega i in that case x naught will be shifted towards mu i indicating that more of space will be allocated to class omega i.
in which case your decision will be biased in favor of classbiased in favor of class omega j.
So, now let us try to see an example that how this really works .
So, to illustrate this I take a set of points both from class omega 1 and class omega 2.
So, first I take a point say 12 4.
So, I am considering a two dimensional feature space.
and the features are say x 1 and x 2.
So, first I consider a point 12 4, then I consider a point 12 8, then I consider a point 10 6 and say 14 6 and I assume that these are the points which are taken from class omega 1.
Similarly, I also consider another set of points say 9 10, 9 14, 7 12 and 11 12.
And, I consider these points to be taken from class omega 2.
And, these are the points we call as training samples because using these feature vectors using these feature vectors I am going to train my classifier and that is where what is the learning in this particular case.
So, I have a set of points from one class omega 1 and another set of points from class omega 2.
Now, given this set of points first I consider the points taken from class omega 1, I compute the mean vector mu 1 which is simply average or mean of all these vectors taken from class omega 1 and the that mean vector comes out to be 12 6.
And once I have this mean vector then I have to compute the covariance matrix and as we said that covariance matrix is nothing but the expectation value of X minus mu.
into X minus mu transpose.
So, if I consider the first vector I call it say X 1 that is vector 12 4, I subtract the mean 12 6 from this.
So, 12 4 minus 12 6 simply becomes 0 minus 2.
So, a part of the partial covariance matrix simply X minus mu 1 into X minus mu 1 transpose in this case as is shown here.
x minus mu 1 into x minus mu 1 transpose simply becomes 0 0 0 4.
So, this 0 minus 2 as column vector is my x minus mu 1 and 0 minus 2 as row vector is x minus mu 2.
In the same manner I also compute x 2 minus mu 1 into x 2 minus mu 1 transpose which is M 2 that again comes out to be 0 0 0 4.
I also compute X t minus mu 1 into X t minus mu 2 transpose mu 1 transpose and that comes out to be 4 0 0 0.
Similarly, M 4 which is X 4 minus mu 1 into X 4 minus mu 1 transpose and that comes out to beagain 4 0 0 0.
So, once I have these four matrices, now the covariance matrix is nothing but the mean of all these four matrices.
So, the covariance matrix from class for class omega 1 which is sigma 1 is simply 1 upon 4 into M 1 plus M 2 plus M 3 plus M 4.
And if you compute this, it will simply come out to be 2 0 0 2.
which is nothing butmatrix of the form 2 into i where this i is an unity matrix.
So, this is what I get for class omega 1.
Similarly, for class omega 2 I consider all those feature vectors that have taken from class omega 2 which you remember that the feature vectors were 9 10, 9 14, 7 12 and 11 12.
So, using this 4 feature vectors, I compute the mean of the vectors belonging to class omega 2 and which in this case comes out to be 9, 12.
And once I have this mean vectors in the same manner as I have done previously I am not repeating all those calculations, you can calculate the same way and you can find out that the covariance matrix for class omega 2 will come out to be sigma 2 which is nothing but 2 i, again i is the unity matrix.
So, here you find that if you remember that for class omega 1 we had sigma 1 which was 2 i and for class omega 2 I have sigma 2 which is also 2 i.
So, this is a case where as we said earlier that sigma i is equal to i sigma square i .
So, it is this same condition and under which case we have seen that.
the discriminant functions will be linear and not only that the decision boundary between the two classes omega i and omega j that will also be linear.
So, this is a perfect case of that where I have sigma 1 and sigma 2 to be equal and that is 2 into i which is of the form sigma square i and what is sigma square?
Sigma square in this case is 2.
So, sigma is square root of 2 .
So, given this calculations now let us see how the decision boundary will look like.
So, come to the same set of points as we shown earlier that these points were belonging to class omega 2 and these were the set of points which were taken from class omega 1.
And we just shown earlier that when the covariance matrix sigma i is of the form sigma square i for all the classes.
The decision boundary takes the form W transpose X minus X naught equal to 0, where W is nothing but mu 2 minus mu 1 that is the vector drawn from omega 2 to omega 1 right or omega 1 to omega 2 if I compute the other way that is g 1 X minus g 2 X here the computation was g 2 X minus g 1 X and the middle point will be given by there is a mistake it is not mu 1 minus mu 2 that it, but it should be mu 1 plus mu 2.
2.
And the midpointthe point on this decision boundary is given by X naught is equal to half of mu 1 plus mu 2 minus sigma squared mu 1 minus mu 2 square into log of p of omega 1 upon p of omega 2 into mu 1 minus mu 2.
So, if you look at this if I consider the case that p of omega 1 and p of omega 2 that is the aft priority probabilities to be equal then this term will be equal to 0 and X naught will be half of mu 1 plus mu 2 which is nothing but the midpoint of the line joining mu 1 and mu 2.
And, the decision surface being orthogonal to W which is nothing but a line joining mu 1 and mu 2 or vector drawn from mu 1 to mu 2, my decision surface will be orthogonal bisector of the line joining mu 1 and mu 2 and that is what is this one.
You have this blue line that is this which is the line joining mu 1 and mu 2 here this was mu 1.
2 and this was mu 2 and this red dotted line which is an orthogonal bisector of this blue line is the decision boundary between the two classes omega 1 and omega 2.
And as I said that as p as I am assuming p omega 1 and p omega 2 to be equal.
So, this X naught is nothing but midway between mu 1 and mu 2.
So, given this if I have any point x which is falling on this side of the boundary the x will be classified to class omega 2 because I am considering these to bethe vectors taken from class omega 2.
Whereas, if I have an unknown vector y falling on this side of the boundary this unknown vector will be classified to class omega 1.
And as it is clear very clear from here the kind of classification rule that we have is nothing but a minimum distance classification rule.
2, because for any point on this side of the boundary its distance from mu 1 will be less than its distance from mu 2.
Similarly, for any point on this side of the boundary its distance from mu 2 will be less than its distance from mu 1.
So, the kind of classification rule that you have is a minimum distance classification.
So, this is what we get for a simple case when I have a situation.
that mu i is equal to sigma square i.
Now, let us go to the next case that when I have a situation that mu i a sigma i is equal to sigma, but this may not be of the form sigma square i.
What does it mean?
It means that the covariance matrix of the samples belonging to all the classes are same.
0, but the components of the vectors may not be statistically independent or in other words the off diagonal elements of this covariance matrix may not be 0.
Whereas, in our earlier simplified case we have we had assumed that the off diagonal elements were 0 right.
So, given this situation my g i x if I compute from here So, as before the g i y x was minus half log of 2 pi minus half log of mod of sigma i, but now sigma i is equal to sigma.
So, it becomes minus half log of determinant of sigma minus half x minus mu mu i transpose sigma i inverse.
Now, sigma i is equal to sigma.
So, it simply becomes sigma inverse into X minus mu i plus log of p of omega i.
Again if I simplify this you find that this is independent of the class, this is independent of the class.
So, these two do not participate in discrimination.
So, my g i x now becomes same as minus half x minus mu i transpose sigma inverse x minus mu i plus log of p of omega i and if you expand this it simply becomes minus half x transpose sigma inverse x minus 2 mu i transpose X plus mu i transpose mu i plus log of p of omega i .
As before you find that this X transpose sigma inverse X this is class independent because sigma is same for all the classes.
So, I can ignore this I can remove this from the discriminant function .
So, the discriminant function now simply becomes mu i transpose sorry here it should be there is a mistake this should be 2 mu i transpose sigma inverse x plus mu i transpose sigma inverse mu i .
So, what I get is mu i transpose .
sigma inverse x minus half mu i transpose sigma inverse mu i plus log of p of omega i ok. And this you find that this is again of the form w i transpose x plus w i naught And, this equation is again a linear equation, where this w i will now be sigma inverse mu i and w i naught will be minus half mu i transpose sigma inverse mu i plus log of p of omega i.
So, I will stop in this stop here in this particular lecture.
So, here we have what we have seen is.
starting from the discriminant function which we have seen that both in the case where sigma i is of the form of sigma square i, whereas and also sigma i is equal to sigma our decision or discriminant functions are linear.
In case of sigma i is equal to sigma square i, we have seen that the decision surface is also linear and the decision surface is orthogonal to the line joining mu i and mu j.
And, in case the a priori probabilities are equal that this is decision surface becomes an orthogonal bisector of the line joining mu i and mu j.
And, in the other case where my covariance matrix may not be a diagonal matrix and all the variances variances of all different components may not be equal, but even in that case the discriminant function g i x is a linear discriminant function.
So, we will stop here today in our next lecture we will start from this point.
Thank you.
Hello, welcome to the NPTEL online certification course on deep learning.
In, the previous class we have talked about topics like different types types of classifiers like Bayes minimum error classifier and Bayes minimum risk classifier.
And, we have also seen that Bayes minimum risk classifier under a specific case of 0 1 loss that is when the loss function for a correct decision is taken to be 0.
And, the lost function for an incorrect decision is taken to be 1 under that situation Bayes minimum risk classifier and Bayes minimum error classifier they are identical.
So, in today's lecture we will talk about we will start from those Bayes classifiers and then we will move on to what is known as discriminant function.
And then using the discriminant function we will also try to derive and we will also try to demonstrate.
the decision boundary between different classes.
So, when we talk about discriminant function, you remember from the previous class in case of Bayes minimum risk classifier or Bayes minimum error classifier, what we said is that for Bayes minimum error classifier, if p of say omega i given x is greater than p of omega j given x where omega i and omega j are two different classes and x is the unknown input vector in that case we classify the x to thisclass omega i.
And if I expand this p of omega i given x is nothing but p of 1, X given omega i multiplied by the a priori probability P of omega i.
Where P of X given omega i is what is known as the class conditional probability density, P of omega i is the a priori probability and P of omega i given X is the a posteriori probability based on which we make the decision.
that whether this unknown vector x should be classified to class omega i or it should be classified to class omega j.
So, obviously, if p of omega i given x is greater than p of omega j given x, then it is more likely or more probable that your unknown feature vector belongs to class omega i.
And this is what we had derived in our previous lectures using Bayes minimum error classification rule.
And, then base minimum risk classification what we had said is for an unknown feature vector x if we take an action alpha i then the risk involved is given by R of alpha i given x and which we said that this is nothing, but lambda alpha i given omega j .
into p of omega j given x, you take the summation over this for all the classes omega j .
So, for every action alpha i if I have say c number of actions.
So, i varies from 1 to c. So, for every such action I have to compute this risk function and for whichever action the risk the estimated risk r of alpha i given x is minimum I have to take that corresponding action .
And, as we said that under a specific case when this lambda of alpha i given omega j is equal to 0 for i is equal to j and if I take this equal to 1 whenever i is not equal to j that means, for every correctdecision the loss incurred is 0.
And, for every incorrect decision the loss incurred is 1 under that situation we had shown in the previous class that Bayes minimum risk classifier and Bayes minimum error classifier they turn out to be identical.
Now, starting from here we can define something called discriminant function because every time you find that whether I go for Bayes minimum error classification or Bayes minimum risk classification in case of Bayes minimum error classification.
For every class I am computing p of omega i given x where i varies from z 1 to c where c is the number of classes that I have.
And for whichever i p of omega i given x turns out to be maximum I classify x to that corresponding class.
Similarly in case of Bayes minimum risk classification for every class I compute r of alpha i given x.
And, then for whichever class R of alpha i given X is minimum that is for whichever class of for whichever action the risk involved is minimum I am taking the corresponding action or I am classifying X to that to that corresponding class.
And, I can say that in each of this case I am taking an action based on certain maximum criteria that is in case of base minimum error classification.
for whichever class the a posteriori probability is maximum, I am taking that corresponding action or classifying X to that corresponding class.
Similarly, for Bayes minimum risk classification for whichever class R of alpha i given X.
So, for whichever value of i R all R of alpha i given X turns out to be minimum or in other case I can say that instead of considering R of alpha i given X, I will consider.
minus r of alpha i given x.
So, if r of alpha i given x is minimum then obviously, r of alpha i given xminus r of alpha i given x will be maximum.
So, for whichever action this negative of the risk value turns out to be maximum I am taking that corresponding decision or I am classifying x to that corresponding class.
So, or in other words I can say that I can define a function.
say g i x for class say omega i.
So, here x is the unknown feature vector and for every class i every class omega i, I am computing a function g i x and for whichever i this g i x turns out to be maximum, I take decision in favor of that particular class or that particular omega i.
So, what I am doing is for an unknown feature vector x, I will compute g 1 of x, I will compute g 2 of x, I will compute g i of x and if there are c number of classes, I will compute g c of x.
And then I will try to find out that out of all these functional values whichever is maximum.
So, I take maximum of all of this.
And, for whichever i this turns out to be maximum I classify x to that corresponding class omega i.
So, this is a function that g i x I want to design for every class omega i.
So, what are the possible options that I can have g i x?
One of the option is obviously I can have g i x to be is equal to p of omega i given x that is straight forward.
which is nothing, but p of x given omega i into afterey probability p of omega i .
So, this is a straight forwarddefinition of g i x or I can also say that I will use g i x to be minus r of alpha i given x .
So, here also if g i x is maximum, then I I take that corresponding decision here also if g i x is maximum I take that corresponding decision.
So, out of these two options possible options because there might be other options as well out of these two we will try to explore this that is based on Bayes minimum error classification rule.
So, I will assume that.
we will use this discriminant function g i x where this discriminant function is defined as p of omega i given x or it is also possible that instead of p of omega i given x if I use a function of p of omega i given x where this function f has to be a monotonically increasing function.
That means If p i given x is greater than p of omega j given x, this should imply that f of p of omega i given x should be greater than f of p of omega j given x.
Let me rewrite this.
So, this implies f of p of omega i given x has to be greater than f of p of omega j given x .
So, that is f has to be a monotonically increasing function .
Then this form that is g i x as f of p of omega i given x that can also be used as a dis schematic function because, whenever g i x is maximum f of p of omega i x will also be maximum.
So, given this you will find that one very convenient function that can be used is logarithmic function.
So, or I can use natural logarithm ln what is the advantage?
The advantage is because p of omega i given x is nothing, but P of X given omega i which is the class conditional probability density function that can be estimated experimentally into P of omega i which is the probability of class omega i that is also precomputed and now if this f the function I used as the logarithmic function L n then the advantage that I get is.
L n p of omega i given x turns out to be L n p of x given omega i plus log of p of omega i .
So, this multiplication straight way is converted to an addition operation and which is very very advantageous in many computational purposes.
So, I will use this particular form that g i x is nothing, but log of p of x given omega i plus log of a priori probability p of omega i.
So, this is the discriminant function form that we will use in the remaining part of this lecture right.
So, given this let us try to see that if I assume a particular form or a particular distribution function probability density function which usually we use as a normal probability density function.
Then what form of expression of the discriminant function that we get or what form of the decision boundary between classes that we get.
So, let us talk about this discrete function under multivariate normal distribution.
So, we all know that the normal distribution if I have a single variable say p x is given by 1 over square root of 2 pi sigma e to the power minus x minus mu square upon 2 sigma square .
This is the normal density in case of a single variable x or scalar variable x where this sigma is nothing, but standard deviation and sigma squared is the variance and mu is the mean .
And you all know that the typical form of this is So, if I plot x and p x the typical form is like this, but this is what is your the mean of x that is mu and the spread of this envelope depends upon the value of sigma or sigma square.
So, if sigma is low or the sigma square is low then I will have a distribution something like this .
If the sigma square is high then the distribution will be flat of this form.
So, this is the form that I get when x is a single variable or it is a scalar variable.
But in our case since we are talking about feature vector which describes an object and the feature vector consists of multiple number of features where every feature captures some property or some attribute of the object.
So, those features may be computed from the shape of the object, they might be computed from the color of the object, they might be computed from the intensity of the object, they might be computed from the texture of the object and various such different properties are put together in the form of a vector or a feature vector.
So, the type of distribution that is important in our case is not a single variate distribution, but it is a multivariate distribution.
So, in our case I have a feature vector x and let me assume that the dimension of the feature vector is d. So, it is a d dimensional feature vector where d is the number of components or the number of features which are packed into this feature vector x.
So, given this the multivariate pro-emptive density is now given by P of x which is 1 over 2 pi to the power d by 2, then instead of variance now I have multiple variables.
So, what I have is a covariance matrix.
So, sigma is the covariance matrix.
You take the determinant of that and square root of the determinant into exponential minus half, minus half.
x minus mu transpose sigma inverse into x minus mu .
That is what is the normal distribution form of normal distribution in case of multivariates or in case of vectors .
Now, what we are interested in or the expression that we have that contains x given omega i that is the class conditional probability density.
And we said earlier that we get this class conditional probability density by taking the feature vectors x from class omega i.
So, when I take feature vectors x from class omega i.
So, for those feature vectors the mean that I will get is class dependent and I will represent that by mu i.
Similarly, the covariance sigma the covariance matrix sigma that I compute.
will also be on for that particular class omega i.
So, I will also represent this as covariance matrix sigma i.
So, what I will do is I will put this class conditional probability density function express it in the form 2 pi to the power d by 2.
Now, this sigma actually becomes sigma i because this is for class omega i.
square root of that into exponent minus half x minus.
Now, this mu becomes mu i it is for ith class transpose sigma becomes sigma i.
So, it is sigma i inverse into x minus mu i.
So, this is my multivariate.
to a pre-tensity function where sigma i is the covariance matrix computed over all the feature vectors which we call as training vectors because using their using those vectors I am computing sigma i and mu i.
So, it is the covariance matrix computed using those feature vectors taken from class omega i mu i is the mean of those feature vectors taken from class omega i.
Now, given this.
So, the way we have defined g i x is equal to log of p omega i given x plus log of a priory probability p of omega i .
Now, you find that p of omega i given x is nothing but 1 over 2 pi .
to the power d by 2, then sigma i square root of this exponential minus half x minus mu i transpose .
So, this is what is .
p of omega i given x .
So, once I use this logarithm then my g i x it simply becomes minus d by 2 log of 2 pi minus half log of sigma i minus half of x minus mu i transpose sigma i inverse x minus mu i ok plus of course, I have this log of p of omega i.
From here you find that d by 2 log of 2 pi term.
This particular term is independent of the class because there is no term like subscript i over here.
So, this minus d by 2 log of 2 pi this does not differentiate between an ith class and a jth class.
So, easily I can conveniently ignore this particular term minus d by 2 log of 2 pi.
2 pi.
So, that simplifies my g i x as minus half log of p sigma i minus half x minus mu i transpose.
sigma i inverse x minus mu i plus log of a priori probability p of omega i .
Now here I can have different cases.
So, for example, this covariance matrix sigma i in a particular case in a specific case if all the components the components of the feature vector x they are statistically independent, then covariance matrix that we get will be a diagonal matrix.
And if every component has same variance, then this covariance matrix sigma i will be of the form sigma squared i .
So, what I am assuming here that for all the classes.
The feature vectors that you obtain the components of the feature vectors are statistically independent.
So that means, if I try to compute the variance involving say ith component and jth component because they are statistically independent.
So, that variance will be equal to 0 which leads to the covariance matrix to be a diagonal matrix where only I will have diagonal elements to be non-zero and all the off diagonal elements will be 0.
And, then again if I assume that all those components for all those components the variance is same in that case all the diagonal elements which are nonzero they will be equal.
So, that ultimately leads to the covariance matrix to be of the form this sigma square i and I am assuming this to be same for all the classes that means, for every sigma i I have this covariance matrix for every omega i.
the covariance matrix sigma i is of the form sigma square i.
So, the sigma square is again same for all the features across the classes.
So, that is one of the simplified assumption that I can make.
The other assumption that can be used is where sigma i is of the form sigma.
So, in this case, It is not necessary that the different components of the feature vector will be statistically independent, not even necessary that every component will have the same variance, but what I am assuming is that whatever is the covariance matrix the same covariance matrix is valid for all the classes.
So, this is a simplified condition condition 2.
And the third one where I have the most general case.
that every class will have its own covariance matrix that is the covariance matrix of one class need not be same as covariance matrix of other classes.
So, that is the most general case which is case 3.
So, initially I will try to see that how this discriminant function look like when I assume the first case that is covariance matrix of every class is of the form sigma square i.
So, let us see this.
So, what I have is g i x is equal to minus half log of sigma i minus half x minus mu i transpose sigma i inverse x minus mu i plus log of p of omega i.
So, here as I am assuming that this sigma i the covariance matrix is same for all the classes.
So, this minus half log of determinant sigma i again this does not have any role in discriminating among different classes.
So, I can simply ignore this term from the function of from the expression of the discriminant function.
So, my g i x now simply becomes minus half x minus mu i transpose and sigma i because it is sigma square i.
So, this sigma i is of the form sigma square i.
So, this sigma i inverse is simply 1 upon sigma square.
So, what I will do is I simply put it as 1 upon 2 sigma square.
x minus mu i transpose into x minus mu i plus log of p of omega i which simply becomes minus 1 upon 2 sigma square.
If I expand this it becomes x transpose x minus twice mu i transpose x plus.
mu i transpose mu i plus log of p of omega i .
In this expression again this X transpose X is class independent right.
So, again this term does not contribute to discrimination.
So, I further simplify this as minus 1 by 2 .
sigma square what I have within the bracket is minus 2 mu i transpose X plus mu i transpose mu i plus log of p omega i.
You simplify this it simply becomes mu i 1 upon sigma square mu i transpose X minus 1 upon 2 sigma square mu i transpose mu i plus log of p of omega i which I can write in the form w i transpose X plus w i naught where this w i is nothing but 1 upon sigma squared mu i and w i naught is 1 upon 2 sigma squaredminus 1 upon 2 sigma squared mu i transpose mu i plus log of p of omega i.
So, you find that the expression that you get is a linear expression.
That means, under the simplified case when all the components all when the components of the feature vectors are statistically independent, all the components have the same variance sigma square and this is same for all the classes or in this particular case I am not assuming it is same for all the classes, but I am considering only a particular class omega The g i x is simply of the form of w transpose x plus w i transpose x plus w i naught which is a simply a linear expression right.
So, from here I can try to find out that what isthe boundary between two different classes.
omega i and omega j.
So, in order to do that let me again try to find out.
So, that boundary I can simply defined as g x and on a boundary I must have g i x is equal to g j x that is the discriminant functional value for ith class and for jth class they should be same on the boundary.
So, the equation of the boundary can simply be written as g x is equal to g i x minus g j x which is equal to 0.
So, this is simply the equation of the boundary between two classes omega i and omega j.
And what we have seen is that for g i x under this simplified assumption we have seen that g i x is nothing, but minus 1 upon 2 sigma square into x minus mu i transpose x minus mu i plus log of p of omega i .
Similarly, for g j x I will also have the case that it is minus 1 upon 2 sigma squared into x minus now it will be mu j transpose x minus mu j plus log of p of omega j and if I equate these two if I met make g x is equal to g i x minus g j x to be equal to 0, then we will find that by putting g i x this expression and in place of g j x this expression, you will find that this g i x equal to 0, this will take a form w transpose x minus x naught is equal to 0, where you will find that this w is nothing but mu 1 minus mu 2 and x naught will be same as half of mu 1 plus mu 2 minus sigma square upon mod of mu 1 minus mu 2 square into log of p omega 1 upon p omega 2 into mu 1 .
2.
So, this is the expression that I will get for the boundary between the two different classes.
So, I will derive the expression of this boundary under this simplified case in our next lecture.
Thank you.
Hello, welcome to the NPTEL online certification course on deep learning.
You recall in the previous class what we did is we discussed about the different descriptors or the features that can be extracted from a given signal, whether the signal is a visual signal like an image or what we can see around the world or it can also be a voice signal like speech signal and the applications are in the object recognition, object classification, understanding the world, speech identification, speaker identification, then speech to text conversion and many such applications.
So, what we talked about in the previous two lectures is that given any such signal we can extract the descriptors or the features from that signal which represent that particular signal whether it is visual or auditory signals.
And once you represent a signal using a number of features or a number of descriptors, those features can be represented in the form of a vector.
So, if I extract say three features from any given signal, the features may be obtained from the shape or may be obtained from the region like its intensity its color or its texture.
So, those three different features if I put in the form of a vector, it becomes a vector in one dimension having three different components.
Or in other words the signal is represented by a point in a three dimensional feature space or by a vector in a three dimensional feature space.
So, in general for a given signal you extract a number of such features.
So, you extract d number of such features or d number of such descriptors and put those descriptors in a particular order to get a vector having d number of elements or a d dimensional vector .
So, once I have a d dimensional vector.
So, effectively what I am doing is I am transforming my signal into a feature space or a vector space, where in that d dimensional vector space a signal will be represented by a particular vector.
And once I represent the signal by a particular vector which is nothing, but a point in my d dimensional space and if I have multiple number of such signals then for each of those signals I will have a corresponding point or a corresponding vector in a d dimensional space.
So, once I have this feature representation or vector representation, then just to measure whether two different signals are similar or two different signals are dissimilar, I can simply try to find out what is the distance between those two vectors or what is the difference of those two vectors.
So, if the difference is very high or the distance between two vectors is very high, in that case I can infer that the signals are not similar.
Whereas, if the distance is very small that means, the points are very close to each other then I can infer that those two signals are very close or they are similar.
So, in today's lecture what we will discuss is we will have the concept of features based representation of any given signal.
Then to understand or to recognize those signals we will discuss about Bayes rule.
Then, we will have two types of classifiers or two types of recognizers, one is Bayes minimum error classifier, the other one is Bayes minimum risk classifier.
So, let us see what this feature space representation means.
So, for the time being let us assume that a signal and to demonstrate this we will mostly take visual signals.
So, let us assume that the visual signal is represented by a two dimensional feature vector having components x 1 and x 2 or in other words we have extracted let us assume that we have extracted only two features.
One feature may be from the shape of the object, the other feature might have been obtained from the intensity of that particular object.
So, I have two different features one is feature x 1 the other one is feature x 2.
So, given these two features now let us assume that I have visual signals or have images or birds.
I have images of cars, I have images of dogs and so on.
So, if I get one image of a bird as we said that this image of this signal will be represented by a vector or a point in my feature space.
So, obviously, here you can see that to represent that image of a bird just two dimensional feature vector is not sufficient, because the birds have different shapes, the birds have different colors, they have different intensity levels, a bird may be obtained from various orientations.
it may might have been observed from various orientations.
So, accordingly I have to have a large number of features to describe a bird.
But since I cannot show any dimension on a paper of more than 2, so I am taking a simplistic case that I am assuming that this bird is represented by two dimensional feature vector.
So, given that I if I take one picture that picture is represented over here in my feature vector space which is given by x 1 and x 2.
If, I take another image of some other bird you find that here this bird or this picture is also placed at a vector location which is very close to the previous location of the bird.
But, you notice that these two locations of these two vectors are not identical because the images are not identical.
Similarly, if I take another one I will again get another which is close to the previous two vectors.
Now, if I take image of a car you find that the a car is represented by a vector over here which is far away from the vector representation of the birds.
And now if I find out the distance or the difference of this vector representing a car and a vector representing a bird the modulus of the vector difference will will be quite high or in other words the distance between these two will be quite high.
which is again an indication that these two images are not similar or a car or a bird neither in terms of shape nor in terms of color they are similar.
Again if I take a second car I get a vector over here vector representation, but again it is not identical to the previous location of the car.
The reason being the images are not identical I can can have I can have cars of various colors of various shapes of various sizes.
the card images may have been obtained from various orientations.
So, accordingly their appearance will change and the descriptors or the features that we compute that will also be changed.
If I take image of a dog you find that a dog may be represented by a vector somewhere in this location.
If I take another dog this dog is also represented by a vector in this location in my feature space.
So, like this and again you find that the distance between the.
first dog and that is and the second dog this distance is smaller than the distance between the dog and the bird and the distance between dog and the car, which again indicates that this first image the image of the first dog and this second dog image they are very similar, whereas this dog image and the car image or the dog image and the bird they are dissimilar.
So, I can continue like this and you find that for a large number of images of birds or a large number of images of cars or a large number of images of dogs each of these vectors they represent a cluster of vectors.
So, all the images of dogs that forms a cluster, all the images of cars that forms a cluster in my vector space, all the images of birds form another cluster in my vector space or feature space.
And, obviously as I said that all of them will not be represented by the same vector, all these bird images are not represented by the same vector, but the because of variation in the images I can have images of different birds of different colors, they may have different intensity values.
So, there will be always be a variation in the vector representation.
Similarly, there will always be a variation in the vector representation of cars, there will always be a variation in the vector representation of the dogs.
So, as a result what we get is given images of the signals belonging to I can call that this is a class of signals, ok.
Similarly, this is another class of signals which represents the class car, this is another class of signals which represent the class dog, right.
So, you find that the vectors coming from a particular class that forms a cluster of feature vectors or a cluster of points in my feature space.
The same is true in case of bird, the same is true in case of cars, the same is true in case of dogs.
And naturally there is a distribution of these points in the feature space right.
So, what we have if I put in the other way every such image represented by a point or by a vector in the feature space, I can put this image in this form.
So, here you find that over here.
this represents one cluster of points.
Similarly, this represents another cluster of points or cluster of feature vectors, this represents another cluster of points.
And when I have these different cluster of points, every cluster is represented by a distribution and that distribution if you look at these figures, the density of the feature vectors at the center is very high as you move away from the center as the distance between the center and the other point increases you find that the density goes on goes on decreasing.
Similarly, over here at the center the density is high which is actually mean location mean of the cluster of vectors and as I move away from the center the density goes on reducing.
Similarly, in this case at the center the density is high as you move away from the center.
the density goes on reducing.
So, this indicates that all these feature vectors follow a particular probability density function.
And in this particular case in this diagram as I have shown it in a two-dimensional feature space, this probability density is elliptic distribution.
In one case it is circular, in other two cases those are elliptic.
If I go for three-dimensional then this will be represented by spherical distributionor ellipsoidal distribution.
When the dimension becomes even more more than 3 which I cannot possibly draw on a two-dimensional plane, I represent those as or term those as hyper spherical or hyper ellipsoidal.
So, what are these spheres or the ellipsoids mean?
See for example, here If I take a contour of equal probability density functions that becomes a circle.
Similarly, in this case if I take a contour of equal probability density values it becomes an ellipse.
And as I move away from the center the value of the probability density goes on reducing.
So, that is what is meant by these different diagrams.
Now, if I go further in a three dimensional representation those densities can be represented by surface plots.
So, as you see over here this one may represent the probability density or distribution of the vectors coming from a particular class of objects.
This class may be the class of birds, this distribution may be coming from a class of dogs, this distribution may be coming from a class of cars, this distribution may be coming from a class of bicycles and so on.
So, this diagram through this diagram what I wanted to show that the distribution that you get is dependent on the class.
So, if I take any value of x.
So, what I get is that what is if I take any feature vector from a class of say dogs, what is the probable value of X?
If I take a feature vector from a class of cars, what is the probable value of that feature vector X?
So, that is the distribution which is given by this figure.
So, or in other words what I get is I get a class conditional probability density function which is known as X.
P of X given omega i, this omega i I represent as i th class.
So, as I said that this class may be class of dogs, this class may be class of birds, this class may be class of cars and so on.
So, what I have is a class conditional probability density function and how do you get this class conditional probability density function?
This is a part of training.
What I take a large number of images belonging to bird and for all those images I compute what is the feature vector x.
And then I find out what is the distribution of those feature vectors x in my feature space.
And because all these feature vectors I obtained from images from the class birds.
So, this distribution is for the feature vectors coming from the class bird.
So, I get P of the feature vector x given omega i, where this particular omega i is nothing but class parts.
Similarly, if I take a large number of images from of cars, find out the corresponding feature vectors and find out the distribution of all those feature vectors again in the feature space.
So, in this case the class of cars I write as say class omega j .
which indicates cars .
So, the distribution of the feature vectors that I get in this particular case is nothing, but the probability density of the feature vector x given the class as cars .
So, these are all the class conditional probability density functions which I have to compute from a large number of signals or large number of images with the examples that we are taking .
from a particular class.
So, I represent this as this class conditional probability density function and once I have this then given an unknown image if I want to recognize that what that image is then I have to make use of this class conditional probability density function to find out that given a signal.
which class it belongs to that is given by class conditional probability density functions p of omega j or p of omega i .
So, what I have is p of x given omega i or p of x given omega j where x is feature vector and omega i or omega j is the class.
During classification what I want to have is or what I need to do is that given an unknown feature vector vector, I have to identify whether this unknown feature vector is coming from a bird or this unknown feature vector corresponds to a car or this unknown feature vector corresponds to a dog.
So, what I need to compute is what is P of omega i given x.
So, what I have during training is P of x given omega i and what I need to compute is P of omega i given x.
In addition to this there is another concept.
So, this is what is known as class conditional probability density function and this is what is known as a posteriori probability density function.
In addition to this there is another concept of a priori probability of classes that means, what is the probability that in a given domain a sample will belong to the class omega i or p of omega i which is the priori probability.
So, as an example just to give you an example that how this may occur.
See if I go to a forest and I see an object, then after looking at the object if I have to infer whether that object or that animal or whatever I am seeing in the forest should be a car or it should be a bird and it is quite obvious.
that, if I am visiting a forest it is more likely to see a bird than to see a car because you cannot expect a car to enter a forest until until and unless there is a motorable road whereas, the birds are abundant in the forest.
So, given any object the a priori probability or the probability that that object of thewhat I am seeing will be a bird if I have to compare between bird and a car.
that the a priori probability that it will be a bird is more than the probability that will be that it will be a car and that is what is this p omega i or a priori probability.
Now given this I want to go for how the classification of the objects or the signals can be done given the feature vectors belonging to different objects.
So, for that what I will have is Bayesian learning.
So, before we go for what is Bayesian learning, let us just recapitulate probability rule.
That the probability says that if I have two events say event A and event B, the probability of event A and B can be probability of a given b into probability of b which is also same as probability of b given a into p of a .
So, the Bayes rule or the Bayes decision Bayes decision theory is based on this probability rule .
So, in this case we can say that this x is my feature vector a is my feature vector x .
and B is a class omega i.
So, what does it mean?
That if I have been given a feature vector of an object or of an image belonging to class omega i, then what is the probability that both of them occur together?
So, which is nothing, but P of X given omega i into p of omega i which is the a priori probability of class omega i and that is also same as p of omega i given x into a priori probability p of x .
So, you recall that what I said just few minutes ago that through training what I compute is P of X given omega i.
That means, if I take a large number of images of birds and I compute the distribution of the feature vectors computed from all those bird images, then actually what I am computing is P of X given the class of objects as birds.
Similarly if I take a large number of images of cars and compute the feature vectors from those images and find out what its distribution I compute what is what is the distribution of those feature vectors computed from images of cars.
What I am actually computing is the class conditional probability P of x given cars, where this x is the feature vector and cars are my class.
or class of objects.
So, what I have computed is basically class conditional probability density function.
In addition to that in that particular situation or in a given situation, I also have the aftere probability of birds and I also have aftere probability of cars.
So, what does it mean?
If my domain is a forest that means, I have visited a forest.
So, I And, there what is the probability that an object that you see is a car or it is a bird then those are the a priori probabilities P of birds and P of cars.
So, given these two now using that probability theory I can say that P of X given omega i into P of omega i is equal to P of of omega i given x into p of x.
So, this p of x given omega i, this is the class conditional probability that I have computed p of omega i is the a priori probability, but what I need to compute is p of omega i given x that is a posti a posti probability.
So, this computation p of omega i given x is nothing, but p of x given omega i into p of omega i upon p of x .
So, if I compute the same suppose omega i is the class of birds and I have another class omega j which is the class of cards.
This is nothing, but p of x given omega j into p of omega j that is a priori probability upon p of x .
Where what is this p of x?
p of x is nothing, but p of x given omega i into a priori probability omega i take the summation over all i .
And if you look at these two expressions .
out of these two whichever is more I will classify or I will associate this input vector X to that corresponding class.
So, if I find that P of omega i given X is greater than P of omega j given X, then obviously, my interpretation will be that X belongs to class omega i.
This is the interpretation that I will have.
So, given these two expressions of P of omega i given x and P of omega j given x, you find that this P x appears in the denominator both of them.
So, this P x does not contribute anything in discrimination.
So, I can simply compute P of omega i given x is equal to P of omega j.
X given omega i into a priori probability omega i and P of omega j given X is equal to P of X given omega j into a priori probability P of omega j .
So, I can simply compute this which is an approximation of P of omega i given X because I have ignored the denominator which is P of X .
And, out of these two whichever is larger I will associate my input vector to that corresponding class.
And, you find that given an x if I have P of omega i given x to be greater than P of omega j given x I still have.
So, I am deciding that x is associated with class omega i.
1, but I still have a finite probability that X may belongs to class omega j and that is what is my probability of error.
But I am taking decision in favor of that particular class for which a posteriori probability is more and my probability of error is the probability of X belonging to the other class.
So, in this particular case the probability of error is the probability p of omega j given X which is minimum of the two.
So, this particular classifier that we have discussed today is what is known as Bayes minimum error classifier.
So, in this lecture what we have done is we have talked about the vector representation of the different signals and then we have talked about the Bayes rule and discussed about the Bayes minimum error classifier.
So, in the next class we will talk about Bayes minimum risk classifier .
And, we will continue further.
Thank you.
Hello, welcome to the NPTEL online certification course on deep learning.
You remember that in the previous lecture we have talked about the feature distribution in the feature vector space and we have shown that this distribution because the different objects that we get from the same class all of them may not be identical.
The reason that they may not be identical is that the variation among the objects whether it is shape or color or texture or illumination or orientation whatever .
So, when you get multiple instances of the objects belonging to the same class, it is hardly possible that the feature vectors that we compute given two instances of the objects belonging to the same class, those feature vectors will be identical.
And because of this variation, when I have large number of objects belonging to a particular class and I compute the feature vectors of all those different objects belonging to the same class, all these feature vectors are not identical.
Rather in the feature space they will form a sort of distribution.
So, in today's class in the previous class what we have talked about this feature space representation, then we have talked about the Bayes rule and then we have also talked about the Bayes minimum error classifier.
So, today we will further analyze the remaining part of Bayes minimum error classifier.
Then, we will go to what is known as Bayes minimum risk classifier and I will also try to discuss whether there is any relation between Bayes minimum error classifier and Bayes minimum risk classifier we will try to see that.
So, this is what we have shown in the previous class that is given images from the same class, but multiple number of images belonging to the same class over here.
So, we had considered three different classes the class of birds, the class of dogs and the class of cars.
you find that all theserepresentations of these objects in the feature space that forms a cluster or a distribution.
So, we had shown in the other figure in 3D how this distribution looks like.
And we have said that because we are computing thedistribution of the feature vectors belong from objects belonging to a particular class.
So, this tells us what is the class conditional probability density function or p of x given omega i, where omega i is the class and x is the feature vector that we have computed.
So experimentally by collecting large number of objects from different classes, I compute this p of x given omega i.
And at the same time I also have an probability that is what is the probability of occurrence of class omega i.
So, I have two concepts p of x given omega i and the a priori probability p of omega i and from this for classification or for recognition the what I have to compute is what is p of omega i given x and we have shown using Bayes rule that this is nothing but p of x given omega i into a priori probability.
omega i.
So, given two classes say birds and cars I have to compute given an unknown feature vector what is p of say car given x and I also have to compute the probability of what is p of bird given the same x and this p of car given x is nothing, but P of X given car which we have already computed through experiments that is our cross conditional probability density multiplied by the appare probability what is P of car.
Similarly, in this case we will compute what is P of X given bird into appare probability what is P of bird.
So, these are my class conditional probability densities, these are the a priori probabilities and from this we compute the a posteriori probability p of car given x and p of bird given x.
So, out of these two whichever is more for a given unknown x feature vector.
So, if I find that p of car given x is greater than p of bird given x, then my inference will be that this feature vector x belongs to car or the object from which this feature vector x has been computed.
that object is nothing, but a card nothing, but, but a car.
So, what we are computing is P of omega i given x if it is greater than P of omega j given x then my interpretation is x belongs to omega i.
If P of omega j given x is greater than p of omega i given x, then my interpretation will be the other ways that is x belongs to omega j.
Now, even in this case you find that if I plot these two in one dimension say p of omega i given x may be a plot something like this.
So, here what I have is my feature vector x and in this direction what I have is p of omega i given x.
So, if my omega i is barred then I can have this sort of a posteriori probability density whereas, for car maybe p of car given x is having some density something like this.
So, this is my say omega i, this is my j.
So, given any unknown x say x vector is somewhere over here, you find that for this x p of car given x is more than p of bird given x.
So, as a result I am deciding that this x belongs to car, it does not belong to bird.
But still there is a finite probability that x may belong to bird also.
error and that is what is my probability of error.
So, in this case what is the probability of error that I have?
The probability of error is nothing but the minimum of the two.
So, p error if I put it write it like this p of error given x is nothing but .
minimum of p of omega i given x and p of omega j given x .
So, as I am taking the decision which minimizes the error because it is minimum of these two quantities p of omega i given x and p of omega j given x.
So, the classifier that we design is what is known as Bayes minimum error classifier.
And what is the total error in this case?
Total error of classification is nothing but the error or the area under this curve.
So, which is nothing but if I integrate this area minimum of these two over x varying from minus infinity to infinity, then what is what I get is the total error of this classification rule that is given by this minimum error classifier, ok.
Given this now I go to the next type of classifier which is Bayes minimum risk classifier.
So, this minimum risk classifier is more general from Bayes minimum error classifier.
In the sense that in case of B S minimum error classifier, we have considered only the classes.
See if I have c number of classes, then I have classes omega i, i varying from 1 to c. So, I take if I take c number of classes omega i, where this i varies from 1 to c. See if I have c number of classes.
In case of Bayes minimum risk classifier, we consider this omega i to be the states of nature which are nothing but classes for our classification problem and we also have a set of actions say alpha i where or alpha j set of actions alpha j where j varies from say 1 to capital K. So, I have seen number of 2 states of nature.
I have k number of actions alpha i as before I consider the feature x to be a d dimensional feature vector, but what makes Bayes minimum risk classifier more general than Bayes minimum error classifier is introduction of a loss function.
So, this loss function is introduced loss function lambda which is alpha i given omega j.
That means, if I take an action alpha i where the true state of nature is omega j or the true class is omega j, then the loss that we incur is lambda alpha i given omega j.
So, given this the base minimum risk classifier works in this fashion.
In our classification problem, our problem is that given an unknown feature vector x, we told earlier that any input signal now will consider to be of feature vector in my feature space.
So, my classification problem is that given any unknown feature vector x, I have to compute or I have to predict to which class omega i that feature vector belongs.
So, that is my classification problem.
In case of Bayes minimum risk classifier, it is assumed that for every such action you take or for every such prediction that x belongs to a particular class, you have a risk involved in it.
So, you compute the risk, the value of the risk for every decision that you are taking.
And, the decision that gives you the minimum risk you have to take the corresponding decision.
So, it is like this.
So, again I assume that I have a given feature vector x and on this x I take an action alpha i.
So, the risk involved in taking action alpha i given x can be computed as if the true state of nature is say omega g.
Then, we said that we incur a loss lambda alpha i given omega j because alpha i is the action that I am taking whereas, two state of nature is omega j it should have been omega j.
So, for while taking this action I incur some loss.
So, that is my loss function lambda omega i given lambda alpha i given omega j into what is the probability p of sorry let me rewrite.
So, I have been given vector x and I take an action alpha i.
So, I compute the risk r of alpha i given x which is nothing, but the risk involved for taking action alpha i if the true state of nature is omega j multiplied by a posteriori probability p of omega j is given x.
And, you take the sum of this over all j, why over all j because I am taking action alpha i, maybe the actual true the true state of nature is omega 1, the signal actually belongs to class omega 1.
So, for that what is the risk?
It may actually belong to class omega 2 for that what is the risk?
And likewise, may actually belong to class omega c and therefore, they for that what is the risk.
And if I add all these risks then I get the overall risk for taking action alpha i given my feature vector x right.
So, this I have to take.
So, I have to compute this R of alpha i given x for all i we said that we have k number of actions.
So, for all i varying from 1 to k and out of all these for whichever r of alpha i given x is minimum, I have to take that corresponding action.
So, that is why it is Bayes minimum risk classification that is I want to take that particular action for which my risk is minimum.
Unlike in case of Bayes minimum error classification, there you decided x to belong to a particular class which minimized your error.
So, now it is minimization of risk.
Now, let us see whether I can establish any relation between this minimum risk classification and minimum error classification.
Let us assume that we have an 1 0 loss function that is I assume that lambda alpha i given omega j, let me represent this in short as lambda i j.
So, if i is equal to j that means I am correcting, I am taking the correct action, it belongs to class omega j and I am saying that and I am my decision is it belongs to class omega j and it actually belongs to class omega j.
So, in that case the loss function, the loss that I incur is 0.
So, that this lambda the loss function is equal to 0 if i is equal to j and I also assume that this is equal to 1 if i is not equal to j.
So, that is for every incorrect action you incorporate a unity loss and which is same for all incorrect decisions, but if your decision is correct obviously you are not incurring any loss.
So, lambda i j is equal to 0.
So, given this now if I compute the risk involved in taking an action alpha i given x which we have said that this is nothing but lambda i j that is lambda of alpha i given omega j into p of omega j given x and sum of this.
over all j.
And now coming taking these values of lambda i j you find that we have defined that wherever i is equal to j lambda i j is equal to 0 and wherever i is not equal to j lambda i j is equal to 1.
So, this expression simply becomes p of omega j given x take the summation over all j not equal to i because wherever j is not equal to i lambda i j is 1 and whenever j is equal to i lambda i j is equal to 0.
So, I have to take this summation over all j where j is not equal to i for all of them lambda i j is 1 and this is nothing but 1 minus p of omega i.
given X.
Now, see that what we wanted is in Bayes minimum risk classification, I want to take that particular action alpha i for which r of alpha i X alpha i given X is minimum.
In Bayes minimum risk classification, we wanted to classify X to that particular class for which p of omega i given X is maximum.
Now, if you look at this expression, you can see that R of alpha i given x is equal to 1 minus p of omega i given x.
Obviously, you can find out you can check that R of alpha i given x will be maximum or R of alpha i given x will be minimum when p of omega i x is maximum because R of alpha i given x is nothing but 1 minus p of omega i given x.
So, however, r of alpha i x is minimum, p of omega i given x is maximum.
So, in this particular case when my loss function is 1 0 loss function or 0 1 loss function that is for every correct decision I assume that I incur a loss or I do not incur any loss that is the loss function value is 0 for every incorrect decision I incur unity loss.
So, under that condition my base minimum risk classifier and base minimum error classifier both of them are same.
But in general for taking wrong decisions for different types of wrong decisions my loss function will not be same right.
So, we will discuss about thatmore later.
Now, let us considercase oftwo classes.
So, we are discussing about the risk function and we have defined that if you take an action say alpha i where the true state of nature is say omega j, then for an given input vector x the risk involved is given by r of alpha i given x which is nothing but lambda alpha i given omega j.
into p of omega j given x and you take the sum of this over all j.
So, that is the total risk for taking an action alpha i given an input feature vector x.
So, if I have say k number of such actions where alpha i, i varying from 1 to So, for each of this i, I have to compute what is the risk function and I have to take that particular action for which the risk involved is minimum.
So, that is what Bayes minimum risk classifier says.
So, now let us take a two class problem where I assume that I have two actions given as alpha 1 and alpha 2 and I have 2 states of nature or 2 classes given by omega 1 and omega 2.
So, let us compute try to find out that what is the risk involved if I take an action alpha i or what is the risk involved if I take an action alpha 2.
So, alpha 1 and alpha 2 I want to compute the 2 risk functions.
And the loss function that we have defined lambda alpha i given omega j.
For simplicity, I will write this in the form of lambda ij.
So, the risk or the loss function for taking an action alpha i, if the true state of nature is omega j which is which is lambda alpha i given omega j, I represent this as lambda ij.
So, given this now I will have two risk functions involved, one is for taking action alpha i, the other one is for taking action alpha j.
So, I have to compute r of alpha 1 given omega 1 and I also have to compute r of alpha 2 givensorry r of alpha 1 given x and I also have to compute r of alpha 2 given x.
So, this r of alpha 1 given x is nothing but lambda 1 1 p of omega 1 given x plus lambda 1 2 p of omega 2 given x.
So, here you find that lambda 1 1 is nothing but lambda alpha 1 given omega 1.
Similarly, lambda 1 2 2 is nothing but lambda alpha 1 given omega 2.
And in the same form I can write all of r of alpha 2 given x as lambda 2 1 p of omega 1 given x plus lambda 2 2 p of omega 2 given x.
So, given these two risk values my decision will be in favor of action alpha 1.
or deciding that input x belongs to class omega 1 is when I find that r of alpha 1 given x is less than r of alpha 2 given x.
That is the risk involved in taking action alpha 1 is less than the risk involved in taking taking action alpha 2.
So, if I put this bringing or using the risk values from here.
I have to have lambda 1 1 p omega 1 given x plus lambda 1 2 p omega 2 given x this has to be less than lambda 2 1 p of omega 1 given x plus lambda 2 2 p of omega 2 given x .
Or I can rewrite this as lambda 2 1 minus lambda 1 1 p of omega 1 given x has to be greater than lambda 1 2 minus lambda 2 2 p of omega 2 given x right .
So, this is the condition that has to be satisfied that is lambda 1 2 minus lambda 2 1 minus lambda 1 1 into p of omega 1 given x has to be greater than lambda 1 2 minus lambda 2 2 p of omega 2 given x.
So, let me just rewrite this in the form as it may or refresh this .
So, my condition was that lambda 1 1 p of omega 1 given x plus lambda 1 2 p of omega 2 given x that has to be less than p lambda 2 1.
p of omega 1 given x plus lambda 2 2 p of omega 2 given x.
So, which we have rewritten in the form lambda 2 1 minus lambda 1 1 p of omega 1 given x has to be greater than lambda 1 2 minus lambda 2 2 into p of omega 2 given x.
So, this is the condition that has to be satisfied for taking a decision in favor of class omega 1 or for taking action alpha 1.
So, I can also rewrite this in the form p of omega 1 given x 2 upon p of omega 2 given x to be greater than lambda 1 2 minus lambda 2 2 upon lambda 2 1 minus lambda 1 1.
And you remember that lambda 1 2 is the loss function for taking an action alpha 1.
when the actual true of nature is omega 2 and lambda 2 2 is the loss function involved when you are taking action alpha 2 when the true class of nature is omega 2.
So, naturally lambda 2 2 has to be less than lambda 1 2.
Similarly, lambda 1 1 will also be less than lambda 2 1.
So, both these quantities both in the numerator and denominator on the right hand side both these quantities are positive.
And again you remember from the Bayes rule that p omega 1 given x which is the a posteriori probability.
I can write this as p of omega 1 given x as p of x given omega 1 which is the class conditional probability of x.
into the a priori probability p of omega 1.
Similarly, p of omega 2 given x can also be written as p of x given omega 2 into p of omega 2.
So, using this now this expression can be written as p of x given omega 1 upon p of X given omega 2 has to be greater than lambda 1 2 minus lambda 2 2 upon lambda 2 1 minus lambda 1 1 into p of omega 2 upon p of omega 1.
So, now considering this p of X given omega 1. .
to be a function of omega 1, p of x given omega 1 gives me the likelihood value.
And accordingly p of x given omega 1 upon p of x given omega 2 that gives me the likelihood ratio.
So, this expression that in order to take an action in favor of class omega 1 which is p of x given omega 1 upon p of x given omega 2.
has to be greater than lambda 1 2 minus lambda 2 2 upon lambda 2 1 minus lambda 1 1 into p of omega 2 given omega 1.
This condition has to be true for taking an action in favor of class omega 1 and that is what comes from the Bayes minimum risk classification rule.
So, going by that you find that on the right hand side of this expression of this inequality.
that is lambda 1 2 minus lambda 2 2 upon lambda 2 1 minus lambda 1 1 into p of omega 2 upon p of omega 1 this is independent of X.
So, a favorable decision in favor of class omega 1 can be that if the likelihood ratio is greater than certain threshold where the threshold is given by this.
The thresholds are in terms of the loss functions and the appraisal probabilities.
So, we can say that if the likelihood ratio is above than this threshold, then we take an action in favor of class omega 1.
So, we will continue this discussion further.
So, in today's lecture what we have discussed about we have recapitulated our previous lectures content that is feature representation of a given signal.
Then we have talked about the Bayes theory.
classification and the Bayes minimum error classifier, Bayes minimum risk classifier and we have also tried to establish that what is the relation between Bayes minimum error classification and Bayes minimum risk classification.
Thank you.
Hello, welcome to the NPTEL certification course on Deep Learning .
So, in today we are going to introduce the content of this course and we are going to talk about that what all we will be covering in this lecture series on deep learning.
So, the topics that I will cover is obviously, the first one is an introduction to deep learning, then we will talk about that when we learn something how do we learn.
When we are going for deep learning, what is the difference between machine learning and deep learning?
Then coming to deep learning again, we will talk about two different models of deep learning.
One of them is the discriminative model and the other one is the generative model.
Then we will see that what are the challenges of deep learning applications, when we want to have any application of the deep learning techniques, what are the challenges that we face.
how we try to mitigate those challenges.
And then we will briefly talk about that what is the power of deep learning techniques or what we can do using deep learning methods.
So, first let us try to see that what is learning or then we will come to what is machine learning.
So, to talk about what is learning I will show you these two pictures or .
you take any picture for thatmatter or even a word say father.
So, coming to these two pictures I will simply ask you can you recognize these two pictures?
The answer will be obviously yes.
All of you hopefully will say that the picture on the left is a cave painting in Aujanta caves and picture on the right is the picture of parliament building in New Delhi.
So, we have been able to recognize these two pictures or we will be able to recognize thousands of such pictures which are shown to us or we will be able to understand thousands of words which we will hear or thousands of sentences we will hear.
But the question is we have been able to understand we or we have been able to recognize these two pictures that is fine, but the question is how do you recognize it.
So, you find that when we look at any of these two pictures, obviously how do I recognize that the picture on the left is a Ojantha is a painting from Ojantha caves.
The reason is either my parents, my friends they have shown me that these are the paintings from Ojantha cave.
If not maybe I have visited Ojantha cave and there I have seen this painting or maybe I have seen this paintings in textbooks in history books.
right.
In most of the history books such paintings images of such paintings are abundant.
So, I have while seeing those pictures I have unknowingly unintentionally tried to capture certain properties or certain descriptors from this picture.
And using those properties or those pictures I have built a model which is embedded in my pen.
So, next time whenever this painting is shown to me.
I try to find out similar descriptors or similar features and try to and I try to match those features with the model that I have been that is embedded in my brain.
That means, this picture is associated with an a priori knowledge that I have right.
So, using that a priori knowledge I try to recognize or I can recognize this picture very easily.
But suppose the picture on the right.
that has never been shown to me.
I have never seen parliament building or I have never seen an image of parliament building, nobody has shown me, nobody has told me.
So, if this picture on the right is shown to me, I will probably say ok this is some building because I know the buildings look like this, but I will not be able to say that this is parliament building because that association I do not have.
So, how do you recognize or how do you get the description or the features of any event or any object or any picture, we will try to see.
and profit from experience.
So, you find that this definition is very very crisp definition and these three words comprehend, understand and profit from experience.
These three lies at the heart of what is machine learning or what is deep learning.
Or in other words we can also say that the intelligence is the capability to acquire and apply knowledge.
experiencing various events every day each and every day, we are watching new objects every day and through that we are acquiring knowledge.
And then what we are trying to do is we are applying the knowledge that we have acquired through experience and that is what is intelligence.
So, coming back it is almost 2300 years ago or even more than The great philosopher Plato during the who was there during the period 427 to 347 BC, he brought the concept that the abstract ideas are known to us a priori through a mystic connection with the world.
So, you note what Plato said that it is a mystic connection while with the world.
So, the abstract ideas are known to us after.
And of course, this makes sense because otherwise how is it possible that a newly born baby can easily recognize his or her mother right.
So, definitely it is something mystic connection with the world.
And Plato concluded that ability to think is found in a priori knowledge of the concepts.
Right.
So, everything is as per Plato everything is a priori.
So, does it mean that we do not learn anything new?
So, that was something which was missing in what Plato said more than 2300 years ago.
But soon after it was actually Plato's pupil Plato's student who brought in the concept of learning.
He pointed out Aristotle pointed out that, in Plato's concept a very important aspect was missing that is ability to learn or adapt to changing world.
So, all of you know that every day as we experience new and new events, as we see new and new things, we are learning continuously.
So, learning is a never ending process, every day we are learning something new.
So, that is what has been introduced by Platothat is what has been introduced by Aristotle who was Plato's student.
So, coming back again to the machine learning or when we try to recognize something why when we learn some object or some animal or some word how do we learn it let us come to this.
So, either we see an object or an image of an object or maybe we listen to some words to some sentences every day.
So, when you talk about say machine learning or deep learning a very very important application of this deep learning or machine learning is being able to understand or recognize objects that we see or the images that we see or being able to understand.
or comprehend the sentences that we hear.
See when I am someone says go to school, I do not I know that what does the sentence mean go to school.
So, that means, we understand it ok.
So, coming to the machine when the machine is to learn and the machine is to deliver then obviously, these sentences or these images are to be converted into a form which the machine will understand.
So, come to a picture which is shown on the left, apparently it is a picture of a cat all these pictures are actually represented by two dimensional arrays of numerical values and mostly those are integer values.
So, those of you who know about the digital images you know that we talk about pixels, we talk about megapixels 18 megapixel, 40 megapixel, 50 50 megapixel and so on which are actually the power of the camera that we have.
So, these pixels are nothing, but an element in a two dimensional array or an element in a matrix.
So, any image is represented as a two two dimensional matrix which is as shown over here.
So, this is a two dimensional matrix which is part of the image image which is shown on the left.
And, every element in this matrix is normally an integer value and you know that these integer values are 8 bit integer values.
That means, every element in the image every element in this matrix can assume a value from 0 to 255.
And, that is true for a black and white image or a gray level image which does not have any color information.
But, if I have a color image the color image is represented in two planes the red plane green plane and blue plane.
And, each of these planes can be considered as a grayscale image and every pixel in such grayscale images are again 8 bit quantized.
That means, coming to a color image every pixel in a color image will have three components the red component, blue component and green component and each of these components can take values from 0 to 255.
That means, every pixel in a color image is represented by 24 bits.
8 bit per color component.
So, that is how an image is represented in a computer.
Similarly, when it comes to word recognition or voice recognition or speech recognition, then the speech signals are to be represented digitally.
So, what is shown on the right hand side is what you get from the output of a microphone, you know that microphone converts an acoustic signal which is a voice signal or any other sound.
input into an electrical form and then into an electrical waveform.
So, this is the snapshot of an waveform which is output of a microphone and this you get when you utter a pair of words say news items, then the microphone output will be something like this.
And now if we sample these outputs and digitize each of the samples what we get is a time series a sequence of samples in time.
So, this is how a voice signal or a speech signal or a sound signal can be represented in a computer.
So, once I have these representations, next is how do we process these informations so that this processed information can be used by a computer for understanding or for recognizing.
So, for that what we need are the descriptors or the feature vectors.
So, here I am showing you two different pictures, one is the picture of a horse, the other one is the picture of a zebra.
Now, given those two pictures you find that both of them has got two types of properties, one is the shape property, other one is the region property.
So, what is the shape property?
Let us come to the next picture.
If I simply take the boundary of a horse.
or the boundary of a zebra what we get is shape of these two animals.
And here you find that the shapes of these two animals are almost similar.
So, by looking at the shape only possibly I will not be able to say which one is horse or which one is zebra right.
But maybe that using this shape information I will be able to say that ok this is either a horse or a zebra it is not a bird ok.
So, this is a kind of information that I will be possiblybe generated, I will be able to generate given the shape information that this might be a horse or a zebra, but definitely this is not a bird.
But to distinguish between horse and zebra what I need is the additional information that is what is the property of the region bounded within this boundary.
So, if you look at On the left where I have a horse or on the right where I have a zebra, in case of a horse the color of the bounded region and the texture of the bounded region is totally different from the color and texture of the bounded region corresponding to a zebra.
So, in case of a zebra I have black and white stripes which usually I do not have in case of a horse.
So, if I want to distinguish between a horse and a zebra or given an image if one I have to recognize that which is the image of a horse or which is the image of a zebra, then possibly I will make use of this information both the shape information as well as the region information or the region properties which will help me to identify that which figure is the figure of a horse or which figure is the figure of a zebra.
So, we need both these kind of informations the shape information and the boundary information there are different shape informations possible.
So, given a shape I will have multiple informations because I may have to distinguish one shape from other.
So, I have to distinguish a rectangle from a circle.
So, the shape information of a rectangle and the shape information of a circle will not be same they will be different.
And this discrimination can be done using multiple number of properties it may not be possible that I will have a single property this has to be done using multiple number of properties.
Similarly, when I will go for color.
Again, from the color I can extract multiple number of properties.
Similarly, when I go for texture for texture I can extract multiple number of properties.
And each of these properties if I can represent in the form of numerical values, then you concatenate all these properties together right.
Say from shape if I extract 5 properties, then the shape information can be represented by of by a vector having 5 elements.
Similarly, color from the color if I extract 10 properties or 10 descriptors describing a color all those 10 descriptors putting together gives me a vector having 10 number of elements.
Similarly, for texture again I can have 10 number of elements.
So, if I concatenate all of them together I get say total 25 elements.
So, that means, a bounded shape a bounded region is described by 25 features or by feature vector having 25 elements.
Now, using these feature vectors I go and go for identification or classification of the objects or classification of the bounded region.
Whether the bounded region is corresponds to a horse or it corresponds to a zebra or it corresponds to a bird, if it is a bird what kind of bird and all these detailed classification can be made once I compute these descriptors or features.
So, the first level of understanding or learning is to find out the features.
So, once I have these features then let us talk about what is machine learning and when I talk about the machine learning then I will come to deep learning.
So, for that let me try to find out.
Let me see that what is machine learning right.
So, what you have obtained so far given an object or given an image.
So, given an image or an object the image is converted into a set of vectorsinto a set of features where the features collected together is forming a feature vector f .
So, you find that if I have say 25 such features or I have a feature vector having 25 elements .
Then, this image is represented by a vector or by a point in a 25 dimensional feature space.
Now, for simplicity let me assume that this f is actually a 2 dimensional feature vector for simplicity.
Obviously, a shape cannot be represented by or any image cannot be represented by 2 dimensional feature vector accurately, but for simplicity I am doing So, if I have this two dimensional feature vector, let me put this feature vector feature components as component f 1 and component f 2.
So, f 1 f 2 gives you a feature vector.
Now if I have images of hordes, they will form a point distribution somewhere over here, say in my two dimensional space.
Similarly, if I have images of zebras, they are very similar right.
So, they can have a set of point cloud somewhere over here.
But if I have images of say apples and I compute the same similar feature vector similar features f 1 and f 2 for apples, they may form a set of points somewhere over here.
Now, why this distribution?
The distribution is because every apple is not identical, every horse is not identical.
every zebra is not identical.
So, for all those different pictures when I compute f 1 and f 2 they will not always give me unique values, but there will be a variation.
And because of this I get distribution of these feature vectors.
Now, what is the advantage of having such a kind of distribution?
The advantage isadvantage of having such a kind of feature vectors or descriptors.
The advantage is the moment I have a feature vector that means, my image is now represented.
by point in the feature space.
And given two points in the feature space, if I find that the distance between the two points is very large, I can immediately say that these two images are not similar they are different.
But given two points like this where the distance is very small, I can immediately say that these two images or these two objects are similar.
So, that is what you get the advantage that you get when I represent these as feature vectors.
Now, given an image.
I can have different types of picture vectors I said as I said the shape, color, texture and many other features.
Given an image I can directly compute a vector from that.
So, an image is typically of size m by n which has got m number of rows and n number of columns.
One way of vectorization is you take every column from this image.
So, I get 1 column having n number of elements, then I take the second column having again n number of m number of elements and concatenate with this.
So, first here I have the first column that is first m number of elements, here I have the second column second m number of elements and so on and that we continue and concatenate with this the last column.
So, you find that this entire image of size m by n is now converted.
2 and 1 dimensional vector, where this vector has m into n number of elements.
So, this whole image is now represented by a single vector.
Now, coming to what is the difference between our conventional machine learning and the deep learning.
In case of conventional machine learning techniques, what used to be that you decide that what are the features or what are the descriptors that are that are to be extracted from the input signal.
whether shape features, color features, texture features and what are those features.
So, accordingly you have a pre-processing technique and this pre-processing technique gives you the feature vectors.
And these feature vectors are inputted to your machine learning algorithms.
And for that we can have different types of machine learning algorithms, statisticalmachine like Bayes rules, Bayes classification rules.
I can have linear or non-linear classifier I can have support vector machines, we will talk about each of these in our later part later lectures.
So, I can have different types of classifiers, even I can have neural networks as classifiers.
So, this feature vectors are inputted to those classification algorithms or machine learning algorithms.
And for training the machine learning algorithm or for training the classifier, what you do is you feed a large number of feature vectors.
taken from known objects that means, the feature vectors I know that from from which object or from which class that feature vector has been computed.
And using this knowledge you try to train your classifiers whether it is neural network, it is base classifier, it is a support vector machine or whatever it is.
And this is something which is very similar to what we do.
Coming to our previous the first example when you are shown the image cave painting, Ajanta cave painting it was also told that it was Ajanta cave painting.
So, I knew what that object is or what that image is.
So, in the same manner you train your machine learning algorithms, but the feature vectors are pre computed by some pre processingmodules.
In deep learning what we do is no I will not have a pre processing module, I will directly input the raw signal to our machine learning algorithm.
So, the machine learning algorithm will not only learn the classes, it will also learn which feature to look for.
And usually these all the machine learning deep learning algorithms that we will be talking about they are all neural networks.
So, effectively what you are doing is you are adding additional layers in your neural network to learn the features as well.
So, typically that is the difference between your traditional machine learning and the deep learning algorithms.
Now, there are two types of deep learning algorithms, one is discriminative learning, one is generative learning.
In case of discriminative learning what you try to do is, once you are shown a set of images or set of objects, you try to discriminate among those objects.
That means, given the image of a dog, I try to say that it is a dog, it is not a cat right.
So, basically as I said that every image or every object If, I represent as a point distribution given by p x in case of discriminative learning or discriminating model given this distribution x p x and given the set of classes say y I try to find out what is the posterior a posteriori probability p of y given x that is given an input what is the probability that it belongs to certain class that is what is discriminative model.
In case of generative model.
So, it ismotivated bywhat was told by Richard Feynman that what I cannot create I do not understand.
That is it is not only that you will be able to classify or discriminate, it you should also be able to recreate what you learn and that is what is your generative model.
So, how do you do this?
You collect a large amount of data in the same domain and then train a model to generate data like it.
We will also talk about this in our future lectures.
Then what are the challenges in deep learning or the challenges in machine learning?
When you look at an object, you can look at object from various angles and from various angles or various viewing angles, it will appear to be different.
So, same object can have different views and from those different views we have to identify the object or we have to classify the objects that is one of the challenge.
They can may be available in different poses There may be illumination variation, there may be inter class variation.
Say for example, here who will say that this is a chair though all these are chair classes.
So, I can also have intra class variation that is another challenge in this deep learning algorithms, but because our deep learning algorithm has to work even in presence of intra class variation.
There may be distortions and occlusions, may be part of the object is visible, the majority of the object is not visible, it might be present in a distorted fashion and so on.
So, those are all challenges of the deep learning algorithm.
And coming to the power of deep learningtechniques what we can achieve is we can even able to synthesize high resolution images and this is what is done through generativemodel.
From a low resolution image, we can get a we can create super resolution image.
Given any sketch of image any sketch of an object I can generate I can synthesize a photograph of that object.
Given a semanticsegmented output I can have a real looking image.
So, all these are possible using the recent modern deep learning techniques.
Similarly, we can alsocompose videos with different tiles.
different styles.
So, all these are possible using modern deep learning techniques and that are multiple applications like in medical image processing, in object recognition, in speech recognition and so on.
So, we will talk about all these different aspects of these deep learning techniques insubsequent lectures of this course.
I hope you will be able to learn the content of these deep learnings and will be able to apply it in real life problems.
Thank you.
Hello, welcome to the NPTEL online certification course on deep learning .
In today's lecture, we will try to find out how do we capture the information or the descriptions of from the signals that we capture from the real world.
In our previous introductory lecture, we had given we have shown you the images of a horse and a zebra.
And then we have told that to differentiate between these two images that is to identify that which is the horse and which is the zebra.
zebra.
We can extract two types of descriptors, one is the shape descriptor that is the shape of the horse and the shape of the zebra and the second one is the region descriptor which means that what is the content of or what is the color intensity and texture of the body of the horse and the body of the zebra.
So, if you look at the shape of these two animals the horse and the zebra.
zebra, you find that the shape boundary is more or less same.
That means, this shape information of the boundary information does not give you the sufficient description or sufficient information by which you can differentiate between a horse and a zebra.
But when you look at the entire figure that is along with the shape information, when we also consider the information of the color, the information of intensity.
the information of texture, then only I can differentiate or I can say that which is the horse and which is the zebra.
So, that makes two things very clear that for images or for objects that we see in the real world, we can have two types of information.
One is the shape information or the boundary information and the other kind of information is the region information which gives you what is the intensity or what is the color or what is the texture.
And, when we combine both these informations that is the shape information, the color information, the intensity information and the texture information all of them together can identify a particular object or a particular animal.
So, in today's lecture what we are going to talk about that how we can obtain the descriptors or features from the signals that we obtain from the real world.
These signals can be visual signals as in the form of images or what we can see through our eyes.
These signals can also be audio signals that we can hear and signals like the speech signals or voice signals using which I can differentiate among different speakers, I can also understand what is being spoken.
The applications can be speaker identification.
speech to text conversion and many such applications.
So, firstly I will talk about the visual signals or how do I extract the different types of features or different types of descriptors from a visual signal.
And these descriptors will can be of two types as we have already said that the descriptors can be obtained from the boundary which tells you what is the boundary property.
It can also be obtained from the region which tells you what is the region property that includes the intensity, that includes the color and that includes the texture properties.
So, firstly let us see that how we can obtain the different types of boundary features or how we can extract the boundary properties.
So, for that I will take a very simple shape .
So, I take a boundary shape .
something of this form, let me change the size of the pen .
So suppose I have a shape of this form which is a closed boundary.
Now, find that though this closed boundary I have shown it as a continuous curve, but you remember that we are talking about the discrete signals or digital signals.
So, this curve is not really a closed curve or a continuous curve rather this curve is consists of a set of discrete points.
So, something like this.
I have a set of points on this discrete curve.
So, one of the ways in which such an arbitrary boundary can be represented is in the form of a polygon.
The way we can represent an arbitrary shaped boundary in the form of a polygon is that we can recursively subdivide that arbitrary shape into a number of segments.
So, the way we can do this is suppose I take two points on this boundary which are at maximum distance.
So, the first division will be that I draw a straight line passing to these points on the boundary which are maximum at maximum distance.
So, once I do that this particular chord or this particular straight line.
sub divides this boundary into two sub boundaries.
One sub boundary is in the upper part.
So, let me call it sub boundary A and the other one is on the lower part let me call it as sub boundary B .
So, at the next level we can again subdivide both these sub boundaries and for that we have to use a criteria.
The criteria can be that I compute the perpendicular distance of different points on these boundary segments from this straight line.
So, I can find out given a point over here, I can try to compute what is the perpendicular distance of this point on this boundary.
I can also compute what is the perpendicular distance of this pointon the boundary from this straight line.
And, I find out a point on this boundary which is at maximum distance from this line segment.
So, I can identify that maybe this is a point on the boundary which is at maximum distance.
So, once I identify this point then this point subdivides this uppersub boundary into two parts.
So, one part is starting from this point to this point, the other part is starting from this point to this point.
So, let me name these points the initial points let me name this as P and Q.
this point is S. So, the next subdivision after doing the next subdivision the vertices of the polygon that I get is something like this.
Similarly coming to the lower part of the boundary maybe this is a point which is at maximum distance from the straight line P Q and in that case this point let me call it point R. So, this point R subdivides this lower part of the boundary segment P Q into two more segments.
1 is P R the other one is R Q.
So, the next level of polygonal representation is by this.
So, you find that up to this I have got a polygon P S Q R. So, this is the polygonal representation at this level of this arbitrary shaped boundary that we had.
So, this process can be repeated recursively.
This may be another.
of this arbitrary shaped boundary.
So, once I get such a polygonal representation from this polygon I can try to extract different types of boundary features simply from the properties of the polygon.
So, this is one way of polygonization of an arbitrary boundary and once I have such polygonal approximation representation from this polygonal representation I can obtain different types of descriptors or.
I can also obtain different types of descriptors from this boundary segments, right.
So, here you find that I have different boundary segments, starting from this vertex to this vertex.
This is one boundary segment from this vertex to this vertex I have another boundary segment from this vertex to this vertex I have another boundary segment.
So, I can have different techniques to find out what is the shape of these different boundary segments.
So, the properties of the polygons as well as the properties of these boundary segments can give me important information about the shape of the boundary.
So, this is one kind of descriptor or one kind of features that we can obtain.
The next kind of features that we can obtain is what is known as signature.
So, to see what is a signature let me take a very simple shape which is a square.
So, I take a square something like this.
What is signature?
Signature is the plot of the distance of different boundary points from the centroid of this shape taken in various directions or in various orientations.
So, as I have to compute or I have to find out the distance of different boundary points from the centroid of the shape in different orientations.
So, I have to have a reference line.
So, let me assume that my reference line is this.
So, this is the centroid and my reference line is this one.
What I do is I compute the distance of the boundary point from the centroid.
which is oriented at an angle theta from the reference line and suppose this distance I call as d theta .
So, what I plot is the distance d theta against theta as theta varies from 0 to .
360 degree ok. And in this case you find that when theta is 0 as my reference line over is over here the distance will be one of the minimas.
So, I start from here and when theta is 45 degree as I have taken a square when theta is 45 degree that means, I am computing the distance of this vertex of the square from the centroid this distance will be maximum.
So, I will have.
a d theta versus theta plot which will be something like this and it will go on up to 360 degree.
So, I will have a minima at 0, I will have a maxima at 45 degrees, I will have another minima at 90 degree, I will have a maxima at 135 degrees and so on.
So, such of d theta versus theta gives you important information about the nature of the boundary or the shape of the boundary.
So, this is a kind of plot which I am getting for a square figure if the figure instead of being square it is something else then naturally naturally the nature of the plot will also be different.
So, the shape of the plot or the nature of the plot also gives me important information about the shape of the boundary.
So, by processing this signature I can also obtain boundary descriptors.
So, this is another way of obtaining the boundary descriptors.
The next type of features that we can obtain is what is known as Fourier descriptor.
So, to obtain the Fourier descriptor I hope all of you know what is a Fourier transformation or Fourier coefficients.
So, to obtain Fourier descriptor I represent the boundary points the points on the boundary as a sequence of say complex numbers.
Let us see how we do it.
So, I assume a two-dimensional space and suppose I have a closed boundary something like this.
As I said before that this boundary consists of a number of discrete points .
So, I take any points say kth point on this boundary segment and this kth point I call it say S k which have two components one is in the x dimension direction and other one is in the y direction.
So, this k will have two components which is x k and I represent this as say complex number.
So, j times y k. So, when I consider all these different points on this boundary, what I get is a sequence of such complex numbers.
So, you remember that all these points are to be represented as a sequence.
So, I get a sequence of complex numbers which is given by this x k is equal to x k plus x k plus times y k where k may vary from say 0 to capital N minus 1 where I have total number of points on this boundary which is capital N. So, once I have this what I can do is this sequence of numbers can be represented by their Fourier coefficients.
So, for that what I have to do is I have to take the Fourier transformation of this sequence of numbers.
So, I had sequence of numbers S k given by X k plus j times Y k, k varying from 0 to capital N minus 1.
And what I do is I take the Fourier transformation and you know the Fourier transformation is given by A u is equal to S k.
e to the power minus j 2 pi u k by n right.
Take the sum of this over k is equal to 0 to capital N minus 1.
And this I will get coefficients I will get for all values of u varying from 0 to capital N minus 1.
So, as I have capital N number of points in my sequence, I will also have capital N number of coefficients.
And the magnitudes of these coefficients represent or give me useful information of the shape of the boundary.
Now, you can also find out that instead of considering all the N number of coefficients, if I consider say lesser number of coefficients, kind of effect I can have.
Suppose I want to consider p number of coefficients that means, I want a u where u varies from 0 to capital P minus 1 where p is less than capital N. Then what kind of effect this truncation of the coefficients you will have.
So, to see the to see this truncation of the effect of the truncation of the coefficients, I can take the inverse Fourier transformation to reconstruct.
the boundary with which I have started for which I have taken the forward transformation.
So, if you do that you find that if we had started with say a square shape something like this.
Suppose this square shape had capital N number of boundary points and from this capital N number of boundary points we have also obtained capital N number of Fourier coefficients.
Then what I have done is.
this capital N number of Fourier coefficients out of this I have considered say first p number of coefficients.
And if you know the Fourier transformation, you know that the low order coefficients Fourier coefficients gives you some information about the trend of the signal, whereas high order Fourier coefficients gives you the detailed information of the signal.
So, it may so happen that if I consider say p equal to 2 that is I consider only first two Fourier coefficients a u where u is equal to 0 and 1 .
So, only with these coefficients I want to reconstruct or find out the inverse Fourier transformation for reconstruction of the boundary .
So, for that my inverse Fourier transformation expression will be S k is equal to a u e to the power g .
2 pi u k upon capital N sum of this over u is equal to 0 to capital P minus 1 and k will vary from 0 to capital N minus 1 .
So, what we are doing is since I had capital N number of points on the boundary, I am reconstructing capital N number of points through this inverse Fourier transformation.
But while doing so, the number of Fourier coefficients that I am considering is not N number of Fourier coefficients, rather p number of Fourier coefficients where p is lesser than n. So, given this if I consider p equal to 2 that is if I consider only the Fourier coefficients A u 0.
and u 1 for reconstruction or in the inverse Fourier transformation, then maybe the kind of shape that I will reconstruct will be something like this, which will be a circular shape.
Because as I said that low order coefficients gives you only trend of the signal.
So, this But, the high order coefficients gives you the details of the signal.
So, in a square shape the details means the presence of all these corners of the vertices.
So, as I have truncated all the high order coefficients this detailed information in the reconstructed signal is lost.
If I increase the value of p say from 2 if I go to 10 where I assume say p n is equal to something like.
say128.
So, against 128 if I take only 10 coefficients I will get slightly better reconstruction.
So, which is like this it is not a perfect square neither a perfectcircle.
So, I get some detailed information present the reconstructed signal.
Only when I consider all these 128 coefficients in the reconstruction or p varies u varies from 0 to 0 to 0.
127 in this inverse Fourier transformation expression, then only I will get back my original shape which is a square.
So, all these Fourier coefficients can also give you some information ofor important information of the boundary of a bound shape of the boundary.
So, over here I have obtained the different types of descriptors that I can get firstly going for a polygonal representation.
When I go for polygonal representation then from the polygon itself I can compute some descriptors or when I go for polygonal representrepresentation then you find that the different vertices of the polygon that I get those vertices break the original boundary into a number of some boundaries.
So, I can also obtain some shape information of all those sub boundaries.
Then I can have what we have talked about is the signature and the nature of the signature or the shape of the signature gives you inform a important information of the boundary.
Then we have also talked about the Fourier descriptor which also gives you the important information of the shape of the boundary.
Now, let me go for some further analysis of the shape which is through statistical moments.
So, what we have said is that say for example, I had an arbitrary shape something of this form and I had a polygonal representation through a polygonal representation a segment of the shape which is this ok.
So, one is I have this edge of the polygon, I also have this segment of the boundary.
So, I can compute the shape of this boundary, I can have different information about the shape of the boundary through statistical moments.
So, how I can do it?
If I just put this shape like this, so I have a shape of this form right, this is a sub or segment of the arbitrary shape.
What I can do is this straight line I rotate So, that I rotate it by an angle theta in the clockwise direction.
So, that this straight line chord or which is the edge one of the edges of the polygon that becomes horizontal.
So, if I do that I will have a boundary something like this.
So, this is a boundary segment.
Using this I can represent it in the form of a function.
So, I can put this as r. So, and this function can be represented as g r. So, what I am doing is I am representing this boundary or the boundary segment as a function g r. Obviously, this will not be a continuous function because as I said before that I actually have a set of discrete points on this boundary segment .
So, if I have if I represent this variable r by set of discrete variables say r i, this boundary is represented by a function g r i.
Now, if I normalize this boundary, normalize it by the area under this segment in that case this g r i is nothing, but a histogram.
So, what I can say is that after normalization this g r i tells me what is the frequency of occurrence of this discrete variable r i in this case.
in this particular scenario.
So, it is a frequency of occurrence.
So, once I have this frequency of occurrence, then I can compute different types of statistical moments.
So, a statistical moment of order k is given by this r i is a minus mu to the power k times pi r i.
where P r i is the frequency of occurrence or probability of occurrence of i take the summation of this over all i.
So, this is a statistical moment of order k. So, I can put it as mu k sorry I will not put it as mu k. So, this one what I will do is instead of mu k.
as I have used mu to represent the mean let me call it say sigma k .
So, sigma k is the statistical moment of order k given this particular distribution.
And you find that for different values of k I can get different types of shape information.
Obviously, in this case mu being the mean mu is nothing, but r i times p r i .
take the sum of this over all i.
So, here you find that if I take the value of k is equal to 2 right.
So, what I get is the variance.
If I take value of k to be equal to 3, I get third order moment.
Second order moment ispopularly known as the variance that tells you what is the spread of this distribution.
If The second order moment is very high that means, the distribution will be of this form, it will be spread higher if the value of sigma square is small my distribution will be something like this.
The third order moment tells you about the skewness or the symmetry of the distribution about its mean.
Similarly, all moments of different orders captures some information of the shape of this distribution or in this particular case.
it captures the shape of this sub segment of this particular boundary .
So, till now what we have covered is the different types of boundary information that we can obtain from a given shape .
So, with this I conclude .
this part of the lecture.
In the next lecture, I will talk about how we can obtain the different region descriptors or the region information including intensity, color and texture.
Thank you.
Hello, welcome to the NPTEL online certification course.
on deep learning.
In the previous lecture, we have discussed about the ways in which we can obtain the boundary descriptor or boundary features of any given shape, which tells you what is the shape of the boundary or capture some information of the shape of the boundary.
So, in the previous class, I showed you these two figures one of the horse and one of the zebra.
And, we have said that though the shape of those two animals horse and the zebra are similar.
So, these shape information is not really sufficient to distinguish between a horse and a zebra.
So, to distinguish between these two animals horse and zebra, we need the description of the shape which is obtained from the boundary of the figures.
In addition, we also need information of what is the color, what is the texture and what is the intensity.
So, these are the descriptors or the features which are known as region descriptors or region features.
So, in the previous class, we have talked about the boundary features, how we can obtain the boundary descriptors or boundary features from any arbitrary shape.
In today's lecture.
I am going to discuss about the region features.
So, how the intensity, the texture and the color that description can be obtained what are the techniques for that.
In addition as we said in the previous class that machine learning or deep learning not necessarily is concerned about only the visual signals, it is also concerned or applicable for understanding the audio signals.
So, for that I also need to understand how we can extract the discriminating features of the discriminating descriptors from the audio signals using which I can different I can have different applications like speech car identification, speech to text conversion and all that.
So, firstly I will talk about the region descriptors, how we can obtain region descriptors.
So, as we said before that when we talk about region descriptors, I am concerned about extraction of three types of information.
One of the information that I want to obtain from the figure is what is the intensity or the intensity profile.
So, this is one of the information of the region that I want to capture or I want to obtain.
The second kind of information that I will also be interested in is what is the texture.
So, texture information is also important to distinguish between two different figures or among different figures.
Similarly, the third kind of information that will be interested to obtain is what is the color of the particular object or the particular region.
So, in region descriptor extraction, we are mainly concerned about these three quantities.
the intensity, the color and the texture.
So, we will talk about what are the different ways in which we can obtain these three different types of informations.
So, I will go to them one by one.
So, how do I obtain the intensity?
You find that over here I have shown a picture and on the right hand side what I have is what is the histogram or intensity histogram.
So, before going into that let me define what is meant by intensity histogram.
We told earlier that if I have a black and white image or a grayscale image like this, then at every point or every pixel in the image, the intensity is quantized by 8 bit binary number.
So, because it is 8 bit number, so I can have an intensity at any pixel varying from 0 to 255.
So, the minimum intensity level or very dark pixel is having an intensity value 0 and a white pixel or having an intensity which is maximum the intensity value is 255.
So, what I say the way a histogram is defined is like this.
If I have take an intensity value say i then histogram h i tells me that how many times this intensity i appears within the given image or in other words h i tells me the number n i that is how many pixels within the image have intensity value h i if i normalize this histogram that means if i put it as h i is equal to n i by capital N, where capital N is the total number of pixels in the given image and n i is the number of pixels having intensity value i.
So, this actually tells you that what is the frequency of occurrence of intensity level i within the given image or this is nothing but what is the probability of a pixel having an intensity value i.
So, when I normalize the histogram.
histogram, the normalized histogram also gives me the intensity probability distribution.
So here you find that in this particular image, the intensity values, most of the pixels are having intensity values which are on the lower side.
So on this side I have intensity 0, on this side I have intensity 255 that is the maximum intensity value and in between the intensity values changes accordingly.
as per this scale.
So, this histogram says that most of the pixels as over here the h i value for i within a small region over here is quite high compared to the frequency over here where the intensity is high, here the intensity is low.
So, for lower intensity the h i is high that indicates that most of the pixels within this image have intensity value which are very low that is.
the image is very dark and that is quite obvious by looking at the nature of this image.
Now, as against this if I take the next image here, here you find that the histogram, the intensity values over here may be something around 70 or so, it is in between 50 and 100, the probability of occurrence.
or the frequency of occurrence of intensity values around 70 is very high compared to other two.
This is as against the previous one, where the intensity values were maximum maximum probability at intensity values near about 0.
And you see that what is the effect of that?
This image appears to be brighter than the previous image that we have shown.
So that clearly says that this histogram gives you very important information.
about the intensity distribution within the given image or within a given region.
So, as we have extracted the shape information or the shape descriptors in our previous class, in this class we are going to describe discuss about the region property extraction and this intensity distribution is one of the important region properties.
And it is clearly shown that this intensity distribution is captured is in is captured within what is known as intensity histogram.
So, based on the nature or shape of the intensity histogram, we can estimate the different descriptors, we can find out different intensity descriptors.
So, a very important approach for intensity distribution extraction is using the histogram shape.
So, similarly over here if I have a color image as we said earlier that for color images we have three different planes right.
One of the plane is a red plane, the other one is green plane and the third one is blue plane.
So, here in this particular image you find that the color image which is given over here.
If I take the histogram of the red plane the histogram shape is something like this.
So, this tells you that how strong or what is the distribution of the strength of the red color component within this image.
This is the histogram of the green component.
So, this tells that what is the distribution of the strength of the green component within this color image.
Similarly, the third one gives you the information of the distribution of a blue component or strength of the blue component within the given image.
So, as we have seen in the previous example with the black and white image or the grayscale image that the histogram shape of the histogram gives you important information about the intensity distribution within the given image.
Similarly, these three colored histograms gives you important information about the color distribution, the nature of the color of the given image.
And, by analyzing these histograms I can update important information, important descriptors of the intensity distribution and important descriptors of the color distribution.
So, how we can do it?
So, the way I will do it is I assume that the histogram So, my intensity is intensity level I put it as r i and the histogram is given by h of r i and this histogram is having different types of shape depending upon the intensity distribution if it is the histogram taken from an intensity image or color distribution where I will have three such histograms.
for each of the color planes.
Now, once I have this histogram and if the histogram is normalized that means, it gives you the frequency of occurrence of an intensity value r i which is indicated by h r i.
So, in that case this shape information can also be captured through different statistical moments that we have said earlier.
So, to capture that different statistical moments firstly what I have compute up to compute is what is mu that is the mean of the intensity values.
which is nothing but r i times h of r i where h r i is the frequency of occurrence of the intensity value r i take the summation of this over all i.
So, that gives you what is the mean intensity value.
And once I have this mean intensity value then I can compute the statistical moment of order k which I write as sigma k which is given by r i.
minus mu to the power k times h of r i take the summation of this over all i.
So, this is my statistical moment of order k and as we said earlier this statistical moment of order k gives you different shape information, information about the shape of the histogram or how the intensity varies, what is the variation of intensity or what is the variation of different color components within the given image.
And, we said earlier that if k is equal to 2 this is nothing but what is variance, the variance tells you what is the spread of the histogram, whether it is a wide or a narrow histogram.
If k equal to 3, it tells you the skewness of the histogram that means whether the histogram is symmetric about the mean or it is asymmetric about the mean.
If it is asymmetric how the asymmetricity is which is captured by the third order moment.
Similarly fourth order, fifth order, all higher order moments gives you some information about the shape of the histogram.
And, the shape of the histogram if it is histogram of an intensity image or black and white image tells you useful information about the nature of intensity distribution.
And, if it is for color image obviously, for color image I have to compute this histogram for all the three components the red component, green component and blue component.
And, all of them tells you useful information about how the green component varies within the image, how the red component varies in within the image or how the blue component varies in the image.
So, I can have different color information from that.
So, this in histograms or moments from the shape of the histogram tells you useful information about the intensity of the region, it tells you useful information about the color within the region.
The next type of region descriptors that we are going to talk about is the texture descriptors.
This is also a region descriptor along with intensity and color.
So, what is a texture descriptor?
Let us look at this figure.
The image which is shown on the left, this is a texture image though we do not have a solid definition, a convincing definition of a texture, but it tells you the way the intensity varies within the given image and it follows certain pattern and it may not always be possible to.
represent this pattern through a formal definition.
However, earlier we have said that an image is nothing but a two dimensional array of integer values, where normally in an image and intensity is represented by an 8 bit binary number.
So, an intensity value varies from 0 to 255.
So, given that if I take a small rectangular or square area within this texture image.
This is nothing but a two dimensional array or a matrix of integer numbers which is as shown over here.
So, given this and texture image is also nothing but a matrix of two dimensional matrix of such integer numbers.
So, given this matrix I have to compute or I have to extract the texture features where the texture is nothing but variation of intensity values at a regular or semi regular manner.
So, how we can do it?
So, one of the ways in which this texture information can be obtained in the pixel domain is by using something known as a co-occurrence matrix.
So, let us see that what this co-occurrence matrix means.
Co-occurrence matrix says that given two intensity values say i and j, i is one intensity value, j is another intensity value.
So, it says that how these two intensity values co-occur within the given image.
So, when I say co-occur that is two intensity values i and j, the pixels having intensity values i and j also has to follow certain geometric or locationalconstant.
So, I put this location information in the form of a vector say p or in the form of a parameter p, where this p will consist of two components.
one is L and the other one is theta.
So, by this what I want to say is if I have 2 pixels say A having an intensity value say I and another pixel say B having an intensity value J, then the pixels A and B will be separated following this positional parameter.
That means the distance between the pixel A and B will be L.
And, the orientation of the line joining these 2 pixels a and b will be theta with say horizontal axis.
So, given this and an image of this form which obviously, I have shown by a very small square matrix having intensity values in this case intensity values from minimum intensity value is 0 maximum intensity value is 15.
15.
That means it is basically a 4 bit quantization.
So, intensity values of every pixel varies from 0 to 15.
I am not considering up to 255 because there my co-occurrence matrix size will be very high.
And I want to find out the co-occurrence matrix or for a particular L theta pair.
And for this case, let me assume that value of L is equal to 1 and value of theta is equal to 45 degree.
ok. And I also put say I intensity I is equal to say 4 and intensity j is equal to 8.
So, given this what I want to compute is this co-occurrence matrix I will put it as a matrix A, I want to compute the element value A i j.
where i may vary from 0 to 15 as I have said that the minimum intensity value in this image is 0 the maximum intensity value is 15.
Similarly, j is also varying from 0 to 15.
So, effectively I have a co-occurs matrix A of dimension 1516 by 16 varying from 0 to 15 for each of i and j.
So, as I said earlier that I assume value of i to be is equal to 4 and value of j to b is equal to 8 and this follows a positional constant p, where p is given by L theta pair having L is equal to 1 and theta is equal to 45 degree.
So, basically what I want to compute is I want to compute the number of occurrences of 2 pixels, the first pixel having intensity value 4.
what are the values at these two locations, how many such occurrences I have within this given image.
So, if you scan this image you will find that this is one such occurrence ok.
If you look at this is another such occurrence, do I have any more?
This is one such occurrence I think that is all.
So, you find there are 3 occurrences of pair 4, 8 intensity pair 4, 8 following this positional constant p within the given image.
So, the content at location 4, 8 will be equal to 3.
So, my co-occurrence matrix is a matrix A having 16 elements starting from 0 to 15 horizontally.
0 to 15 vertically and at location 4, 8 the value is 3 indicating that this pair of intensity values 4, 8 following this positional constant p occurs 3 times in this given image and that I compute for all values of i and j.
So, effectively what it gives me is the number of occurrences different pairs of intensity values, how many times they occur within the given image for a given p. So, if I vary p, obviously I will have different number of such co-occurrence matrices.
Now, if I normalize this co-occurrence matrix, then what I basically get is a joint distribution, right.
That is the, what is the frequency of occurrence of ijth pair of intensity values following this positional constant p.
occurring in the given events.
So, it is the frequency of occurrence which is nothing but the probability distribution of the pair of intensity values.
So, once I have this probability distribution then I can compute different types of features from such a matrix.
One of the feature is what is the maximally occurring pair of intensity values.
So, that is what is the maximum probability.
So, this C i j represents the normalized matrix A i j.
So, one of the property that I can extract is what is the maximally occurring pair of intensity values i j.
The other one that I can compute is what is known as element difference moment of order k which is given by i minus j to the power k C i j take the summation over all values of i and j.
What is the significance of this?
The element difference moment you find that if the larger values of C i j are concentrated around the diagonal, around the main diagonal of the co-occurrence matrix or normalized co-occurrence matrix C i j, then the value of element difference moment will be very low.
because, of this factor i minus j to the power k. Similarly, I can have the opposite one which is inverse element difference moment.
So, which is c i j upon a i minus j to the power k. So, this will have an effect which is just opposite to the element difference moment.
So, when element difference moment is large, inverse element difference moment will be low.
I can also have a measure of uniformity.
So, you find that the sum of c i j square will be maximum if all the elements of this co-occurrence matrix c i j they are equal that tells me what is the uniformity.
Similarly, it also tells me that what is the entropy right.
So, this entropy will be maximum if the contents c i j are maximally random.
So, all these different types of features or the properties that can be computed from the co-occurrence matrix .
that we have just computed.
So, this is what you obtain from the raw image, the raw texture image itself.
We can also obtain the features in the frequency domain.
These are the features that we can have in the pixel domain in the raw image.
So, if I want to compute features in the frequency domain, there are different transformations that can be applied on the given texture image to transform into frequency domain and I can compute.
the different types of descriptors in the frequency domain.
So, for texture images the number of transformations, the different transformations which have been very very popular to extract texture features are one is weblet transformation and the other one is Gabor transformation.
So, what you do in case of Weibullet transformation is this breaks the original signal into different frequency sub bands.
And then you compute the energies in the different sub bands and put those energies of different sub bands as different features.
In case of Gabor transformation, this is very interesting Gabor transformation is nothing but a filter which is cosine modulated Gaussian envelope and that can be oriented in various directions.
So, here as it is as it is cosine modulatedGaussian.
So, I have two parameters over there.
One is what is the variance of the Gaussian that tells you about the scale.
And, because it is cosine modulated, what is the cosine, what is the frequency of that cosine signal?
And, the other parameter that I get, because it can be oriented, what is the orientation angle?
So, I can have three different parameters for Gabor filters, frequency, scale and orientation.
And, I can have different filter coefficients by varying these three outputs.
So, I can generate an array of filtered signal outputs.
And, for each of these outputs, I can compute the energy and put the energies in the form of a feature vector.
So, that is what can be done in the transformation domain as well.
So, I can have components in the frequent in the spatial domain, I can also have features extracted in the transformation domain or in the frequency domain.
So, all these what I have done is for the visual signals like images or what we can see.
But as we have seen said before that the frequency extraction or feature extraction.
is not only required for the visual information, it is also for the audio information.
So, in case of an audio, an audio signal whatever I am speaking, audio signal is generated by capturing this information by a microphone.
And if you check with a CRO, what is the output of the microphone that is nothing but a signal of this form, something like this.
This is the output of the microphone.
So, for audio signal feature extraction what is done is this output is passed through a digitizer.
So, through a digitizer I get a sequence of samples and from the sequence of samples I can compute different types of features right and all those features can be used as descriptors to be used for speech recognition for speech recognition for speech to text conversion and all that.
So, even an audio signal can be represented in the form of a vector.
So, the different types of vector representation that we can have is one of them is a well known linear predictive coding or LPC coefficients.
The other kind of feature vectors which has become very popular and very powerful is what is known as MFCC or male frequency capstrum coefficients.
So, this shows what are the steps.
for computation of MFCC.
First is you take the Fourier transformation of the signal samples that you have or speech samples that you have, then you convert these frequency coefficients the Fourier coefficients into male frequency coefficients.
This is required because our auditory system is not that sensitive to high frequency components, but it is very sensitive to low frequency components, right.
So, that is what is known as this.
conversion to male frequencies, then after that you take the logarithm of male frequency coefficients.
Why do you need logarithm?
Because again our auditory system is not that sensitive to loud signals, but it is very sensitive to the signals which are not so loud.
So, you get a logarithm you have to perform a logarithmic transformation.
And the output of this log operation.
you take the discrete Fourier transformation of that and the discrete Fourier transformation coefficients are nothing but your MFCC coefficients.
And using this MFCC coefficients, I can discriminate among different speakers, I can identify the different spoken words, okay.
So, all these that have talked about is regarding or traditional machine learning approaches.
So, what is that?
In traditional machine learning approach, the system is like this.
I have input raw signal which passes through a feature extracted block and these features are fed to the machine learning algorithms, where this machine learning algorithm has some parameter set of parameters theta.
And using this machine learning algorithm you take decision on this input signal what it is.
But when you talk about today's deep learning algorithm, this feature extraction block is absent.
So, what you have is I have this machine learning algorithm again with some set of parameters theta and raw signal is fed directly to this machine and output of the machine is the decision that you take from this signal.
So, even for deep learning, you can see that I also need to represent the input signal as vector or set of vectors.
So, how do I represent that that input signal as a set of vectors?
So, for that let me assume again from the visual domain that I have an image and I take a small segment of the image or 2 by 3 image and this image may be the pixel intensities may be 10, 15, 12, 9, 8, 10, 11, 10 again 3, 12, 6 something like this.
So, I need to convert this image itself into a form of vector.
How we can do it?
Take each column of this image and concatenate all the columns together to form a single vector.
So, what you are doing is from a matrix I am converting this to a vector.
So, in this case my vector will be 10, 9, 3, 4, 5, 6, that is the first column, then I take the next column it will be 15, 8, 12, then I take the third column again it will be 12, 10, 6.
So, you find that from this 3 by 3 matrix I have made a one dimensional vector having 9 different components.
And this vector is fed to the machine learning algorithm whether it is during training of the machine.
or during the testing testing phase.
And that is what you do in case of deep learning algorithms, where we expect in case of machine learning, traditional machine learning, the features are to be decided by the user or the human being accordingly I have to have feature extraction algorithms and then the machine learning comes into picture.
In case of deep learning, we expect that no, we will not tell what features to analyze for taking a particular decision.
But, let the machine learn the features also.
So, during training phase what we say is you feed the input raw signal in the form of vector to the machine and tell the machine what this signal is and based on this.
So, that is basically you are trying to supervise the machine to learn the features from the signal and not only the features it also learns to take the decision or to learns to describe those signals or those features.
So, starting from feature extraction to decision making this end to end application is done by machine and that is what is done in deep learning.
So, you see that in today's lecture we have talked about the different types of features both from the visual signal domain as well as from audio signal domain.
How to extract those different feature vectors?
In case of traditional machine learning, in case of deep learning given a raw signal in the form of a raw image or this is also applicable in case of audio signals.
So, I have given audio signal inputs in the form of sequence of samples, I can form vectors out of it following window operation.
So, given a raw signal how we can obtain or how we can vectorize that raw signal.
So, next class onwards.
we will not discuss about the descriptor or how do you obtain a descriptor of a given signal.
I will assume that given any signal it is represented as a vector.
So, the input to my machine will be a vector and everything that we do whether it is learning or testing decision making everything has to be done on that vector.
Thank you.
