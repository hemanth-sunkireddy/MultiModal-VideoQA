{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f1c1376",
   "metadata": {},
   "source": [
    "* From this notebook, all miniLM, mpnet and QWEN models scores can be computed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d657e91d",
   "metadata": {},
   "source": [
    "Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1bfbf162",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import ffmpeg\n",
    "import os\n",
    "import faiss\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import re\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import csv\n",
    "\n",
    "# Paths\n",
    "VIDEO_DIR = \"Data/Videos\"             # Folder containing input video files\n",
    "AUDIO_DIR = \"Data/Audios\"             # Folder to store extracted audio files\n",
    "CHUNK_DIR = \"Data/Audio-Chunks\"       # Folder to save audio chunks after VAD\n",
    "\n",
    "# Create output directories if they don't exist\n",
    "os.makedirs(AUDIO_DIR, exist_ok=True)\n",
    "os.makedirs(CHUNK_DIR, exist_ok=True)\n",
    "\n",
    "# Audio processing\n",
    "TARGET_SAMPLE_RATE = 16000  # or 32000 Hz depending on your use case\n",
    "\n",
    "# VAD settings\n",
    "MIN_CHUNK_DURATION_SEC = 30  # Minimum duration for an audio chunk\n",
    "USE_ONNX_MODEL = False      # Set True to use ONNX version of Silero VAD\n",
    "\n",
    "from silero_vad import (\n",
    "    load_silero_vad, read_audio, get_speech_timestamps, \n",
    "    save_audio, VADIterator\n",
    ")\n",
    "\n",
    "faiss_index = faiss.read_index(\"Data/sentence_embeddings.index\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a321f47e",
   "metadata": {},
   "source": [
    "### Model for Semantic Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "554479d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Sentence-BERT model\n",
    "# model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')\n",
    "# model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "model = SentenceTransformer(\"Qwen/Qwen3-Embedding-0.6B\")\n",
    "# model = SentenceTransformer(\"Alibaba-NLP/gte-Qwen2-7B-instruct\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d41c6490",
   "metadata": {},
   "source": [
    "## Video to Audio Conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d21fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through all .mp4 files in the input folder\n",
    "for filename in os.listdir(VIDEO_DIR):\n",
    "    if filename.endswith(\".mp4\"):\n",
    "        input_path = os.path.join(VIDEO_DIR, filename)\n",
    "        output_path = os.path.join(AUDIO_DIR, filename.replace(\".mp4\", \".wav\"))\n",
    "\n",
    "        print(f\"Processing: {input_path} -> {output_path}\")\n",
    "        \n",
    "        # Extract audio\n",
    "        input_video = ffmpeg.input(input_path)\n",
    "        output_audio = ffmpeg.output(input_video.audio, output_path, ac=1, ar=TARGET_SAMPLE_RATE)\n",
    "        ffmpeg.run(output_audio, overwrite_output=True)\n",
    "        \n",
    "        # Probe the generated audio file for details\n",
    "        audio_info = ffmpeg.probe(output_path, v=\"error\", select_streams=\"a\", show_entries=\"stream=codec_name,codec_type,sample_rate,channels,bit_rate,duration\")\n",
    "        \n",
    "        codec_name = audio_info['streams'][0]['codec_name']\n",
    "        sample_rate = int(audio_info['streams'][0]['sample_rate'])\n",
    "        channels = int(audio_info['streams'][0]['channels'])\n",
    "        bit_rate = audio_info['streams'][0].get('bit_rate', 'N/A')\n",
    "        duration_sec = float(audio_info['streams'][0]['duration'])\n",
    "        duration_ms = duration_sec * 1000\n",
    "        \n",
    "        print(f\"Audio extracted: {output_path}\")\n",
    "        # print(f\"Codec: {codec_name}, Sample Rate: {sample_rate} Hz, Channels: {channels}, Bit Rate: {bit_rate}, Duration: {duration_ms} ms\\n\")\n",
    "\n",
    "print(\"Video to Audio converted successfully!.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be5d8a67",
   "metadata": {},
   "source": [
    "Voice Activity Detection Algorithm on Audio Files - Converting into Smaller Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "184e3aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Silero VAD model\n",
    "model = load_silero_vad(onnx=USE_ONNX_MODEL)\n",
    "\n",
    "def process_audio_file(audio_path, output_chunk_dir):\n",
    "    \"\"\"Process an audio file, split it into chunks, and save them.\"\"\"\n",
    "    wav = read_audio(audio_path, sampling_rate=TARGET_SAMPLE_RATE)\n",
    "    speech_timestamps = get_speech_timestamps(\n",
    "        wav, model, sampling_rate=TARGET_SAMPLE_RATE, return_seconds=True\n",
    "    )\n",
    "    \n",
    "    # Format timestamps to 4 decimal places\n",
    "    for segment in speech_timestamps:\n",
    "        segment['start'] = float(f\"{segment['start']:.4f}\")\n",
    "        segment['end'] = float(f\"{segment['end']:.4f}\")\n",
    "    \n",
    "    vad_iterator = VADIterator(model, sampling_rate=TARGET_SAMPLE_RATE)\n",
    "    chunks = []\n",
    "    current_chunk_start = 0\n",
    "    \n",
    "    for segment in speech_timestamps:\n",
    "        start, end = segment['start'], segment['end']\n",
    "        if (end - current_chunk_start) >= MIN_CHUNK_DURATION_SEC:\n",
    "            chunk_wav = wav[int(current_chunk_start * TARGET_SAMPLE_RATE):int(end * TARGET_SAMPLE_RATE)]\n",
    "            chunk_path = os.path.join(output_chunk_dir, f\"{len(chunks) + 1}.wav\")\n",
    "            save_audio(chunk_path, chunk_wav, sampling_rate=TARGET_SAMPLE_RATE)\n",
    "            chunks.append((current_chunk_start, end, chunk_wav))\n",
    "            current_chunk_start = end\n",
    "    \n",
    "    # Save the last chunk if necessary\n",
    "    if current_chunk_start < speech_timestamps[-1]['end']:\n",
    "        chunk_wav = wav[int(current_chunk_start * TARGET_SAMPLE_RATE):]\n",
    "        chunk_path = os.path.join(output_chunk_dir, f\"{len(chunks) + 1}.wav\")\n",
    "        save_audio(chunk_path, chunk_wav, sampling_rate=TARGET_SAMPLE_RATE)\n",
    "        chunks.append((current_chunk_start, speech_timestamps[-1]['end'], chunk_wav))\n",
    "    \n",
    "    vad_iterator.reset_states()\n",
    "    print(f\"Processed {audio_path}, saved chunks in {output_chunk_dir}\")\n",
    "\n",
    "def process_all_audio_files():\n",
    "    \"\"\"Process all .wav files in the main audio folder and save their chunks.\"\"\"\n",
    "    if not os.path.exists(AUDIO_DIR):\n",
    "        print(f\"Audio folder '{AUDIO_DIR}' does not exist.\")\n",
    "        return\n",
    "    \n",
    "    for file_name in sorted(os.listdir(AUDIO_DIR)):\n",
    "        if file_name.endswith(\".wav\"):\n",
    "            audio_path = os.path.join(AUDIO_DIR, file_name)\n",
    "            audio_id = os.path.splitext(file_name)[0]  # Extract the number without extension\n",
    "            output_chunk_dir = os.path.join(CHUNK_DIR, audio_id)\n",
    "            process_audio_file(audio_path, output_chunk_dir)\n",
    "\n",
    "# Process VAD on Audio Files.\n",
    "process_all_audio_files()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e7588d",
   "metadata": {},
   "source": [
    "## SRT to Sentences generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eba7e8ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing SRT files: 100%|██████████| 26/26 [00:00<00:00, 1447.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding sentences into embeddings...\n",
      "\n",
      "Embeddings created for 2744 sentences from 26 SRT files.\n",
      "FAISS index saved as 'Data/sentence_embeddings.index'\n",
      "Metadata saved as 'Data/srt-embedding-metadata.tsv'\n",
      "Sentences saved as 'Data/sentences.txt'\n"
     ]
    }
   ],
   "source": [
    "# Function to extract combined sentences with timestamps from .srt file\n",
    "def extract_sentences_from_srt(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    sentences = []\n",
    "    timestamps = []\n",
    "    current_sentence = \"\"\n",
    "    current_timestamp = \"\"\n",
    "\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "\n",
    "        # Check for timestamp lines\n",
    "        timestamp_match = re.match(r'(\\d{2}:\\d{2}:\\d{2}[.,]\\d{3}) --> (\\d{2}:\\d{2}:\\d{2}[.,]\\d{3})', line)\n",
    "        if timestamp_match:\n",
    "            current_timestamp = timestamp_match.group(1).replace(',', '.') + ' --> ' + timestamp_match.group(2).replace(',', '.')\n",
    "            continue\n",
    "\n",
    "        # Skip empty lines and cue identifiers\n",
    "        if not line or line.isdigit():\n",
    "            continue\n",
    "\n",
    "        # Add line to current sentence\n",
    "        current_sentence += \" \" + line if current_sentence else line\n",
    "\n",
    "        # If sentence ends, save it\n",
    "        if re.search(r'[.!?]$', line):\n",
    "            sentences.append(current_sentence.strip())\n",
    "            timestamps.append(current_timestamp)\n",
    "            current_sentence = \"\"\n",
    "            current_timestamp = \"\"\n",
    "\n",
    "    return sentences, timestamps\n",
    "\n",
    "# Directory containing SRT files\n",
    "srt_directory = 'Data/SRT-Files'\n",
    "\n",
    "# Initialize lists for all sentences, timestamps, and filenames\n",
    "all_sentences = []\n",
    "all_timestamps = []\n",
    "all_filenames = []\n",
    "\n",
    "# Process all SRT files with tqdm\n",
    "srt_files = [f for f in os.listdir(srt_directory) if f.endswith('.srt')]\n",
    "for file_name in tqdm(srt_files, desc=\"Processing SRT files\"):\n",
    "    file_path = os.path.join(srt_directory, file_name)\n",
    "    sentences, timestamps = extract_sentences_from_srt(file_path)\n",
    "    all_sentences.extend(sentences)\n",
    "    all_timestamps.extend(timestamps)\n",
    "    all_filenames.extend([file_name] * len(sentences))  # associate each sentence with its file\n",
    "\n",
    "print(\"Encoding sentences into embeddings...\")\n",
    "# Uncomment for all miniLM and all mpnet.\n",
    "# sentence_embeddings = np.array(\n",
    "#     model.encode(all_sentences) \n",
    "# ).astype('float32')\n",
    "\n",
    "sentence_embeddings = model.encode(all_sentences)\n",
    "\n",
    "# Create FAISS index (use Inner Product for cosine similarity)\n",
    "embedding_dimension = sentence_embeddings.shape[1]\n",
    "faiss_index = faiss.IndexFlatIP(embedding_dimension)\n",
    "faiss_index.add(sentence_embeddings)\n",
    "\n",
    "# Save FAISS index\n",
    "faiss.write_index(faiss_index, \"Data/sentence_embeddings.index\")\n",
    "\n",
    "# Save metadata to file\n",
    "metadata_file = 'Data/srt-embedding-metadata.tsv'\n",
    "with open(metadata_file, 'w', encoding='utf-8') as file:\n",
    "    file.write(\"filename\\ttimestamp\\tsentence\\n\")\n",
    "    for fname, timestamp, sentence in zip(all_filenames, all_timestamps, all_sentences):\n",
    "        clean_sentence = sentence.replace('\\t', ' ').replace('\\n', ' ')\n",
    "        file.write(f\"{fname}\\t{timestamp}\\t{clean_sentence}\\n\")\n",
    "\n",
    "# Save sentences to a text file\n",
    "sentences_file = 'Data/sentences.txt'\n",
    "with open(sentences_file, 'w', encoding='utf-8') as file:\n",
    "    for sentence in all_sentences:\n",
    "        file.write(sentence.strip().replace('\\n', ' ') + '\\n')\n",
    "\n",
    "# Summary\n",
    "print(f\"\\nEmbeddings created for {len(all_sentences)} sentences from {len(srt_files)} SRT files.\")\n",
    "print(\"FAISS index saved as 'Data/sentence_embeddings.index'\")\n",
    "print(f\"Metadata saved as '{metadata_file}'\")\n",
    "print(f\"Sentences saved as '{sentences_file}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f406234c",
   "metadata": {},
   "source": [
    "## Finding Related Sentences to a Question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5016a7f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Related Sentences:  79\n",
      "Question:  How can you tell if your model is overfitting without a test set?\n",
      "Model: all-mini-LM\n",
      "\n",
      " Related Sentences:\n",
      "- We'll say later that if my model is overparameterized, then it doesn't do very well in the test data. - 0.7558\n",
      "- And it's an important way to figure out whether my model is overparameterized or not. - 0.6929\n",
      "- because the model is too complex to the data, therefore it's overfitting. - 0.6686\n",
      "- And max features also can help with the overfitting because it can make the model less flexible by looking at the less number of features when we have so many features. - 0.6237\n",
      "- However, the test error will go down in the beginning and then at some point it will start going up again as the model complexity is increased. - 0.6168\n",
      "- And we're going to also talk about how well my model predicts on unseen data. - 0.6105\n",
      "- So around k equals 21, the optimal value happens and the test error is minimized, whereas it can go up if we keep increasing the model complexity. - 0.6044\n",
      "- Alright, so let's talk about how well my model fits. - 0.6004\n",
      "- In that case you may have already good test error for the simple model as well like this. - 0.5842\n",
      "- Instead of using all of them to fit the model, we're going to set aside some data. - 0.5819\n",
      "- with the already fitted model and dot predict and supply test data instead this time and this will give Y prediction from the test data. - 0.5799\n",
      "- Initially, the model doesn't predict very well, so there will be some error between the prediction and target variable. - 0.5743\n",
      "- As you saw previously that the training error goes down as the model complexity increases, test error goes down in the beginning but then it has an optimal value and it goes up again as the model complexity increases. - 0.5716\n",
      "- Or if the data were pretty homogeneous and your model is doing well then train error and test error could be similar value. - 0.5705\n",
      "- And from the summary table values, what gives an idea of how my model fits. - 0.5700\n",
      "- and the test set has feature test and label test. - 0.5670\n",
      "- and where we stop adding more terms to the model by monitoring train and tester error, and we also talked about the bias-variance trade-off principle. - 0.5665\n",
      "- We can define a benchmark model, say y equals y mean, and then we can compare how good is my error from my model. - 0.5661\n",
      "- And we'll also discuss how well my model fits. - 0.5656\n",
      "- You can have a look at the supplemental note but this is the result and according to this the test error is a sum of this variance of the model and bias scaled of the model. - 0.5651\n",
      "- These are metrics for how well the model fits. - 0.5648\n",
      "- They are very easy to overfit so we're gonna talk about some strategies to prevent overfitting. - 0.5647\n",
      "- Decision trees are very easy to overfit, so to mitigate we talked about all the stopping last time. - 0.5622\n",
      "- Our optimal value k equals 7 and as you can see the test accuracy dropped very sharply as we increase the number of features that are included in the model. - 0.5614\n",
      "- So if you want to make the model less flexible, so less overfit, then increase this number. - 0.5611\n",
      "- So this can be a good quantity that measures how my model fits compared to my null model. - 0.5608\n",
      "- As you can see here, boosting algorithm can have overfitting as well if the running rate is too big and number of trees are too big as well. - 0.5601\n",
      "- And I'm gonna call my object clf this time, and then if you want to get an accuracy, so this score uses accuracy by default, you can do fitted model.score and throw your test data and it's going to give some result. - 0.5583\n",
      "- Let's say we had some data that look like this and then we were to fit the data with the simple model and maybe we can also fit with the more complex model that tends to have a lower error so lower bias However, if we chose different data set, like this for example, then if we fit the simple model again, they will be very similar. - 0.5536\n",
      "- in machine learning community validation error is more used term for the data set that's set aside for the purpose of testing while you're training the model. - 0.5506\n",
      "- So with that in mind, let's see what happens if we throw all the features into the model and fit to the data. - 0.5502\n",
      "- So last time we talked about some ways to prevent overfitting in decision trees. - 0.5482\n",
      "- And this lambda helped the model to learn slowly so that we can avoid overfitting. - 0.5460\n",
      "- So using the train set, we're gonna fit the model. - 0.5451\n",
      "- For simple linear regression, this may not happen, but as you might see later, in a more complex model, sometimes the model can fit worse than the baseline. - 0.5440\n",
      "- So it's very common that the test error is a slightly larger than the train error. - 0.5413\n",
      "- And this red curve is a bagging test error. - 0.5369\n",
      "- So the most direct way to prevent overfitting in the decision tree is a max depth. - 0.5367\n",
      "- So if you want to compare apple to apple how my new model is doing in terms of the error, you can just directly calculate RSS for our new model. - 0.5346\n",
      "- And sometimes the simple model fits well to the data already. - 0.5317\n",
      "- You can also see the out-of-bag test error. - 0.5300\n",
      "- So we had a data set that we have both feature and label and then we set aside some portion of this data and called it test data. - 0.5299\n",
      "- But let's think that my model is so good that all the data points are on my model's line. - 0.5291\n",
      "- So this strategy can be effective for preventing overfitting, but it doesn't guarantee that the performance of the tree will be better. - 0.5268\n",
      "- Previously we talked about the trees have a problem that they are weak learner and they can overfit very easily. - 0.5265\n",
      "- You can think about like putting different data set into bag and then make the model trained on this bag of the data. - 0.5243\n",
      "- If you are not convinced by that, the model's assumption, then maybe we can use bootstrapping method. - 0.5238\n",
      "- And lastly, we talked about how to measure the error for training data and test data and how to compare them. - 0.5236\n",
      "- Before we build a model, let's have a qualitative inspection. - 0.5233\n",
      "- Sometimes if the model is not very good, then you may encounter this case. - 0.5232\n",
      "- So, as a conclusion, which models to use, we recommend you inspect the data first and then pick the method that's likely to be better suited for the data. - 0.5230\n",
      "- Then we can test on this the rest of the data that we didn't select to train on. - 0.5229\n",
      "- is 1 over 1 minus R squared value of this fitting and this fitting is actually not fitting the target variable Y but fitting that variable Xi using all other variables. - 0.5223\n",
      "- So after fitting this stump model, using this stump model, and here is that, and then we compare how much accurate it is. - 0.5217\n",
      "- or they can build machine learning models and do the testing on those data or build a system. - 0.5213\n",
      "- And then the key idea we're going to discuss is that when the model complexity increases by adding more features, it can fit the data better, but it can also introduce some other problems. - 0.5210\n",
      "- Okay, so the behavior of test header that goes down first and goes up later as we increase the model's flexibility can be explained by bias-variance trade-off. - 0.5205\n",
      "- You may have to think about Occam's Razor principle, which tells that if the model performances are similar, the simpler model is always better. - 0.5188\n",
      "- And this error can be used to tweak the model to have a better prediction next iteration, and over this iteration, the model becomes more accurate. - 0.5185\n",
      "- Training the logistic regression on the same data set and measuring the training time can give some comparison. - 0.5166\n",
      "- And if the parameters for the parametric model is not optimized, then this prediction value will be far away from the target. - 0.5164\n",
      "- The another extreme case is that my model is actually just as good as my null model y equals y mean. - 0.5164\n",
      "- Ok, so let's talk about how we measure the error from the test data and the training data and how to compare them. - 0.5162\n",
      "- Obviously when you see the model fitness The model fitness will go up and up as you add more model complexity. - 0.5145\n",
      "- One idea might be maybe we can train our models on different subsets of data. - 0.5140\n",
      "- So we talked about this popular error measure so we're going to use them or one of them and let's say we have original training data that we used to use to fit the model. - 0.5117\n",
      "- And it will give the result that which model hyperparameter gave the best result. - 0.5116\n",
      "- So you remember we talked about how to measure the test data error and training data error. - 0.5105\n",
      "- Curse of dimensionality is that the model performs very poorly when we have a lot of features. - 0.5074\n",
      "- So test error, let me see, can be written as a variance of the model, estimated model, and then the bias of the estimated model also and squared plus some irreducible error, the variance of the residuals. - 0.5067\n",
      "- That means if we have another new data point like this, our model should be able to classify that correctly. - 0.5064\n",
      "- And we also talked about the goodness of the model fit. - 0.5061\n",
      "- It's essentially telling that if the model performance are similar, For simpler model and complex model, we prefer choosing simpler model. - 0.5058\n",
      "- So we define this quantity and we know that if this quantity is minimized, we know the model has a good fit. - 0.5048\n",
      "- So let's derive R squared as a measure of model fit. - 0.5026\n",
      "- But anyway, with this we can measure errors for the training and testing. - 0.5017\n",
      "- some useful functions and we'll talk about hyperparameters of the decision trees that we need to pick values such that we minimize overfitting. - 0.5013\n",
      "- So last time, we tried to fit the model that has all the features inside and found that some of the coefficients were not significant. - 0.5013\n",
      "- Which means that we add some feature that maximize the R-squared first and then fit the new model and inspect the result and see if there are coefficients that are insignificant and if there are insignificant coefficients, just remove the features. - 0.5001\n",
      "\n",
      "Related Sentences with Metadata:\n",
      "- [5.srt] [00:16:42.030 --> 00:16:49.810] We'll say later that if my model is overparameterized, then it doesn't do very well in the test data. - Distance: 0.7558\n",
      "- [5.srt] [00:16:50.490 --> 00:16:56.320] And it's an important way to figure out whether my model is overparameterized or not. - Distance: 0.6929\n",
      "- [14.srt] [00:07:36.590 --> 00:07:41.430] because the model is too complex to the data, therefore it's overfitting. - Distance: 0.6686\n",
      "- [17.srt] [00:04:46.569 --> 00:04:52.469] And max features also can help with the overfitting because it can make the model less flexible by looking at the less number of features when we have so many features. - Distance: 0.6237\n",
      "- [6.srt] [00:11:38.409 --> 00:11:47.379] However, the test error will go down in the beginning and then at some point it will start going up again as the model complexity is increased. - Distance: 0.6168\n",
      "- [2.srt] [00:11:30.049 --> 00:11:34.120] And we're going to also talk about how well my model predicts on unseen data. - Distance: 0.6105\n",
      "- [14.srt] [00:07:45.780 --> 00:07:59.250] So around k equals 21, the optimal value happens and the test error is minimized, whereas it can go up if we keep increasing the model complexity. - Distance: 0.6044\n",
      "- [4.srt] [00:00:11.160 --> 00:00:14.089] Alright, so let's talk about how well my model fits. - Distance: 0.6004\n",
      "- [7.srt] [00:05:59.979 --> 00:06:06.219] In that case you may have already good test error for the simple model as well like this. - Distance: 0.5842\n",
      "- [5.srt] [00:13:29.600 --> 00:13:34.779] Instead of using all of them to fit the model, we're going to set aside some data. - Distance: 0.5819\n",
      "- [5.srt] [00:15:04.220 --> 00:15:16.580] with the already fitted model and dot predict and supply test data instead this time and this will give Y prediction from the test data. - Distance: 0.5799\n",
      "- [1.srt] [00:15:20.929 --> 00:15:26.669] Initially, the model doesn't predict very well, so there will be some error between the prediction and target variable. - Distance: 0.5743\n",
      "- [14.srt] [00:07:20.490 --> 00:07:36.380] As you saw previously that the training error goes down as the model complexity increases, test error goes down in the beginning but then it has an optimal value and it goes up again as the model complexity increases. - Distance: 0.5716\n",
      "- [5.srt] [00:16:32.310 --> 00:16:39.950] Or if the data were pretty homogeneous and your model is doing well then train error and test error could be similar value. - Distance: 0.5705\n",
      "- [2.srt] [00:11:10.370 --> 00:11:15.009] And from the summary table values, what gives an idea of how my model fits. - Distance: 0.5700\n",
      "- [5.srt] [00:13:59.720 --> 00:14:05.240] and the test set has feature test and label test. - Distance: 0.5670\n",
      "- [7.srt] [00:06:31.060 --> 00:06:39.649] and where we stop adding more terms to the model by monitoring train and tester error, and we also talked about the bias-variance trade-off principle. - Distance: 0.5665\n",
      "- [4.srt] [00:01:54.009 --> 00:02:05.399] We can define a benchmark model, say y equals y mean, and then we can compare how good is my error from my model. - Distance: 0.5661\n",
      "- [2.srt] [00:11:04.829 --> 00:11:09.269] And we'll also discuss how well my model fits. - Distance: 0.5656\n",
      "- [7.srt] [00:05:09.899 --> 00:05:21.649] You can have a look at the supplemental note but this is the result and according to this the test error is a sum of this variance of the model and bias scaled of the model. - Distance: 0.5651\n",
      "- [4.srt] [00:00:20.410 --> 00:00:23.609] These are metrics for how well the model fits. - Distance: 0.5648\n",
      "- [17.srt] [00:03:11.150 --> 00:03:13.060] They are very easy to overfit so we're gonna talk about some strategies to prevent overfitting. - Distance: 0.5647\n",
      "- [18.srt] [00:00:16.410 --> 00:00:21.890] Decision trees are very easy to overfit, so to mitigate we talked about all the stopping last time. - Distance: 0.5622\n",
      "- [14.srt] [00:13:38.129 --> 00:13:48.230] Our optimal value k equals 7 and as you can see the test accuracy dropped very sharply as we increase the number of features that are included in the model. - Distance: 0.5614\n",
      "- [17.srt] [00:05:23.509 --> 00:05:30.089] So if you want to make the model less flexible, so less overfit, then increase this number. - Distance: 0.5611\n",
      "- [4.srt] [00:02:56.419 --> 00:03:02.579] So this can be a good quantity that measures how my model fits compared to my null model. - Distance: 0.5608\n",
      "- [21.srt] [00:08:29.409 --> 00:08:36.569] As you can see here, boosting algorithm can have overfitting as well if the running rate is too big and number of trees are too big as well. - Distance: 0.5601\n",
      "- [13.srt] [00:04:53.060 --> 00:04:57.530] And I'm gonna call my object clf this time, and then if you want to get an accuracy, so this score uses accuracy by default, you can do fitted model.score and throw your test data and it's going to give some result. - Distance: 0.5583\n",
      "- [7.srt] [00:02:05.869 --> 00:02:25.609] Let's say we had some data that look like this and then we were to fit the data with the simple model and maybe we can also fit with the more complex model that tends to have a lower error so lower bias However, if we chose different data set, like this for example, then if we fit the simple model again, they will be very similar. - Distance: 0.5536\n",
      "- [6.srt] [00:09:47.989 --> 00:09:57.669] in machine learning community validation error is more used term for the data set that's set aside for the purpose of testing while you're training the model. - Distance: 0.5506\n",
      "- [8.srt] [00:08:05.600 --> 00:08:11.440] So with that in mind, let's see what happens if we throw all the features into the model and fit to the data. - Distance: 0.5502\n",
      "- [18.srt] [00:00:10.009 --> 00:00:16.079] So last time we talked about some ways to prevent overfitting in decision trees. - Distance: 0.5482\n",
      "- [21.srt] [00:00:26.829 --> 00:00:31.719] And this lambda helped the model to learn slowly so that we can avoid overfitting. - Distance: 0.5460\n",
      "- [5.srt] [00:14:07.920 --> 00:14:11.019] So using the train set, we're gonna fit the model. - Distance: 0.5451\n",
      "- [4.srt] [00:05:18.230 --> 00:05:27.689] For simple linear regression, this may not happen, but as you might see later, in a more complex model, sometimes the model can fit worse than the baseline. - Distance: 0.5440\n",
      "- [5.srt] [00:16:26.100 --> 00:16:31.040] So it's very common that the test error is a slightly larger than the train error. - Distance: 0.5413\n",
      "- [19.srt] [00:06:28.230 --> 00:06:30.630] And this red curve is a bagging test error. - Distance: 0.5369\n",
      "- [17.srt] [00:05:03.099 --> 00:05:07.829] So the most direct way to prevent overfitting in the decision tree is a max depth. - Distance: 0.5367\n",
      "- [4.srt] [00:07:57.540 --> 00:08:07.680] So if you want to compare apple to apple how my new model is doing in terms of the error, you can just directly calculate RSS for our new model. - Distance: 0.5346\n",
      "- [7.srt] [00:05:55.229 --> 00:05:59.899] And sometimes the simple model fits well to the data already. - Distance: 0.5317\n",
      "- [19.srt] [00:06:55.030 --> 00:06:56.560] You can also see the out-of-bag test error. - Distance: 0.5300\n",
      "- [6.srt] [00:09:25.169 --> 00:09:35.429] So we had a data set that we have both feature and label and then we set aside some portion of this data and called it test data. - Distance: 0.5299\n",
      "- [4.srt] [00:03:54.109 --> 00:04:01.759] But let's think that my model is so good that all the data points are on my model's line. - Distance: 0.5291\n",
      "- [18.srt] [00:01:03.950 --> 00:01:13.520] So this strategy can be effective for preventing overfitting, but it doesn't guarantee that the performance of the tree will be better. - Distance: 0.5268\n",
      "- [20.srt] [00:00:12.339 --> 00:00:19.550] Previously we talked about the trees have a problem that they are weak learner and they can overfit very easily. - Distance: 0.5265\n",
      "- [19.srt] [00:01:56.199 --> 00:02:03.379] You can think about like putting different data set into bag and then make the model trained on this bag of the data. - Distance: 0.5243\n",
      "- [5.srt] [00:05:56.839 --> 00:06:01.659] If you are not convinced by that, the model's assumption, then maybe we can use bootstrapping method. - Distance: 0.5238\n",
      "- [5.srt] [00:18:08.730 --> 00:18:16.640] And lastly, we talked about how to measure the error for training data and test data and how to compare them. - Distance: 0.5236\n",
      "- [8.srt] [00:05:46.790 --> 00:05:50.560] Before we build a model, let's have a qualitative inspection. - Distance: 0.5233\n",
      "- [7.srt] [00:03:30.909 --> 00:03:37.479] Sometimes if the model is not very good, then you may encounter this case. - Distance: 0.5232\n",
      "- [26.srt] [00:15:41.089 --> 00:15:51.539] So, as a conclusion, which models to use, we recommend you inspect the data first and then pick the method that's likely to be better suited for the data. - Distance: 0.5230\n",
      "- [19.srt] [00:03:46.530 --> 00:03:52.680] Then we can test on this the rest of the data that we didn't select to train on. - Distance: 0.5229\n",
      "- [9.srt] [00:06:46.500 --> 00:07:01.469] is 1 over 1 minus R squared value of this fitting and this fitting is actually not fitting the target variable Y but fitting that variable Xi using all other variables. - Distance: 0.5223\n",
      "- [21.srt] [00:02:32.180 --> 00:02:42.980] So after fitting this stump model, using this stump model, and here is that, and then we compare how much accurate it is. - Distance: 0.5217\n",
      "- [1.srt] [00:02:19.560 --> 00:02:24.189] or they can build machine learning models and do the testing on those data or build a system. - Distance: 0.5213\n",
      "- [6.srt] [00:00:24.160 --> 00:00:34.579] And then the key idea we're going to discuss is that when the model complexity increases by adding more features, it can fit the data better, but it can also introduce some other problems. - Distance: 0.5210\n",
      "- [7.srt] [00:00:09.900 --> 00:00:19.859] Okay, so the behavior of test header that goes down first and goes up later as we increase the model's flexibility can be explained by bias-variance trade-off. - Distance: 0.5205\n",
      "- [26.srt] [00:16:25.039 --> 00:16:34.599] You may have to think about Occam's Razor principle, which tells that if the model performances are similar, the simpler model is always better. - Distance: 0.5188\n",
      "- [1.srt] [00:15:27.490 --> 00:15:36.240] And this error can be used to tweak the model to have a better prediction next iteration, and over this iteration, the model becomes more accurate. - Distance: 0.5185\n",
      "- [14.srt] [00:10:26.579 --> 00:10:33.349] Training the logistic regression on the same data set and measuring the training time can give some comparison. - Distance: 0.5166\n",
      "- [2.srt] [00:01:01.459 --> 00:01:06.920] And if the parameters for the parametric model is not optimized, then this prediction value will be far away from the target. - Distance: 0.5164\n",
      "- [4.srt] [00:04:22.979 --> 00:04:35.599] The another extreme case is that my model is actually just as good as my null model y equals y mean. - Distance: 0.5164\n",
      "- [5.srt] [00:13:07.909 --> 00:13:13.720] Ok, so let's talk about how we measure the error from the test data and the training data and how to compare them. - Distance: 0.5162\n",
      "- [6.srt] [00:08:13.719 --> 00:08:19.519] Obviously when you see the model fitness The model fitness will go up and up as you add more model complexity. - Distance: 0.5145\n",
      "- [19.srt] [00:01:44.619 --> 00:01:49.229] One idea might be maybe we can train our models on different subsets of data. - Distance: 0.5140\n",
      "- [5.srt] [00:13:27.220 --> 00:13:28.790] So we talked about this popular error measure so we're going to use them or one of them and let's say we have original training data that we used to use to fit the model. - Distance: 0.5117\n",
      "- [17.srt] [00:07:55.920 --> 00:08:00.770] And it will give the result that which model hyperparameter gave the best result. - Distance: 0.5116\n",
      "- [6.srt] [00:09:18.769 --> 00:09:24.860] So you remember we talked about how to measure the test data error and training data error. - Distance: 0.5105\n",
      "- [14.srt] [00:11:38.339 --> 00:11:45.139] Curse of dimensionality is that the model performs very poorly when we have a lot of features. - Distance: 0.5074\n",
      "- [7.srt] [00:04:49.919 --> 00:05:07.839] So test error, let me see, can be written as a variance of the model, estimated model, and then the bias of the estimated model also and squared plus some irreducible error, the variance of the residuals. - Distance: 0.5067\n",
      "- [23.srt] [00:13:00.140 --> 00:13:07.500] That means if we have another new data point like this, our model should be able to classify that correctly. - Distance: 0.5064\n",
      "- [5.srt] [00:17:22.910 --> 00:17:26.029] And we also talked about the goodness of the model fit. - Distance: 0.5061\n",
      "- [6.srt] [00:12:33.829 --> 00:12:38.749] It's essentially telling that if the model performance are similar, For simpler model and complex model, we prefer choosing simpler model. - Distance: 0.5058\n",
      "- [4.srt] [00:01:15.009 --> 00:01:22.299] So we define this quantity and we know that if this quantity is minimized, we know the model has a good fit. - Distance: 0.5048\n",
      "- [4.srt] [00:00:42.129 --> 00:00:45.560] So let's derive R squared as a measure of model fit. - Distance: 0.5026\n",
      "- [6.srt] [00:09:58.419 --> 00:10:04.589] But anyway, with this we can measure errors for the training and testing. - Distance: 0.5017\n",
      "- [17.srt] [00:01:01.849 --> 00:01:10.109] some useful functions and we'll talk about hyperparameters of the decision trees that we need to pick values such that we minimize overfitting. - Distance: 0.5013\n",
      "- [9.srt] [00:00:11.050 --> 00:00:18.500] So last time, we tried to fit the model that has all the features inside and found that some of the coefficients were not significant. - Distance: 0.5013\n",
      "- [9.srt] [00:01:44.649 --> 00:01:51.019] Which means that we add some feature that maximize the R-squared first and then fit the new model and inspect the result and see if there are coefficients that are insignificant and if there are insignificant coefficients, just remove the features. - Distance: 0.5001\n"
     ]
    }
   ],
   "source": [
    "# Load lecture sentences\n",
    "with open('Data/sentences.txt', 'r') as file:\n",
    "    lecture_sentences = file.readlines()\n",
    "lecture_sentences = [line.strip() for line in lecture_sentences if line.strip()]\n",
    "\n",
    "lecture_data = []\n",
    "with open('Data/srt-embedding-metadata.tsv', 'r', encoding='utf-8') as file:\n",
    "    tsv_reader = csv.reader(file, delimiter='\\t')\n",
    "    for row in tsv_reader:\n",
    "        if len(row) == 3:\n",
    "            filename, timestamp, sentence = row\n",
    "            lecture_data.append((filename.strip(), timestamp.strip(), sentence.strip()))\n",
    "\n",
    "\n",
    "student_question = input(\"Enter your question: \")\n",
    "# Uncomment below line for encoding all miniLM and all mpnet.\n",
    "# question_embedding = np.array(\n",
    "#     model.encode([student_question])\n",
    "# ).astype('float32')\n",
    "\n",
    "# This is for qwen\n",
    "question_embedding = model.encode(student_question, prompt_name=\"query\")\n",
    "if question_embedding.ndim == 1:\n",
    "    question_embedding = np.expand_dims(question_embedding, axis=0)\n",
    "\n",
    "\n",
    "distances, indices = faiss_index.search(question_embedding, len(lecture_sentences))\n",
    "\n",
    "# Define a Similarity threshold (lower means more similar)\n",
    "similarity_threshold = 0.5\n",
    "\n",
    "related_sentences = []\n",
    "related_results = []\n",
    "for j in range(len(indices[0])):\n",
    "    i = indices[0][j]\n",
    "    similarity = distances[0][j]\n",
    "    sentence = lecture_sentences[i]\n",
    "    \n",
    "    # Check if the sentence is above the similarity threshold and is not a question\n",
    "    if similarity >= similarity_threshold and not sentence.strip().endswith('?'):\n",
    "        related_sentences.append((sentence, similarity))\n",
    "        filename, timestamp, _ = lecture_data[i+1]\n",
    "        related_results.append((filename, timestamp, sentence, similarity))\n",
    "\n",
    "print(\"Number of Related Sentences: \", len(related_sentences))\n",
    "print(\"Question: \", student_question)\n",
    "print(\"Model: all-mini-LM\")\n",
    "# Display related sentences with distances\n",
    "print(\"\\n Related Sentences:\")\n",
    "for sentence, distance in related_sentences:\n",
    "    print(f\"- {sentence} - {distance:.4f}\")\n",
    "\n",
    "\n",
    "print(\"\\nRelated Sentences with Metadata:\")\n",
    "for filename, timestamp, sentence, distance in related_results:\n",
    "    print(f\"- [{filename}] [{timestamp}] {sentence} - Distance: {distance:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
