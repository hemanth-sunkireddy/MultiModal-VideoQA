{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f1c1376",
   "metadata": {},
   "source": [
    "## Qwen 0.6B Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "056097c4",
   "metadata": {},
   "source": [
    "### Install SUMMARISER MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dc777f0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bert-extractive-summarizer in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (0.10.1)\n",
      "Requirement already satisfied: transformers in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from bert-extractive-summarizer) (4.55.0)\n",
      "Requirement already satisfied: scikit-learn in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from bert-extractive-summarizer) (1.7.1)\n",
      "Requirement already satisfied: spacy in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from bert-extractive-summarizer) (3.8.7)\n",
      "Requirement already satisfied: numpy>=1.22.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from scikit-learn->bert-extractive-summarizer) (2.3.2)\n",
      "Requirement already satisfied: scipy>=1.8.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from scikit-learn->bert-extractive-summarizer) (1.16.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from scikit-learn->bert-extractive-summarizer) (1.5.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from scikit-learn->bert-extractive-summarizer) (3.6.0)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from spacy->bert-extractive-summarizer) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from spacy->bert-extractive-summarizer) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from spacy->bert-extractive-summarizer) (1.0.13)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from spacy->bert-extractive-summarizer) (2.0.11)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from spacy->bert-extractive-summarizer) (3.0.10)\n",
      "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from spacy->bert-extractive-summarizer) (8.3.6)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from spacy->bert-extractive-summarizer) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from spacy->bert-extractive-summarizer) (2.5.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from spacy->bert-extractive-summarizer) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from spacy->bert-extractive-summarizer) (0.4.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from spacy->bert-extractive-summarizer) (0.17.4)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from spacy->bert-extractive-summarizer) (4.67.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from spacy->bert-extractive-summarizer) (2.32.4)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from spacy->bert-extractive-summarizer) (2.11.7)\n",
      "Requirement already satisfied: jinja2 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from spacy->bert-extractive-summarizer) (3.1.6)\n",
      "Requirement already satisfied: setuptools in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from spacy->bert-extractive-summarizer) (80.9.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/hemanth/Library/Python/3.13/lib/python/site-packages (from spacy->bert-extractive-summarizer) (25.0)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from spacy->bert-extractive-summarizer) (3.5.0)\n",
      "Requirement already satisfied: language-data>=1.2 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from langcodes<4.0.0,>=3.2.0->spacy->bert-extractive-summarizer) (1.3.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy->bert-extractive-summarizer) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy->bert-extractive-summarizer) (2.33.2)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy->bert-extractive-summarizer) (4.14.1)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy->bert-extractive-summarizer) (0.4.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from requests<3.0.0,>=2.13.0->spacy->bert-extractive-summarizer) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from requests<3.0.0,>=2.13.0->spacy->bert-extractive-summarizer) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from requests<3.0.0,>=2.13.0->spacy->bert-extractive-summarizer) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from requests<3.0.0,>=2.13.0->spacy->bert-extractive-summarizer) (2025.8.3)\n",
      "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from thinc<8.4.0,>=8.3.4->spacy->bert-extractive-summarizer) (1.3.0)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from thinc<8.4.0,>=8.3.4->spacy->bert-extractive-summarizer) (0.1.5)\n",
      "Requirement already satisfied: click>=8.0.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from typer<1.0.0,>=0.3.0->spacy->bert-extractive-summarizer) (8.2.1)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from typer<1.0.0,>=0.3.0->spacy->bert-extractive-summarizer) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from typer<1.0.0,>=0.3.0->spacy->bert-extractive-summarizer) (14.1.0)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from weasel<0.5.0,>=0.1.0->spacy->bert-extractive-summarizer) (0.22.0)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from weasel<0.5.0,>=0.1.0->spacy->bert-extractive-summarizer) (7.3.1)\n",
      "Requirement already satisfied: wrapt in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy->bert-extractive-summarizer) (1.17.3)\n",
      "Requirement already satisfied: marisa-trie>=1.1.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy->bert-extractive-summarizer) (1.3.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy->bert-extractive-summarizer) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/hemanth/Library/Python/3.13/lib/python/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy->bert-extractive-summarizer) (2.19.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy->bert-extractive-summarizer) (0.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from jinja2->spacy->bert-extractive-summarizer) (3.0.2)\n",
      "Requirement already satisfied: filelock in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from transformers->bert-extractive-summarizer) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from transformers->bert-extractive-summarizer) (0.34.4)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from transformers->bert-extractive-summarizer) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from transformers->bert-extractive-summarizer) (2025.7.34)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from transformers->bert-extractive-summarizer) (0.21.4)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from transformers->bert-extractive-summarizer) (0.6.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers->bert-extractive-summarizer) (2025.7.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers->bert-extractive-summarizer) (1.1.7)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install bert-extractive-summarizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d657e91d",
   "metadata": {},
   "source": [
    "Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1bfbf162",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import ffmpeg\n",
    "import os\n",
    "import faiss\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import re\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import csv\n",
    "import pickle\n",
    "\n",
    "# Paths\n",
    "VIDEO_DIR = \"Data/Videos\"             # Folder containing input video files\n",
    "AUDIO_DIR = \"Data/Audios\"             # Folder to store extracted audio files\n",
    "CHUNK_DIR = \"Data/Audio-Chunks\"       # Folder to save audio chunks after VAD\n",
    "\n",
    "# Create output directories if they don't exist\n",
    "os.makedirs(AUDIO_DIR, exist_ok=True)\n",
    "os.makedirs(CHUNK_DIR, exist_ok=True)\n",
    "\n",
    "# Audio processing\n",
    "TARGET_SAMPLE_RATE = 16000  # or 32000 Hz depending on your use case\n",
    "\n",
    "# VAD settings\n",
    "MIN_CHUNK_DURATION_SEC = 30  # Minimum duration for an audio chunk\n",
    "USE_ONNX_MODEL = False      # Set True to use ONNX version of Silero VAD\n",
    "\n",
    "from silero_vad import (\n",
    "    load_silero_vad, read_audio, get_speech_timestamps, \n",
    "    save_audio, VADIterator\n",
    ")\n",
    "\n",
    "# faiss_index = faiss.read_index(\"Data/sentence_embeddings.index\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a321f47e",
   "metadata": {},
   "source": [
    "### Model for Semantic Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "554479d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer(\"Qwen/Qwen3-Embedding-0.6B\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af288996",
   "metadata": {},
   "source": [
    "### Summarisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2e89e92",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from summarizer import Summarizer\n",
    "\n",
    "# Initialize summarizer\n",
    "summarizer = Summarizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d41c6490",
   "metadata": {},
   "source": [
    "## Video to Audio Conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d21fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through all .mp4 files in the input folder\n",
    "for filename in os.listdir(VIDEO_DIR):\n",
    "    if filename.endswith(\".mp4\"):\n",
    "        input_path = os.path.join(VIDEO_DIR, filename)\n",
    "        output_path = os.path.join(AUDIO_DIR, filename.replace(\".mp4\", \".wav\"))\n",
    "\n",
    "        print(f\"Processing: {input_path} -> {output_path}\")\n",
    "        \n",
    "        # Extract audio\n",
    "        input_video = ffmpeg.input(input_path)\n",
    "        output_audio = ffmpeg.output(input_video.audio, output_path, ac=1, ar=TARGET_SAMPLE_RATE)\n",
    "        ffmpeg.run(output_audio, overwrite_output=True)\n",
    "        \n",
    "        # Probe the generated audio file for details\n",
    "        audio_info = ffmpeg.probe(output_path, v=\"error\", select_streams=\"a\", show_entries=\"stream=codec_name,codec_type,sample_rate,channels,bit_rate,duration\")\n",
    "        \n",
    "        codec_name = audio_info['streams'][0]['codec_name']\n",
    "        sample_rate = int(audio_info['streams'][0]['sample_rate'])\n",
    "        channels = int(audio_info['streams'][0]['channels'])\n",
    "        bit_rate = audio_info['streams'][0].get('bit_rate', 'N/A')\n",
    "        duration_sec = float(audio_info['streams'][0]['duration'])\n",
    "        duration_ms = duration_sec * 1000\n",
    "        \n",
    "        print(f\"Audio extracted: {output_path}\")\n",
    "        # print(f\"Codec: {codec_name}, Sample Rate: {sample_rate} Hz, Channels: {channels}, Bit Rate: {bit_rate}, Duration: {duration_ms} ms\\n\")\n",
    "\n",
    "print(\"Video to Audio converted successfully!.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be5d8a67",
   "metadata": {},
   "source": [
    "Voice Activity Detection Algorithm on Audio Files - Converting into Smaller Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "184e3aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Silero VAD model\n",
    "model = load_silero_vad(onnx=USE_ONNX_MODEL)\n",
    "\n",
    "def process_audio_file(audio_path, output_chunk_dir):\n",
    "    \"\"\"Process an audio file, split it into chunks, and save them.\"\"\"\n",
    "    wav = read_audio(audio_path, sampling_rate=TARGET_SAMPLE_RATE)\n",
    "    speech_timestamps = get_speech_timestamps(\n",
    "        wav, model, sampling_rate=TARGET_SAMPLE_RATE, return_seconds=True\n",
    "    )\n",
    "    \n",
    "    # Format timestamps to 4 decimal places\n",
    "    for segment in speech_timestamps:\n",
    "        segment['start'] = float(f\"{segment['start']:.4f}\")\n",
    "        segment['end'] = float(f\"{segment['end']:.4f}\")\n",
    "    \n",
    "    vad_iterator = VADIterator(model, sampling_rate=TARGET_SAMPLE_RATE)\n",
    "    chunks = []\n",
    "    current_chunk_start = 0\n",
    "    \n",
    "    for segment in speech_timestamps:\n",
    "        start, end = segment['start'], segment['end']\n",
    "        if (end - current_chunk_start) >= MIN_CHUNK_DURATION_SEC:\n",
    "            chunk_wav = wav[int(current_chunk_start * TARGET_SAMPLE_RATE):int(end * TARGET_SAMPLE_RATE)]\n",
    "            chunk_path = os.path.join(output_chunk_dir, f\"{len(chunks) + 1}.wav\")\n",
    "            save_audio(chunk_path, chunk_wav, sampling_rate=TARGET_SAMPLE_RATE)\n",
    "            chunks.append((current_chunk_start, end, chunk_wav))\n",
    "            current_chunk_start = end\n",
    "    \n",
    "    # Save the last chunk if necessary\n",
    "    if current_chunk_start < speech_timestamps[-1]['end']:\n",
    "        chunk_wav = wav[int(current_chunk_start * TARGET_SAMPLE_RATE):]\n",
    "        chunk_path = os.path.join(output_chunk_dir, f\"{len(chunks) + 1}.wav\")\n",
    "        save_audio(chunk_path, chunk_wav, sampling_rate=TARGET_SAMPLE_RATE)\n",
    "        chunks.append((current_chunk_start, speech_timestamps[-1]['end'], chunk_wav))\n",
    "    \n",
    "    vad_iterator.reset_states()\n",
    "    print(f\"Processed {audio_path}, saved chunks in {output_chunk_dir}\")\n",
    "\n",
    "def process_all_audio_files():\n",
    "    \"\"\"Process all .wav files in the main audio folder and save their chunks.\"\"\"\n",
    "    if not os.path.exists(AUDIO_DIR):\n",
    "        print(f\"Audio folder '{AUDIO_DIR}' does not exist.\")\n",
    "        return\n",
    "    \n",
    "    for file_name in sorted(os.listdir(AUDIO_DIR)):\n",
    "        if file_name.endswith(\".wav\"):\n",
    "            audio_path = os.path.join(AUDIO_DIR, file_name)\n",
    "            audio_id = os.path.splitext(file_name)[0]  # Extract the number without extension\n",
    "            output_chunk_dir = os.path.join(CHUNK_DIR, audio_id)\n",
    "            process_audio_file(audio_path, output_chunk_dir)\n",
    "\n",
    "# Process VAD on Audio Files.\n",
    "process_all_audio_files()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b59306",
   "metadata": {},
   "source": [
    "### Transcribing Audio Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80eae30e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import whisper\n",
    "import os\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Load Whisper model once\n",
    "model = whisper.load_model(\"large\", device=\"cuda\")\n",
    "\n",
    "# Function to transcribe and save sentence-wise output\n",
    "def transcribe_audio(audio_path, output_dir):\n",
    "    # Extract base name (without extension)\n",
    "    base_name = os.path.splitext(os.path.basename(audio_path))[0]\n",
    "\n",
    "    # Transcribe the audio\n",
    "    result = model.transcribe(audio_path, language=\"en\")\n",
    "\n",
    "    # Combine all text segments\n",
    "    full_text = \" \".join(segment[\"text\"].strip() for segment in result[\"segments\"])\n",
    "\n",
    "    # Split into proper sentences\n",
    "    sentences = sent_tokenize(full_text)\n",
    "\n",
    "    # Ensure output directory exists\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Save transcript (sentence-wise) to a text file\n",
    "    txt_file = os.path.join(output_dir, f\"{base_name}.txt\")\n",
    "    with open(txt_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        for sentence in sentences:\n",
    "            f.write(sentence.strip() + \"\\n\")  # Sentence per line\n",
    "\n",
    "    print(f\"✅ Transcription saved: {txt_file}\")\n",
    "\n",
    "# Root directory containing numbered subfolders\n",
    "root_audio_dir = \"Data/Audio-Chunks\"\n",
    "root_output_dir = \"Data/SRT-Files\"\n",
    "\n",
    "# Iterate through numbered folders (1 to 26)\n",
    "for subfolder in sorted(os.listdir(root_audio_dir)):\n",
    "    subfolder_path = os.path.join(root_audio_dir, subfolder)\n",
    "\n",
    "    # Ensure it's a directory\n",
    "    if os.path.isdir(subfolder_path):\n",
    "        # Define corresponding output directory\n",
    "        output_subfolder = os.path.join(root_output_dir, subfolder)\n",
    "\n",
    "        # Process each audio file in the subfolder\n",
    "        for file in os.listdir(subfolder_path):\n",
    "            if file.endswith(\".wav\"):\n",
    "                audio_file_path = os.path.join(subfolder_path, file)\n",
    "                transcribe_audio(audio_file_path, output_subfolder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a840a5fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "# Define folder paths\n",
    "f1 = \"complete_audio_with_text\"\n",
    "f2 = \"complete_extracted_text\"\n",
    "\n",
    "# Iterate over directories\n",
    "for subfolder in sorted(os.listdir(f2), key=lambda x: int(x)):  \n",
    "  \n",
    "    source_folder = os.path.join(f2, subfolder)\n",
    "    destination_folder = os.path.join(f1, subfolder)\n",
    "\n",
    "    # Ensure the destination folder exists\n",
    "    if not os.path.exists(destination_folder):\n",
    "        print(f\"Warning: Destination folder {destination_folder} does not exist, skipping...\")\n",
    "        continue\n",
    "\n",
    "    # Copy all .txt files from source to destination\n",
    "    for file in os.listdir(source_folder):\n",
    "        if file.endswith(\".txt\"):\n",
    "            src_file = os.path.join(source_folder, file)\n",
    "            dest_file = os.path.join(destination_folder, file)\n",
    "            shutil.copy2(src_file, dest_file)  # copy2 preserves metadata\n",
    "            print(f\"Copied {src_file} -> {dest_file}\")\n",
    "\n",
    "print(\"All .txt files copied successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e87ccb31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import os\n",
    "\n",
    "# Define paths\n",
    "root_corpus_dir = \"./audio_chunks\"\n",
    "dict_path = \"english_us_arpa\"\n",
    "model_path = \"english_us_arpa\"\n",
    "root_output_dir = \"./complete_timestamp\"\n",
    "\n",
    "# Iterate through numbered folders (1 to 26)\n",
    "for subfolder in sorted(os.listdir(root_corpus_dir), key=lambda x: int(x)):\n",
    "\n",
    "    # if subfolder not in [\"22\",\"23\",\"24\",\"25\",\"26\"]:\n",
    "    #     continue\n",
    "    subfolder_corpus_path = os.path.join(root_corpus_dir, subfolder)\n",
    "    subfolder_output_path = os.path.join(root_output_dir, subfolder)\n",
    "\n",
    "    # Ensure the output directory exists\n",
    "    os.makedirs(subfolder_output_path, exist_ok=True)\n",
    "\n",
    "    # Run the alignment command\n",
    "    cmd = [\n",
    "        \"mfa\", \"align\",\"--clean\", \"--output_format\", \"csv\",\n",
    "        subfolder_corpus_path, dict_path, model_path, subfolder_output_path\n",
    "    ]\n",
    "    subprocess.run(cmd, check=True)\n",
    "    print(f\"Alignment completed for {subfolder}!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a936963",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import librosa\n",
    "from natsort import natsorted  # To ensure files are processed in correct order\n",
    "\n",
    "def get_audio_length(audio_file):\n",
    "    \"\"\"Get the exact length of an audio file using librosa.\"\"\"\n",
    "    y, sr = librosa.load(audio_file, sr=None)\n",
    "    return librosa.get_duration(y=y, sr=sr)\n",
    "\n",
    "def convert_to_srt_time(seconds):\n",
    "    \"\"\"Convert seconds to SRT time format (hh:mm:ss,ms).\"\"\"\n",
    "    millisec = int((seconds - int(seconds)) * 1000)\n",
    "    hours, remainder = divmod(int(seconds), 3600)\n",
    "    minutes, sec = divmod(remainder, 60)\n",
    "    return f\"{hours:02}:{minutes:02}:{sec:02},{millisec:03}\"\n",
    "\n",
    "def merge_chunks_to_srt(csv_folder,txt_folder,audio_folder, srt_file):\n",
    "    \"\"\"Process all chunk CSVs and TXT files to generate a sentence-level SRT file.\"\"\"\n",
    "    chunk_files = natsorted([f for f in os.listdir(csv_folder) if f.endswith(\".csv\")])  # Sort correctly\n",
    "    # chunk_files = natsorted([f for f in os.listdir(audio_folder) if f.endswith(\".wav\")])  \n",
    "    # print(chunk_files)\n",
    "    total_offset = 0  # Offset to adjust timestamps\n",
    "\n",
    "    with open(srt_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        subtitle_index = 1  # SRT subtitle counter\n",
    "        for chunk in chunk_files:\n",
    "            csv_path = os.path.join(csv_folder, chunk)\n",
    "            txt_path = os.path.join(txt_folder, chunk.replace(\".csv\", \".txt\"))\n",
    "            audio_path = os.path.join(audio_folder, chunk.replace(\".csv\", \".wav\"))  # Assuming audio has the same name\n",
    "\n",
    "            # Get audio length of this chunk\n",
    "            chunk_length = get_audio_length(audio_path)\n",
    "\n",
    "            # Read CSV\n",
    "            df = pd.read_csv(csv_path)\n",
    "\n",
    "            # Filter for words only\n",
    "            df = df[df[\"Type\"] == \"words\"].reset_index(drop=True)\n",
    "\n",
    "            # Read sentences from the corresponding TXT file\n",
    "            with open(txt_path, \"r\", encoding=\"utf-8\") as txt_file:\n",
    "                sentences = txt_file.readlines()\n",
    "\n",
    "            word_index = 0  # Track position in the word list\n",
    "\n",
    "            for sentence in sentences:\n",
    "                words = sentence.strip().split()\n",
    "\n",
    "                if word_index >= len(df):\n",
    "                    break  # Avoid index error\n",
    "\n",
    "                # Get the start and end timestamps for the sentence\n",
    "                start_time = df.loc[word_index, 'Begin'] + total_offset\n",
    "                end_time = df.loc[min(word_index + len(words) - 1, len(df) - 1), 'End'] + total_offset\n",
    "\n",
    "                # Write to SRT\n",
    "                f.write(f\"{subtitle_index}\\n\")\n",
    "                f.write(f\"{convert_to_srt_time(start_time)} --> {convert_to_srt_time(end_time)}\\n\")\n",
    "                f.write(f\"{sentence.strip()}\\n\\n\")\n",
    "\n",
    "                subtitle_index += 1  # Increment subtitle number\n",
    "                word_index += len(words)  # Move to the next sentence\n",
    "\n",
    "            # Update offset for next chunk\n",
    "            total_offset += chunk_length\n",
    "\n",
    "    print(f\"✅ Sentence-level SRT file saved as: {srt_file}\")\n",
    "\n",
    "# Example usage\n",
    "csv_base_folder = \"complete_timestamp\"  # Change to your folder path\n",
    "txt_base_folder=\"audio_chunks_text\"\n",
    "audio_base_folder=\"audio_chunks\"\n",
    "srt_base_folder = \"complete_srt\"\n",
    "\n",
    "for subfolder in sorted(os.listdir(audio_base_folder), key=lambda x: int(x)):\n",
    "    subfolder_path = os.path.join(audio_base_folder, subfolder)\n",
    "\n",
    "    # Ensure it's a directory\n",
    "    if os.path.isdir(subfolder_path):\n",
    "        # Define corresponding output directory\n",
    "        # output_subfolder = os.path.join(root_output_dir, subfolder)\n",
    "        csv_subfolder = os.path.join(csv_base_folder, subfolder)\n",
    "        txt_subfolder=os.path.join(txt_base_folder, subfolder)\n",
    "        audio_subfolder = os.path.join(audio_base_folder, subfolder)\n",
    "        srt_file_path = os.path.join(srt_base_folder, subfolder+\".srt\")\n",
    "\n",
    "        merge_chunks_to_srt(csv_subfolder, txt_subfolder,audio_subfolder,srt_file_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e7588d",
   "metadata": {},
   "source": [
    "## SRT to Sentences generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eba7e8ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing SRT files: 100%|██████████| 31/31 [00:00<00:00, 715.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata saved as 'Data/srt-embedding-metadata.tsv'\n",
      "Sentences saved as 'Data/sentences.txt'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Function to extract combined sentences with timestamps from .srt file\n",
    "def extract_sentences_from_srt(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    sentences = []\n",
    "    timestamps = []\n",
    "    current_sentence = \"\"\n",
    "    current_timestamp = \"\"\n",
    "\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "\n",
    "        # Check for timestamp lines\n",
    "        timestamp_match = re.match(r'(\\d{2}:\\d{2}:\\d{2}[.,]\\d{3}) --> (\\d{2}:\\d{2}:\\d{2}[.,]\\d{3})', line)\n",
    "        if timestamp_match:\n",
    "            current_timestamp = timestamp_match.group(1).replace(',', '.') + ' --> ' + timestamp_match.group(2).replace(',', '.')\n",
    "            continue\n",
    "\n",
    "        # Skip empty lines and cue identifiers\n",
    "        if not line or line.isdigit():\n",
    "            continue\n",
    "\n",
    "        # Add line to current sentence\n",
    "        current_sentence += \" \" + line if current_sentence else line\n",
    "\n",
    "        # If sentence ends, save it\n",
    "        if re.search(r'[.!?]$', line):\n",
    "            sentences.append(current_sentence.strip())\n",
    "            timestamps.append(current_timestamp)\n",
    "            current_sentence = \"\"\n",
    "            current_timestamp = \"\"\n",
    "\n",
    "    return sentences, timestamps\n",
    "\n",
    "# Directory containing SRT files\n",
    "srt_directory = 'Data/SRT-Files'\n",
    "\n",
    "# Initialize lists for all sentences, timestamps, and filenames\n",
    "all_sentences = []\n",
    "all_timestamps = []\n",
    "all_filenames = []\n",
    "\n",
    "# Process all SRT files with tqdm\n",
    "srt_files = [f for f in os.listdir(srt_directory) if f.endswith('.srt')]\n",
    "for file_name in tqdm(srt_files, desc=\"Processing SRT files\"):\n",
    "    file_path = os.path.join(srt_directory, file_name)\n",
    "    sentences, timestamps = extract_sentences_from_srt(file_path)\n",
    "    all_sentences.extend(sentences)\n",
    "    all_timestamps.extend(timestamps)\n",
    "    all_filenames.extend([file_name] * len(sentences))  # associate each sentence with its file\n",
    "\n",
    "# print(\"Encoding sentences into embeddings...\")\n",
    "# Uncomment for all miniLM and all mpnet.\n",
    "# sentence_embeddings = np.array(\n",
    "#     model.encode(all_sentences) \n",
    "# ).astype('float32')\n",
    "\n",
    "# sentence_embeddings = model.encode(all_sentences)\n",
    "\n",
    "# Create FAISS index (use Inner Product for cosine similarity)\n",
    "# embedding_dimension = sentence_embeddings.shape[1]\n",
    "# faiss_index = faiss.IndexFlatIP(embedding_dimension)\n",
    "# faiss_index.add(sentence_embeddings)\n",
    "\n",
    "# Save FAISS index\n",
    "# faiss.write_index(faiss_index, \"Data/sentence_embeddings.index\")\n",
    "\n",
    "# Save metadata to file\n",
    "metadata_file = 'Data/srt-embedding-metadata.tsv'\n",
    "with open(metadata_file, 'w', encoding='utf-8') as file:\n",
    "    file.write(\"filename\\ttimestamp\\tsentence\\n\")\n",
    "    for fname, timestamp, sentence in zip(all_filenames, all_timestamps, all_sentences):\n",
    "        clean_sentence = sentence.replace('\\t', ' ').replace('\\n', ' ')\n",
    "        file.write(f\"{fname}\\t{timestamp}\\t{clean_sentence}\\n\")\n",
    "\n",
    "# Save sentences to a text file\n",
    "sentences_file = 'Data/sentences.txt'\n",
    "with open(sentences_file, 'w', encoding='utf-8') as file:\n",
    "    for sentence in all_sentences:\n",
    "        file.write(sentence.strip().replace('\\n', ' ') + '\\n')\n",
    "\n",
    "# Summary\n",
    "# print(f\"\\nEmbeddings created for {len(all_sentences)} sentences from {len(srt_files)} SRT files.\")\n",
    "# print(\"FAISS index saved as 'Data/sentence_embeddings.index'\")\n",
    "print(f\"Metadata saved as '{metadata_file}'\")\n",
    "print(f\"Sentences saved as '{sentences_file}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f406234c",
   "metadata": {},
   "source": [
    "## Finding Related Sentences to a Question"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "565b77c4",
   "metadata": {},
   "source": [
    "### Grouping Sentences N-gram technique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dbdc7e83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grouped Sentence to Metadata First element: Hi everyone. -> ('2.srt', '00:00:05.429 -> 00:00:06.209')\n"
     ]
    }
   ],
   "source": [
    "metadata_list = []\n",
    "with open(\"Data/srt-embedding-metadata.tsv\", \"r\", encoding=\"utf-8\") as f:\n",
    "    reader = csv.reader(f, delimiter='\\t')\n",
    "    next(reader)\n",
    "    for row in reader:\n",
    "        filename, timestamp, sentence = row\n",
    "        metadata_list.append((filename, timestamp, sentence))\n",
    "\n",
    "grouped_sentences = []\n",
    "grouped_sent_to_metadata = {}\n",
    "group_size = 1\n",
    "\n",
    "def extract_start_end(ts):\n",
    "    start, end = ts.split(\"-->\")\n",
    "    return start.strip(), end.strip()\n",
    "\n",
    "for i in range(len(metadata_list) - group_size + 1):\n",
    "    group = metadata_list[i:i+group_size]\n",
    "    grouped_text = \" \".join(sent for _, _, sent in group)\n",
    "    filename = group[0][0]\n",
    "    first_start, _ = extract_start_end(group[0][1])\n",
    "    _, last_end = extract_start_end(group[-1][1])\n",
    "    timestamp_range = f\"{first_start} -> {last_end}\"\n",
    "    grouped_sentences.append(grouped_text)\n",
    "    grouped_sent_to_metadata[grouped_text] = (filename, timestamp_range)\n",
    "\n",
    "with open(\"Data/sentences.pkl\", \"wb\") as f:\n",
    "    pickle.dump(grouped_sentences, f)\n",
    "\n",
    "with open(\"Data/sent_to_metadata.pkl\", \"wb\") as f:\n",
    "    pickle.dump(grouped_sent_to_metadata, f)\n",
    "\n",
    "first_key = next(iter(grouped_sent_to_metadata))\n",
    "print(\"Grouped Sentence to Metadata First element:\", first_key, \"->\", grouped_sent_to_metadata[first_key])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "41c6e162",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding grouped sentences into embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding batches: 100%|██████████| 172/172 [07:04<00:00,  2.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ FAISS index created and saved!\n"
     ]
    }
   ],
   "source": [
    "# Load grouped sentences\n",
    "with open(\"Data/sentences.pkl\", \"rb\") as f:\n",
    "    grouped_sentences = pickle.load(f)\n",
    "\n",
    "# Load precomputed metadata mapping\n",
    "with open(\"Data/sent_to_metadata.pkl\", \"rb\") as f:\n",
    "    grouped_sent_to_metadata = pickle.load(f)\n",
    "print(\"Encoding grouped sentences into embeddings...\")\n",
    "\n",
    "batch_size = 16  # you can adjust this based on memory\n",
    "all_embeddings = []\n",
    "\n",
    "for i in tqdm(range(0, len(grouped_sentences), batch_size), desc=\"Encoding batches\"):\n",
    "    batch = grouped_sentences[i:i+batch_size]\n",
    "    batch_emb = model.encode(batch)  # encode the batch\n",
    "    all_embeddings.append(batch_emb)\n",
    "\n",
    "# Combine all batches into one array and convert to float32\n",
    "grouped_embeddings = np.vstack(all_embeddings).astype('float32')\n",
    "\n",
    "# Create FAISS index (Inner Product for cosine similarity)\n",
    "embedding_dim = grouped_embeddings.shape[1]\n",
    "faiss_index = faiss.IndexFlatIP(embedding_dim)\n",
    "faiss_index.add(grouped_embeddings)\n",
    "\n",
    "# Save FAISS index\n",
    "faiss.write_index(faiss_index, \"Data/sentences-embeddings.idx\")\n",
    "\n",
    "print(\"✅ FAISS index created and saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "53dd7e19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPS available: True\n",
      "MPS enabled: True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"MPS available:\", torch.backends.mps.is_available())\n",
    "print(\"MPS enabled:\", torch.backends.mps.is_built())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "947e47fe",
   "metadata": {},
   "source": [
    "### QA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9c19be7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is Machine Learning?\n",
      "\n",
      "Top Related Sentences with Metadata:\n",
      "- [1.srt | 00:03:07.229 -> 00:03:17.589] So machine learning extends the statistical learning by including more complex algorithms, which deal with more complex data and bigger data, and more efficient algorithms.\n",
      "\n",
      "- [1.srt | 00:05:20.230 -> 00:05:26.430] And in intersection, when the AI algorithm is learning from the data, it is called machine learning.\n",
      "\n",
      "- [1.srt | 00:02:42.550 -> 00:02:48.879] So machine learning is part of data science and it is also a subfield of artificial intelligence.\n",
      "\n",
      "- [1.srt | 00:02:55.280 -> 00:03:02.489] Machine learning consists of different types of learning, such as supervised learning, unsupervised learning, or reinforcement learning.\n",
      "\n",
      "- [1.srt | 00:03:25.250 -> 00:03:26.830] and build machine learning systems.\n",
      "\n",
      "- [1.srt | 00:06:07.960 -> 00:06:12.370] As you can see, machine learning is a top skill in the jobs that involves AI skills.\n",
      "\n",
      "- [23.srt | 00:00:14.289 -> 00:00:17.660] So in machine learning, we have different learning tasks.\n",
      "\n",
      "- [7.srt | 00:01:12.719 -> 00:01:14.829] So how does that translate to machine learning?\n",
      "\n",
      "- [1.srt | 00:06:21.170 -> 00:06:23.620] So, machine learning is applied everywhere these days.\n",
      "\n",
      "- [1.srt | 00:08:04.560 -> 00:08:07.610] Machine learning is also a key component in Internet of Things.\n",
      "\n",
      "\n",
      "Ordered & Merged Top Related Sentences (duplicates removed):\n",
      "\n",
      "Group 1\n",
      "Text: So machine learning is part of data science and it is also a subfield of artificial intelligence.\n",
      "Metadata: ('1.srt', '00:02:42.550 -> 00:02:48.879')\n",
      "\n",
      "Group 2\n",
      "Text: Machine learning consists of different types of learning, such as supervised learning, unsupervised learning, or reinforcement learning.\n",
      "Metadata: ('1.srt', '00:02:55.280 -> 00:03:02.489')\n",
      "\n",
      "Group 3\n",
      "Text: So machine learning extends the statistical learning by including more complex algorithms, which deal with more complex data and bigger data, and more efficient algorithms.\n",
      "Metadata: ('1.srt', '00:03:07.229 -> 00:03:17.589')\n",
      "\n",
      "Group 4\n",
      "Text: and build machine learning systems.\n",
      "Metadata: ('1.srt', '00:03:25.250 -> 00:03:26.830')\n",
      "\n",
      "Group 5\n",
      "Text: And in intersection, when the AI algorithm is learning from the data, it is called machine learning.\n",
      "Metadata: ('1.srt', '00:05:20.230 -> 00:05:26.430')\n",
      "\n",
      "Group 6\n",
      "Text: As you can see, machine learning is a top skill in the jobs that involves AI skills.\n",
      "Metadata: ('1.srt', '00:06:07.960 -> 00:06:12.370')\n",
      "\n",
      "Group 7\n",
      "Text: So, machine learning is applied everywhere these days.\n",
      "Metadata: ('1.srt', '00:06:21.170 -> 00:06:23.620')\n",
      "\n",
      "Group 8\n",
      "Text: Machine learning is also a key component in Internet of Things.\n",
      "Metadata: ('1.srt', '00:08:04.560 -> 00:08:07.610')\n",
      "\n",
      "Group 9\n",
      "Text: So how does that translate to machine learning.\n",
      "Metadata: ('7.srt', '00:01:12.719 -> 00:01:14.829')\n",
      "\n",
      "Group 10\n",
      "Text: So in machine learning, we have different learning tasks.\n",
      "Metadata: ('23.srt', '00:00:14.289 -> 00:00:17.660')\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "import torch\n",
    "import pickle\n",
    "import faiss\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "device = torch.device(\"mps\")\n",
    "model.to(device)\n",
    "\n",
    "faiss_index = faiss.read_index(\"Data/sentences-embeddings.idx\")\n",
    "\n",
    "with open(\"Data/sentences.pkl\", \"rb\") as f:\n",
    "    grouped_sentences = pickle.load(f)\n",
    "\n",
    "with open(\"Data/sent_to_metadata.pkl\", \"rb\") as f:\n",
    "    grouped_sent_to_metadata = pickle.load(f)\n",
    "\n",
    "student_question = input(\"Enter your question: \")\n",
    "question_embedding = model.encode(student_question, prompt_name=\"query\")\n",
    "if question_embedding.ndim == 1:\n",
    "    question_embedding = np.expand_dims(question_embedding, axis=0)\n",
    "\n",
    "distances, indices = faiss_index.search(question_embedding, 10)\n",
    "\n",
    "related_results = []\n",
    "for idx in indices[0]:\n",
    "    grouped_sent = grouped_sentences[idx]\n",
    "    filename, timestamp_range = grouped_sent_to_metadata.get(\n",
    "        grouped_sent, (\"Unknown\", \"Unknown\")\n",
    "    )\n",
    "    related_results.append((filename, timestamp_range, grouped_sent))\n",
    "\n",
    "print(\"Question:\", student_question)\n",
    "print(\"\\nTop Related Sentences with Metadata:\")\n",
    "for filename, timestamp, sent in related_results:\n",
    "    print(f\"- [{filename} | {timestamp}] {sent}\\n\")\n",
    "\n",
    "# Group results by file\n",
    "grouped_by_file = {}\n",
    "for filename, timestamp, sent in related_results:\n",
    "    if filename not in grouped_by_file:\n",
    "        grouped_by_file[filename] = []\n",
    "    grouped_by_file[filename].append((timestamp, sent))\n",
    "\n",
    "# Helper to parse timestamps\n",
    "def parse_ts(ts):\n",
    "    start, end = ts.split(\"->\")\n",
    "    fmt = \"%H:%M:%S.%f\"\n",
    "    return datetime.strptime(start.strip(), fmt), datetime.strptime(end.strip(), fmt)\n",
    "\n",
    "# Helper to clean and normalize sentences\n",
    "def clean_sentences(sentences):\n",
    "    cleaned = []\n",
    "    seen = set()\n",
    "    for s in sentences:\n",
    "        s = s.strip()\n",
    "        while s and s[-1] in \".!?\":\n",
    "            s = s[:-1].strip()\n",
    "        if s and s not in seen:\n",
    "            cleaned.append(s)\n",
    "            seen.add(s)\n",
    "    return cleaned\n",
    "\n",
    "def extract_file_number(fname):\n",
    "    # Extract numeric part from \"1.srt\", \"2.srt\" etc.\n",
    "    match = re.search(r\"(\\d+)\", fname)\n",
    "    return int(match.group(1)) if match else float('inf')\n",
    "\n",
    "# Order by start timestamp\n",
    "related_results_sorted = sorted(\n",
    "    related_results,\n",
    "    key=lambda x: (extract_file_number(x[0]), parse_ts(x[1])[0])\n",
    ")\n",
    "\n",
    "# Merge sentences and remove duplicates\n",
    "merged_results = []\n",
    "for filename, ts_range, text in related_results_sorted:\n",
    "    sentences = clean_sentences(text.split('. '))\n",
    "\n",
    "    if not merged_results:\n",
    "        merged_results.append([filename, ts_range, '. '.join(sentences) + '.'])\n",
    "        continue\n",
    "\n",
    "    prev = merged_results[-1]\n",
    "    prev_start, prev_end = parse_ts(prev[1])\n",
    "    curr_start, curr_end = parse_ts(ts_range)\n",
    "\n",
    "    if filename == prev[0] and curr_start <= prev_end:\n",
    "        new_start = prev_start.strftime(\"%H:%M:%S.%f\")[:-3]\n",
    "        new_end = max(prev_end, curr_end).strftime(\"%H:%M:%S.%f\")[:-3]\n",
    "        prev[1] = f\"{new_start} -> {new_end}\"\n",
    "\n",
    "        prev_sentences = clean_sentences(prev[2].split('. '))\n",
    "        combined_sentences = prev_sentences + sentences\n",
    "        prev[2] = '. '.join(clean_sentences(combined_sentences)) + '.'\n",
    "    else:\n",
    "        merged_results.append([filename, ts_range, '. '.join(sentences) + '.'])\n",
    "\n",
    "print(\"\\nOrdered & Merged Top Related Sentences (duplicates removed):\")\n",
    "for i, (fname, ts, sent) in enumerate(merged_results, 1):\n",
    "    print(f\"\\nGroup {i}\")\n",
    "    print(\"Text:\", sent)\n",
    "    print(\"Metadata:\", (fname, ts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "46f9b6f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('2.srt', '00:00:05.429 -> 00:00:30.570', \"Hi everyone. In this video, we're going to talk about linear regression. So we'll begin by the definition of linear regression, and we'll talk about how this model can optimize to get the best estimate value. And then we're going to talk about important quantities for linear regression, such as a fitness performance metric, things like that. And we'll talk about how statistically significant these estimate values are.\", 0.0), ('2.srt', '00:01:07.469 -> 00:01:30.569', \"And our goal is to tweak this parameter by optimization so that the model makes a prediction that's close to the target as much as possible. So what is a linear regression? It is one of the simplest kind of supervised learning model. And it predicts a real value number which is regression.\", 0.0), ('2.srt', '00:01:40.769 -> 00:02:06.859', \"That means the user doesn't need to figure out some design parameters in advance or during the training. And importantly, linear regression model assumes a linear relationship between the features and the target variable. Well, what does that mean. Well, what does that mean? It means the feature, let's say we have only one feature for now, has a linear relationship to the target variable.\", 0.0), ('2.srt', '00:04:13.289 -> 00:04:42.909', \"this becomes my linear model. And this is called linear combination. So this type of model, whether we have many variables or one variable, that shows some linear relationship of the variable to the target and this type of model is called the linear regression. Let's take an example. This data is coming from Kaggle website.\", 0.0)]\n"
     ]
    }
   ],
   "source": [
    "segment_list = []\n",
    "\n",
    "for filename, ts_range, text in merged_results:\n",
    "    # distance placeholder\n",
    "    distance = 0.0\n",
    "    segment_list.append((filename, ts_range, text, distance))\n",
    "\n",
    "# Now segment_list is ready to use\n",
    "print(segment_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "07dff472",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Building video Data/answer.mp4.\n",
      "MoviePy - Writing audio in answerTEMP_MPY_wvf_snd.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "chunk:  21%|██        | 485/2293 [00:00<00:02, 866.40it/s, now=None]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "chunk:  43%|████▎     | 995/2293 [00:01<00:01, 726.15it/s, now=None]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "chunk:  71%|███████   | 1626/2293 [00:02<00:00, 710.59it/s, now=None]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Writing video Data/answer.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready Data/answer.mp4\n"
     ]
    }
   ],
   "source": [
    "from moviepy.editor import VideoFileClip, concatenate_videoclips, ColorClip\n",
    "import os\n",
    "import datetime\n",
    "\n",
    "def format_srt_timestamp(seconds):\n",
    "    td = datetime.timedelta(seconds=seconds)\n",
    "    total_seconds = int(td.total_seconds())\n",
    "    milliseconds = int((td.total_seconds() - total_seconds) * 1000)\n",
    "    hours, remainder = divmod(total_seconds, 3600)\n",
    "    minutes, seconds = divmod(remainder, 60)\n",
    "    return f\"{hours:02}:{minutes:02}:{int(seconds):02},{milliseconds:03}\"\n",
    "\n",
    "def create_continuous_srt(clips_info, output_filename=\"Data/stitched_output.srt\", transition_sec=0.01):\n",
    "    srt_lines = []\n",
    "    current_time = 0.0\n",
    "\n",
    "    for idx, (duration, sentence, start, video_file) in enumerate(clips_info, start=1):\n",
    "        start_time = format_srt_timestamp(current_time)\n",
    "        end_time = format_srt_timestamp(current_time + duration)\n",
    "        srt_lines.append(f\"{idx}\\n{start_time} --> {end_time}\\n{sentence}\\n\")\n",
    "        current_time += duration + transition_sec  # Account for pause\n",
    "    with open(output_filename, \"w\") as f:\n",
    "        f.write(\"\\n\".join(srt_lines))\n",
    "\n",
    "def parse_timestamp(timestamp_str):\n",
    "    start, end = timestamp_str.split(\" -> \")\n",
    "    return start.strip(), end.strip()\n",
    "\n",
    "def stitch_video_from_segments(segment_list, srt_filename=\"Data/stitched_output.srt\", pause_duration=0.01):\n",
    "    clips = []\n",
    "    clips_info = []\n",
    "    sources = set()\n",
    "\n",
    "    for idx, (filename, timestamp, sentence, distance) in enumerate(segment_list):\n",
    "        lecture_no = os.path.splitext(filename)[0]\n",
    "        video_file = \"Data/Videos/\" + lecture_no + \".mp4\"\n",
    "        start, end = parse_timestamp(timestamp)\n",
    "        sources.add(\"Lecture - \" + lecture_no)\n",
    "\n",
    "        try:\n",
    "            clip = VideoFileClip(video_file).subclip(start, end)\n",
    "\n",
    "            # Resize black screen to match clip size\n",
    "            if idx > 0:\n",
    "                black_clip = ColorClip(size=clip.size, color=(0, 0, 0), duration=pause_duration)\n",
    "                black_clip = black_clip.set_fps(clip.fps)\n",
    "                clips.append(black_clip)\n",
    "\n",
    "            clips.append(clip)\n",
    "            clips_info.append((clip.duration, sentence, start, video_file))\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing segment ({filename}, {timestamp}): {e}\")\n",
    "\n",
    "    if not clips:\n",
    "        print(\"No valid clips found.\")\n",
    "        return\n",
    "\n",
    "    final_clip = concatenate_videoclips(clips, method=\"chain\")\n",
    "\n",
    "    final_clip.write_videofile(\n",
    "        \"Data/answer.mp4\",\n",
    "        codec=\"libx264\",\n",
    "        preset=\"ultrafast\",\n",
    "        threads=4,\n",
    "        audio_codec=\"aac\"\n",
    "    )\n",
    "\n",
    "    create_continuous_srt(clips_info, output_filename=srt_filename, transition_sec=pause_duration)\n",
    "\n",
    "stitch_video_from_segments(segment_list, srt_filename=\"Data/stitched_output.srt\", pause_duration=0.01)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2d4614df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi everyone. In this video, we're going to talk about multilinear regression. So last time we talked about multilinear regression with the higher order terms of a single variable, and this time we're going to talk about multilinear regression model when there are multiple variables.\n",
      "('2.srt', '00:00:06.169 --> 00:00:06.910', '00:00:05.429 --> 00:00:06.209')\n"
     ]
    }
   ],
   "source": [
    "key = list(grouped_sent_to_metadata.keys())[0]\n",
    "print(key)\n",
    "print(grouped_sent_to_metadata[key])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a53200b5",
   "metadata": {},
   "source": [
    "### Summarisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c24187d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUJAL IS NOT HERE\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Combine all top-K grouped sentences into one text\n",
    "all_text = \" \".join([sent for _, _, sent in related_results])\n",
    "print(all_text)\n",
    "\n",
    "print(\"SUJAL IS NOT HERE\")\n",
    "# Summarize across groups (num_sentences controls how many groups to pick)\n",
    "summary_text = summarizer(all_text, num_sentences=4)  # pick top 4 important groups\n",
    "print(\"SUJAL CODE IS WORKING\")\n",
    "# Map summary back to original grouped sentences to get metadata\n",
    "important_groups = []\n",
    "for filename, timestamp, grouped_sent in related_results:\n",
    "    # Check if this grouped sentence appears in the summary\n",
    "    if grouped_sent in summary_text:\n",
    "        important_groups.append((filename, timestamp, grouped_sent))\n",
    "\n",
    "# Print important groups with metadata\n",
    "print(\"\\nMost Important Groups with Metadata:\")\n",
    "for filename, timestamp, grouped_sent in important_groups:\n",
    "    print(f\"- [{filename} | {timestamp}] {grouped_sent}\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
